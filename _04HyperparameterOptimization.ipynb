{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredofosu/python.projects/blob/main/_04HyperparameterOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AXt9qLkjiLgK"
      },
      "outputs": [],
      "source": [
        "#@title Setup a virtual environemnt\n",
        "from IPython.display import clear_output\n",
        "# !pip install --upgrade pip\n",
        "# !pip install virtualenv\n",
        "# !virtualenv /content/drive/MyDrive/Colab_Notebooks/colab_env\n",
        "\n",
        "# import sys\n",
        "# # activate virtual environment\n",
        "# !source /content/drive/MyDrive/Colab_Notebooks/colab_env/bin/activate\n",
        "# #  add virtual environment path to colab's system path\n",
        "# sys.path.append(\"/content/drive/MyDrive/Colab_Notebooks/colab_env/lib/python3.10/site-packages\")\n",
        "clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25-HS8oUWvvR"
      },
      "source": [
        "# Setting up Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yJrhhly7T-8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ac0c2f-94bc-4733-b3bd-3f28dede2a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "PROJECT_ID = \"master-thesis-yorku-aofosu\"  # @param {type:\"string\"}\n",
        "\n",
        "auth.authenticate_user(project_id=PROJECT_ID)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO5D-Rqgfq7V",
        "outputId": "70c89a85-9e28-47bc-8847-d84cbe3d01e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hyperOpt\n"
          ]
        }
      ],
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "!mkdir colab_bucket\n",
        "!gcsfuse --implicit-dirs master-thesis-yorku-aofosu colab_bucket\n",
        "\n",
        "clear_output(wait=False)\n",
        "!ls /content/colab_bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LczoPEBEW9qp"
      },
      "source": [
        "# Libraries and Connections to Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZomxuxWhCfMs"
      },
      "outputs": [],
      "source": [
        "#@title Install Libraries\n",
        "%pip install optuna\n",
        "%pip install optuna.integration\n",
        "%pip install pyyaml h5py\n",
        "%pip install tensorflow==2.13.*\n",
        "\n",
        "%pip install lets-plot\n",
        "%pip install cairosvg\n",
        "\n",
        "%pip install scikeras[tensorflow]\n",
        "%pip install -q \"git+https://github.com/tensorflow/docs\"\n",
        "%pip install -q -U keras-tuner\n",
        "clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa3FaVFDXyQi",
        "outputId": "571c1bb7-1b5b-4129-972d-eee2b0d82426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n",
            "Python version: 3.10.12\n"
          ]
        }
      ],
      "source": [
        "#@title Import Libraries\n",
        "\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "# Data transformation libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Numeric manipulation libraries\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "import random\n",
        "\n",
        "# Other libraries\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Date and time manipulation libraries\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pytz\n",
        "eastern = pytz.timezone('Canada/Eastern')\n",
        "\n",
        "# Hyperparameter tuning libraries\n",
        "import keras_tuner as kt\n",
        "# import optuna\n",
        "\n",
        "# Deep learning libraries\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "# Instantiating a GPU or TPU\n",
        "# try:\n",
        "#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "#   tf.config.experimental_connect_to_cluster(tpu)\n",
        "#   tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "#   strategy = tf.distribute.TPUStrategy(tpu)\n",
        "#   print(\"Number of TPU devices: \",  len(tf.config.list_logical_devices(\"TPU\")))\n",
        "#   print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "# except ValueError:  # detect GPUs\n",
        "#   strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "#   if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "#     print('WARNING: GPU device not found.')\n",
        "#   else:\n",
        "#     print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
        "#   print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "#   # raise BaseException('ERROR: Not connected to a TPU runtime!'\n",
        "#----------------------------------------------------------------------------------\n",
        "\n",
        "# Python in-built libraries\n",
        "from platform import python_version\n",
        "print(\"Python version:\", python_version())\n",
        "\n",
        "# Google colab libraries\n",
        "from google.colab import runtime\n",
        "\n",
        "# Data visualization libraries\n",
        "from lets_plot import *\n",
        "LetsPlot.setup_html()\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Set a fixed random seed to always get the same results\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nSaIVIfItMjW"
      },
      "outputs": [],
      "source": [
        "# set local workspace\n",
        "local_path = \"/content/drive/MyDrive/Colab_Notebooks/master_thesis\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OWUlAgbVuNc1"
      },
      "outputs": [],
      "source": [
        "# #@title Copy files to the bucket\n",
        "# !gsutil -m cp -r \"/content/drive/MyDrive/Colab Notebooks/master_thesis/hyperOpt/all/tensorboard/tb_logs/mse/sLnn/logs\" gs://master-thesis-yorku-aofosu/hyperOpt/all/mse/sLnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HHhnqkXaJju"
      },
      "source": [
        "# Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fukofp4bheBh",
        "outputId": "78bf6330-0f9b-4639-f49e-9b1bfe912a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 1368 entries, 1965-01-31 to 2021-12-31\n",
            "Data columns (total 55 columns):\n",
            " #   Column                             Non-Null Count  Dtype   \n",
            "---  ------                             --------------  -----   \n",
            " 0   major_basin_X                      1368 non-null   category\n",
            " 1   Ca_X                               1368 non-null   float32 \n",
            " 2   Cl_X                               1368 non-null   float32 \n",
            " 3   K_X                                1368 non-null   float32 \n",
            " 4   Mg_X                               1368 non-null   float32 \n",
            " 5   Na_X                               1368 non-null   float32 \n",
            " 6   SO42_X                             1368 non-null   float32 \n",
            " 7   TDS_X                              1368 non-null   float32 \n",
            " 8   ALKT_X                             1368 non-null   float32 \n",
            " 9   CNDT_X                             1368 non-null   float32 \n",
            " 10  HARD_X                             1368 non-null   float32 \n",
            " 11  pH_X                               1368 non-null   float32 \n",
            " 12  CO32_X                             1368 non-null   float32 \n",
            " 13  HCO3_X                             1368 non-null   float32 \n",
            " 14  avg_water_Temp_X                   1368 non-null   float32 \n",
            " 15  avg_discharge_X                    1368 non-null   float32 \n",
            " 16  avg_stage_X                        1368 non-null   float32 \n",
            " 17  avg_air_temp_X                     1368 non-null   float32 \n",
            " 18  avg_precip_X                       1368 non-null   float32 \n",
            " 19  max_Wx_X                           1368 non-null   float32 \n",
            " 20  max_Wy_X                           1368 non-null   float32 \n",
            " 21  missingindicator_Ca_X              1368 non-null   float32 \n",
            " 22  missingindicator_Cl_X              1368 non-null   float32 \n",
            " 23  missingindicator_K_X               1368 non-null   float32 \n",
            " 24  missingindicator_Mg_X              1368 non-null   float32 \n",
            " 25  missingindicator_Na_X              1368 non-null   float32 \n",
            " 26  missingindicator_SO42_X            1368 non-null   float32 \n",
            " 27  missingindicator_TDS_X             1368 non-null   float32 \n",
            " 28  missingindicator_ALKT_X            1368 non-null   float32 \n",
            " 29  missingindicator_CNDT_X            1368 non-null   float32 \n",
            " 30  missingindicator_HARD_X            1368 non-null   float32 \n",
            " 31  missingindicator_pH_X              1368 non-null   float32 \n",
            " 32  missingindicator_avg_water_Temp_X  1368 non-null   float32 \n",
            " 33  missingindicator_avg_discharge_X   1368 non-null   float32 \n",
            " 34  missingindicator_avg_stage_X       1368 non-null   float32 \n",
            " 35  missingindicator_avg_air_temp_X    1368 non-null   float32 \n",
            " 36  missingindicator_avg_precip_X      1368 non-null   float32 \n",
            " 37  missingindicator_max_Wx_X          1368 non-null   float32 \n",
            " 38  missingindicator_max_Wy_X          1368 non-null   float32 \n",
            " 39  Ca_f_X                             1368 non-null   float32 \n",
            " 40  Cl_f_X                             1368 non-null   float32 \n",
            " 41  Na_f_X                             1368 non-null   float32 \n",
            " 42  SO42_f_X                           1368 non-null   float32 \n",
            " 43  pH_max_f_X                         1368 non-null   float32 \n",
            " 44  pH_min_f_X                         1368 non-null   float32 \n",
            " 45  avg_water_Temp_f_X                 1368 non-null   float32 \n",
            " 46  CCMEWQI_X                          1368 non-null   float32 \n",
            " 47  rank_X                             1368 non-null   category\n",
            " 48  Ca_y                               1368 non-null   float32 \n",
            " 49  Cl_y                               1368 non-null   float32 \n",
            " 50  K_y                                1368 non-null   float32 \n",
            " 51  Mg_y                               1368 non-null   float32 \n",
            " 52  Na_y                               1368 non-null   float32 \n",
            " 53  SO42_y                             1368 non-null   float32 \n",
            " 54  TDS_y                              1368 non-null   float32 \n",
            "dtypes: category(2), float32(53)\n",
            "memory usage: 296.9 KB\n"
          ]
        }
      ],
      "source": [
        "#@title Load dataset\n",
        "\n",
        "# A function to load a timeseries dataset\n",
        "def load(path_to_dataset: str, date_column_name: str) -> pd.DataFrame:\n",
        "\n",
        "  '''\n",
        "  @param path_to_dataset: a path where a timeseries dataset is stored -> Pandas Dataframe\n",
        "  @param date_column_name: a column containing date values -> String\n",
        "\n",
        "  '''\n",
        "  df = pd.read_csv(path_to_dataset, parse_dates=[date_column_name])\n",
        "\n",
        "  df[date_column_name] = pd.to_datetime(df[date_column_name]) # <- utc=True, if needed\n",
        "\n",
        "  df.set_index(date_column_name, inplace=True)\n",
        "\n",
        "  # Assuming 'df' is your DataFrame\n",
        "  numeric_columns = df.select_dtypes(include=['int', 'float']).columns\n",
        "\n",
        "  # Convert numeric columns to float32\n",
        "  df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
        "\n",
        "  # Convert columns with data type 'object' to 'category'\n",
        "  object_columns = df.select_dtypes(['object']).columns\n",
        "  df[object_columns] = df[object_columns].astype('category')\n",
        "\n",
        "  df = df.rename(columns={'Ca2+':'Ca', 'Cl-':'Cl', 'K+':'K', 'Mg2+':'Mg', 'Na+':'Na', 'SO42-':'SO42',\n",
        "                          'CO32-':'CO32', 'HCO3-': 'HCO3'})\n",
        "\n",
        "  return df\n",
        "\n",
        "# Load 1st Approach Data - Predicting each major ion with the combined dataset\n",
        "dataset = load(f'{local_path}/data/modelling_data/wq_all_unscaled.csv', 'date')\n",
        "\n",
        "# Load Second Dataset - Up predicts Down\n",
        "dataset_x = load(f'{local_path}/data/modelling_data/wq_up_down_unscaled.csv', 'date')\n",
        "dataset_x.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_x.tail(13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "lG-T_UBQD6uW",
        "outputId": "a82fbaf1-5ad7-4168-90a6-0fcfd7167eb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             major_basin_X        Ca_X         Cl_X    K_X       Mg_X  \\\n",
              "date                                                                    \n",
              "2020-12-31  East Don River  111.500000  1430.000000  3.265  17.200001   \n",
              "2021-01-31  East Don River  139.500000   988.000000  3.140  25.450001   \n",
              "2021-02-28  East Don River  164.000000  2405.000000  6.665  24.049999   \n",
              "2021-03-31  East Don River  119.000000   501.000000  3.035  23.549999   \n",
              "2021-04-30  East Don River   98.500000   860.000000  3.095  17.000000   \n",
              "2021-05-31  East Don River  127.000000   358.000000  0.825  25.000000   \n",
              "2021-06-30  East Don River  115.500000   298.000000  2.785  21.400000   \n",
              "2021-07-31  East Don River  106.000000   256.000000  2.890  20.600000   \n",
              "2021-08-31  East Don River  111.500000   222.500000  2.965  23.100000   \n",
              "2021-09-30  East Don River   42.049999    71.449997  2.550   5.325000   \n",
              "2021-10-31  East Don River   78.750000   178.000000  2.640  12.950000   \n",
              "2021-11-30  East Don River  110.000000   238.500000  2.940  20.650000   \n",
              "2021-12-31  East Don River  113.000000   416.000000  2.505  19.750000   \n",
              "\n",
              "               Na_X      SO42_X    TDS_X      ALKT_X   CNDT_X  ...  \\\n",
              "date                                                           ...   \n",
              "2020-12-31   849.50   57.150002  2857.00  209.000000  4805.25  ...   \n",
              "2021-01-31   577.50   69.650002  2053.00  291.445007  3358.50  ...   \n",
              "2021-02-28  1385.00  117.949997  4393.25  182.054993  7231.00  ...   \n",
              "2021-03-31   264.50   61.549999  1299.75  260.475006  2116.25  ...   \n",
              "2021-04-30   495.00   57.750000  1931.00  194.330002  3172.00  ...   \n",
              "2021-05-31   182.50   53.000000  1018.00  280.589996  1671.50  ...   \n",
              "2021-06-30   157.50   46.450001   923.00  256.600006  1469.25  ...   \n",
              "2021-07-31   130.00   43.950001   870.25  242.899994  1394.75  ...   \n",
              "2021-08-31   116.00   46.250000   751.00  249.100006  1259.00  ...   \n",
              "2021-09-30    53.25   22.799999   325.75  116.099998   519.75  ...   \n",
              "2021-10-31   104.50   38.000000   617.25  207.149994  1013.50  ...   \n",
              "2021-11-30   124.50   49.200001   827.75  272.000000  1341.50  ...   \n",
              "2021-12-31   221.50   58.049999  1143.00  278.850006  1896.75  ...   \n",
              "\n",
              "            avg_water_Temp_f_X  CCMEWQI_X    rank_X        Ca_y         Cl_y  \\\n",
              "date                                                                           \n",
              "2020-12-31                 0.0  64.941902  Marginal   85.800003   572.000000   \n",
              "2021-01-31                 0.0  70.604431      Fair  131.000000  1290.000000   \n",
              "2021-02-28                 0.0  57.866955  Marginal  103.000000  1760.000000   \n",
              "2021-03-31                 0.0  80.404762      Good  100.000000   577.000000   \n",
              "2021-04-30                 0.0  72.761642      Fair   95.800003   564.000000   \n",
              "2021-05-31                 1.0  83.317642      Good   97.400002   377.000000   \n",
              "2021-06-30                 1.0  84.341774      Good   87.599998   340.000000   \n",
              "2021-07-31                 1.0  84.783707      Good  103.190697   386.828918   \n",
              "2021-08-31                 1.0  85.564262      Good   74.166664   216.333328   \n",
              "2021-09-30                 1.0  93.188187      Good   54.700001    75.900002   \n",
              "2021-10-31                 0.0  92.631149      Good   69.000000    44.400002   \n",
              "2021-11-30                 0.0  91.483337      Good   80.699997   265.000000   \n",
              "2021-12-31                 0.0  82.372597      Good  109.000000   445.000000   \n",
              "\n",
              "                 K_y       Mg_y         Na_y     SO42_y        TDS_y  \n",
              "date                                                                  \n",
              "2020-12-31  3.870000  15.600000   392.000000  61.532845  1555.000000  \n",
              "2021-01-31  5.310000  25.600000   844.000000  44.223610  2642.000000  \n",
              "2021-02-28  4.120000  14.800000  1200.000000  68.305794  4179.000000  \n",
              "2021-03-31  4.450000  23.799999   321.000000  53.643860  1497.000000  \n",
              "2021-04-30  3.600000  19.100000   323.000000  76.079018  1435.000000  \n",
              "2021-05-31  5.000000  19.500000   206.000000  42.442326  1093.000000  \n",
              "2021-06-30  4.670000  18.100000   183.000000  30.695601   953.000000  \n",
              "2021-07-31  4.944576  18.557232   149.029083  81.511070  1397.144043  \n",
              "2021-08-31  3.673333  16.250000   144.566666  25.400000   727.333313  \n",
              "2021-09-30  2.830000   6.650000    52.700001  62.618492   312.000000  \n",
              "2021-10-31  3.790000  15.000000   144.000000  48.148754   691.000000  \n",
              "2021-11-30  3.750000  17.200001   164.000000  41.272068   838.000000  \n",
              "2021-12-31  4.220000  20.000000   262.000000  55.932095  1276.000000  \n",
              "\n",
              "[13 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74f7d817-3ee8-4c16-959f-3a86952f1502\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>major_basin_X</th>\n",
              "      <th>Ca_X</th>\n",
              "      <th>Cl_X</th>\n",
              "      <th>K_X</th>\n",
              "      <th>Mg_X</th>\n",
              "      <th>Na_X</th>\n",
              "      <th>SO42_X</th>\n",
              "      <th>TDS_X</th>\n",
              "      <th>ALKT_X</th>\n",
              "      <th>CNDT_X</th>\n",
              "      <th>...</th>\n",
              "      <th>avg_water_Temp_f_X</th>\n",
              "      <th>CCMEWQI_X</th>\n",
              "      <th>rank_X</th>\n",
              "      <th>Ca_y</th>\n",
              "      <th>Cl_y</th>\n",
              "      <th>K_y</th>\n",
              "      <th>Mg_y</th>\n",
              "      <th>Na_y</th>\n",
              "      <th>SO42_y</th>\n",
              "      <th>TDS_y</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-12-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>1430.000000</td>\n",
              "      <td>3.265</td>\n",
              "      <td>17.200001</td>\n",
              "      <td>849.50</td>\n",
              "      <td>57.150002</td>\n",
              "      <td>2857.00</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>4805.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.941902</td>\n",
              "      <td>Marginal</td>\n",
              "      <td>85.800003</td>\n",
              "      <td>572.000000</td>\n",
              "      <td>3.870000</td>\n",
              "      <td>15.600000</td>\n",
              "      <td>392.000000</td>\n",
              "      <td>61.532845</td>\n",
              "      <td>1555.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>139.500000</td>\n",
              "      <td>988.000000</td>\n",
              "      <td>3.140</td>\n",
              "      <td>25.450001</td>\n",
              "      <td>577.50</td>\n",
              "      <td>69.650002</td>\n",
              "      <td>2053.00</td>\n",
              "      <td>291.445007</td>\n",
              "      <td>3358.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.604431</td>\n",
              "      <td>Fair</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>1290.000000</td>\n",
              "      <td>5.310000</td>\n",
              "      <td>25.600000</td>\n",
              "      <td>844.000000</td>\n",
              "      <td>44.223610</td>\n",
              "      <td>2642.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-28</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>2405.000000</td>\n",
              "      <td>6.665</td>\n",
              "      <td>24.049999</td>\n",
              "      <td>1385.00</td>\n",
              "      <td>117.949997</td>\n",
              "      <td>4393.25</td>\n",
              "      <td>182.054993</td>\n",
              "      <td>7231.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.866955</td>\n",
              "      <td>Marginal</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>1760.000000</td>\n",
              "      <td>4.120000</td>\n",
              "      <td>14.800000</td>\n",
              "      <td>1200.000000</td>\n",
              "      <td>68.305794</td>\n",
              "      <td>4179.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>501.000000</td>\n",
              "      <td>3.035</td>\n",
              "      <td>23.549999</td>\n",
              "      <td>264.50</td>\n",
              "      <td>61.549999</td>\n",
              "      <td>1299.75</td>\n",
              "      <td>260.475006</td>\n",
              "      <td>2116.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80.404762</td>\n",
              "      <td>Good</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>577.000000</td>\n",
              "      <td>4.450000</td>\n",
              "      <td>23.799999</td>\n",
              "      <td>321.000000</td>\n",
              "      <td>53.643860</td>\n",
              "      <td>1497.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-04-30</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>98.500000</td>\n",
              "      <td>860.000000</td>\n",
              "      <td>3.095</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>495.00</td>\n",
              "      <td>57.750000</td>\n",
              "      <td>1931.00</td>\n",
              "      <td>194.330002</td>\n",
              "      <td>3172.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.761642</td>\n",
              "      <td>Fair</td>\n",
              "      <td>95.800003</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>19.100000</td>\n",
              "      <td>323.000000</td>\n",
              "      <td>76.079018</td>\n",
              "      <td>1435.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-05-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>358.000000</td>\n",
              "      <td>0.825</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>182.50</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>1018.00</td>\n",
              "      <td>280.589996</td>\n",
              "      <td>1671.50</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>83.317642</td>\n",
              "      <td>Good</td>\n",
              "      <td>97.400002</td>\n",
              "      <td>377.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>206.000000</td>\n",
              "      <td>42.442326</td>\n",
              "      <td>1093.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-06-30</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>115.500000</td>\n",
              "      <td>298.000000</td>\n",
              "      <td>2.785</td>\n",
              "      <td>21.400000</td>\n",
              "      <td>157.50</td>\n",
              "      <td>46.450001</td>\n",
              "      <td>923.00</td>\n",
              "      <td>256.600006</td>\n",
              "      <td>1469.25</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>84.341774</td>\n",
              "      <td>Good</td>\n",
              "      <td>87.599998</td>\n",
              "      <td>340.000000</td>\n",
              "      <td>4.670000</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>30.695601</td>\n",
              "      <td>953.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>2.890</td>\n",
              "      <td>20.600000</td>\n",
              "      <td>130.00</td>\n",
              "      <td>43.950001</td>\n",
              "      <td>870.25</td>\n",
              "      <td>242.899994</td>\n",
              "      <td>1394.75</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>84.783707</td>\n",
              "      <td>Good</td>\n",
              "      <td>103.190697</td>\n",
              "      <td>386.828918</td>\n",
              "      <td>4.944576</td>\n",
              "      <td>18.557232</td>\n",
              "      <td>149.029083</td>\n",
              "      <td>81.511070</td>\n",
              "      <td>1397.144043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-08-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>111.500000</td>\n",
              "      <td>222.500000</td>\n",
              "      <td>2.965</td>\n",
              "      <td>23.100000</td>\n",
              "      <td>116.00</td>\n",
              "      <td>46.250000</td>\n",
              "      <td>751.00</td>\n",
              "      <td>249.100006</td>\n",
              "      <td>1259.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>85.564262</td>\n",
              "      <td>Good</td>\n",
              "      <td>74.166664</td>\n",
              "      <td>216.333328</td>\n",
              "      <td>3.673333</td>\n",
              "      <td>16.250000</td>\n",
              "      <td>144.566666</td>\n",
              "      <td>25.400000</td>\n",
              "      <td>727.333313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-09-30</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>42.049999</td>\n",
              "      <td>71.449997</td>\n",
              "      <td>2.550</td>\n",
              "      <td>5.325000</td>\n",
              "      <td>53.25</td>\n",
              "      <td>22.799999</td>\n",
              "      <td>325.75</td>\n",
              "      <td>116.099998</td>\n",
              "      <td>519.75</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>93.188187</td>\n",
              "      <td>Good</td>\n",
              "      <td>54.700001</td>\n",
              "      <td>75.900002</td>\n",
              "      <td>2.830000</td>\n",
              "      <td>6.650000</td>\n",
              "      <td>52.700001</td>\n",
              "      <td>62.618492</td>\n",
              "      <td>312.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>78.750000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>2.640</td>\n",
              "      <td>12.950000</td>\n",
              "      <td>104.50</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>617.25</td>\n",
              "      <td>207.149994</td>\n",
              "      <td>1013.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92.631149</td>\n",
              "      <td>Good</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>44.400002</td>\n",
              "      <td>3.790000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>144.000000</td>\n",
              "      <td>48.148754</td>\n",
              "      <td>691.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-30</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>238.500000</td>\n",
              "      <td>2.940</td>\n",
              "      <td>20.650000</td>\n",
              "      <td>124.50</td>\n",
              "      <td>49.200001</td>\n",
              "      <td>827.75</td>\n",
              "      <td>272.000000</td>\n",
              "      <td>1341.50</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>91.483337</td>\n",
              "      <td>Good</td>\n",
              "      <td>80.699997</td>\n",
              "      <td>265.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>17.200001</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>41.272068</td>\n",
              "      <td>838.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-31</th>\n",
              "      <td>East Don River</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>416.000000</td>\n",
              "      <td>2.505</td>\n",
              "      <td>19.750000</td>\n",
              "      <td>221.50</td>\n",
              "      <td>58.049999</td>\n",
              "      <td>1143.00</td>\n",
              "      <td>278.850006</td>\n",
              "      <td>1896.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.372597</td>\n",
              "      <td>Good</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>445.000000</td>\n",
              "      <td>4.220000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>262.000000</td>\n",
              "      <td>55.932095</td>\n",
              "      <td>1276.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13 rows × 55 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74f7d817-3ee8-4c16-959f-3a86952f1502')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74f7d817-3ee8-4c16-959f-3a86952f1502 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74f7d817-3ee8-4c16-959f-3a86952f1502');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-95682e0e-ea96-4073-bcc6-b9049b4d746d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-95682e0e-ea96-4073-bcc6-b9049b4d746d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-95682e0e-ea96-4073-bcc6-b9049b4d746d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EbMbKaJxZ_RT"
      },
      "outputs": [],
      "source": [
        "# Group the parameters and features\n",
        "parameters = {'water quality parameters': ['Ca', 'Cl', 'K', 'Mg', 'Na','SO42', 'TDS',],\n",
        "              'water quality parameters x': ['Ca_y', 'Cl_y', 'K_y', 'Mg_y', 'Na_y','SO42_y', 'TDS_y',],\n",
        "              }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_to_model = dataset[['major_basin',\n",
        "#                             'Ca', 'Cl', 'K', 'Mg', 'Na','SO42', 'TDS',\n",
        "#                             'CO32', 'HCO3', 'ALKT', 'CNDT', 'HARD', 'pH','avg_water_Temp',\n",
        "#                             'avg_air_temp', 'avg_precip', 'avg_discharge', 'avg_stage', 'max_Wx', 'max_Wy',\n",
        "#                             'Ca_f', 'Cl_f', 'Na_f', 'SO42_f', 'pH_max_f', 'pH_min_f', 'avg_water_Temp_f',\n",
        "#                             'CCMEWQI', 'rank',\n",
        "#                             'missingindicator_Ca2+', 'missingindicator_Cl-', 'missingindicator_K+',\n",
        "#                             'missingindicator_Mg2+', 'missingindicator_Na+',\n",
        "#                             'missingindicator_SO42-', 'missingindicator_TDS',\n",
        "#                             'missingindicator_ALKT', 'missingindicator_CNDT',\n",
        "#                             'missingindicator_HARD', 'missingindicator_pH',\n",
        "#                             'missingindicator_avg_water_Temp', 'missingindicator_avg_discharge',\n",
        "#                             'missingindicator_avg_stage', 'missingindicator_avg_air_temp',\n",
        "#                             'missingindicator_avg_precip', 'missingindicator_max_Wx',\n",
        "#                             'missingindicator_max_Wy',\n",
        "#                             ]].copy()\n",
        "\n",
        "dataset_to_model_x = dataset_x[['major_basin_X', 'Ca_X', 'Cl_X', 'K_X', 'Mg_X', 'Na_X','SO42_X', 'TDS_X',\n",
        "                                'ALKT_X', 'CNDT_X', 'HARD_X', 'pH_X', 'CO32_X', 'HCO3_X',\n",
        "                                'avg_water_Temp_X', 'avg_discharge_X', 'avg_stage_X',\n",
        "                                'avg_air_temp_X', 'avg_precip_X', 'max_Wx_X', 'max_Wy_X',\n",
        "                                # 'missingindicator_Ca_X', 'missingindicator_Cl_X',\n",
        "                                # 'missingindicator_K_X', 'missingindicator_Mg_X',\n",
        "                                # 'missingindicator_Na_X', 'missingindicator_SO42_X',\n",
        "                                # 'missingindicator_TDS_X', 'missingindicator_ALKT_X',\n",
        "                                # 'missingindicator_CNDT_X', 'missingindicator_HARD_X',\n",
        "                                # 'missingindicator_pH_X',\n",
        "                                # 'missingindicator_avg_water_Temp_X',\n",
        "                                # 'missingindicator_avg_discharge_X', 'missingindicator_avg_stage_X',\n",
        "                                # 'missingindicator_avg_air_temp_X', 'missingindicator_avg_precip_X',\n",
        "                                # 'missingindicator_max_Wx_X', 'missingindicator_max_Wy_X',\n",
        "                                # 'Ca_f_X','Cl_f_X', 'Na_f_X', 'SO42_f_X', 'pH_max_f_X', 'pH_min_f_X','avg_water_Temp_f_X',\n",
        "                                # 'CCMEWQI_X',\n",
        "                                'Ca_y', 'Cl_y', 'K_y','Mg_y', 'Na_y', 'SO42_y', 'TDS_y',]].copy()\n",
        "\n",
        "# dataset_to_model = dataset_to_model.groupby('date').mean()\n",
        "# dataset_to_model.info()\n",
        "# dataset_to_model_x.info()"
      ],
      "metadata": {
        "id": "QWi9wTM52Fjl"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_to_model_x.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "EWQF9TthjKzb",
        "outputId": "ae837293-52b4-4d86-c3c6-5615b341490f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Ca_X         Cl_X          K_X         Mg_X         Na_X  \\\n",
              "count  1368.000000  1368.000000  1368.000000  1368.000000  1368.000000   \n",
              "mean    105.080170   362.850616     3.535991    17.596149   212.923615   \n",
              "std      23.282228   363.382294     1.590683     4.299785   195.785294   \n",
              "min      21.092251     0.000000     0.170104     2.392559     0.000000   \n",
              "25%      89.454746   118.375000     2.550000    14.532604    91.889584   \n",
              "50%     104.993031   233.392815     3.366624    17.536710   164.935104   \n",
              "75%     119.365871   534.327866     4.407233    20.364126   288.819977   \n",
              "max     194.763153  3380.000000    20.000000    32.303604  2090.000000   \n",
              "\n",
              "            SO42_X        TDS_X       ALKT_X        CNDT_X       HARD_X  ...  \\\n",
              "count  1368.000000  1368.000000  1368.000000   1368.000000  1368.000000  ...   \n",
              "mean     49.203148  1011.477051   212.142319   1637.363281   337.739685  ...   \n",
              "std      14.172410   652.732788    44.709831   1128.829102    76.192490  ...   \n",
              "min       8.318313    11.247286    62.843636     42.457413    39.155308  ...   \n",
              "25%      39.903006   586.162552   185.379574    909.773849   287.069656  ...   \n",
              "50%      48.652571   850.095093   213.010849   1285.447998   337.880554  ...   \n",
              "75%      57.714706  1315.555298   243.944450   2126.954407   387.001335  ...   \n",
              "max     117.949997  5449.500000   448.481049  11965.000000   621.632751  ...   \n",
              "\n",
              "       pH_min_f_X  avg_water_Temp_f_X    CCMEWQI_X         Ca_y         Cl_y  \\\n",
              "count      1368.0         1368.000000  1368.000000  1368.000000  1368.000000   \n",
              "mean          0.0            0.277047    85.731232    97.660873   344.450256   \n",
              "std           0.0            0.447703     9.137426    24.917759   394.359039   \n",
              "min           0.0            0.000000    53.753723     0.113000     0.000000   \n",
              "25%           0.0            0.000000    78.985514    80.251501   133.808731   \n",
              "50%           0.0            0.000000    85.908348    96.869648   204.000000   \n",
              "75%           0.0            1.000000    93.025692   114.000000   392.749786   \n",
              "max           0.0            1.000000   100.000000   176.800690  3920.000000   \n",
              "\n",
              "               K_y         Mg_y         Na_y       SO42_y        TDS_y  \n",
              "count  1368.000000  1368.000000  1368.000000  1368.000000  1368.000000  \n",
              "mean      3.751671    16.737942   200.703644    49.947311   898.626831  \n",
              "std       1.554919     4.585690   222.854507    17.064468   598.139343  \n",
              "min       0.003100     0.020100     0.000000     5.914264    25.441805  \n",
              "25%       2.749619    13.756427    72.178301    37.803691   542.812500  \n",
              "50%       3.818478    16.744036   141.783073    49.502325   709.535919  \n",
              "75%       4.693509    19.832062   244.960835    60.872105  1082.120544  \n",
              "max      12.500000    32.381100  1972.887695   178.303329  4840.000000  \n",
              "\n",
              "[8 rows x 53 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d11b257f-1822-4837-8c68-1d4df10b62b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ca_X</th>\n",
              "      <th>Cl_X</th>\n",
              "      <th>K_X</th>\n",
              "      <th>Mg_X</th>\n",
              "      <th>Na_X</th>\n",
              "      <th>SO42_X</th>\n",
              "      <th>TDS_X</th>\n",
              "      <th>ALKT_X</th>\n",
              "      <th>CNDT_X</th>\n",
              "      <th>HARD_X</th>\n",
              "      <th>...</th>\n",
              "      <th>pH_min_f_X</th>\n",
              "      <th>avg_water_Temp_f_X</th>\n",
              "      <th>CCMEWQI_X</th>\n",
              "      <th>Ca_y</th>\n",
              "      <th>Cl_y</th>\n",
              "      <th>K_y</th>\n",
              "      <th>Mg_y</th>\n",
              "      <th>Na_y</th>\n",
              "      <th>SO42_y</th>\n",
              "      <th>TDS_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1368.0</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "      <td>1368.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>105.080170</td>\n",
              "      <td>362.850616</td>\n",
              "      <td>3.535991</td>\n",
              "      <td>17.596149</td>\n",
              "      <td>212.923615</td>\n",
              "      <td>49.203148</td>\n",
              "      <td>1011.477051</td>\n",
              "      <td>212.142319</td>\n",
              "      <td>1637.363281</td>\n",
              "      <td>337.739685</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.277047</td>\n",
              "      <td>85.731232</td>\n",
              "      <td>97.660873</td>\n",
              "      <td>344.450256</td>\n",
              "      <td>3.751671</td>\n",
              "      <td>16.737942</td>\n",
              "      <td>200.703644</td>\n",
              "      <td>49.947311</td>\n",
              "      <td>898.626831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>23.282228</td>\n",
              "      <td>363.382294</td>\n",
              "      <td>1.590683</td>\n",
              "      <td>4.299785</td>\n",
              "      <td>195.785294</td>\n",
              "      <td>14.172410</td>\n",
              "      <td>652.732788</td>\n",
              "      <td>44.709831</td>\n",
              "      <td>1128.829102</td>\n",
              "      <td>76.192490</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.447703</td>\n",
              "      <td>9.137426</td>\n",
              "      <td>24.917759</td>\n",
              "      <td>394.359039</td>\n",
              "      <td>1.554919</td>\n",
              "      <td>4.585690</td>\n",
              "      <td>222.854507</td>\n",
              "      <td>17.064468</td>\n",
              "      <td>598.139343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>21.092251</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170104</td>\n",
              "      <td>2.392559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.318313</td>\n",
              "      <td>11.247286</td>\n",
              "      <td>62.843636</td>\n",
              "      <td>42.457413</td>\n",
              "      <td>39.155308</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>53.753723</td>\n",
              "      <td>0.113000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.020100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.914264</td>\n",
              "      <td>25.441805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>89.454746</td>\n",
              "      <td>118.375000</td>\n",
              "      <td>2.550000</td>\n",
              "      <td>14.532604</td>\n",
              "      <td>91.889584</td>\n",
              "      <td>39.903006</td>\n",
              "      <td>586.162552</td>\n",
              "      <td>185.379574</td>\n",
              "      <td>909.773849</td>\n",
              "      <td>287.069656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>78.985514</td>\n",
              "      <td>80.251501</td>\n",
              "      <td>133.808731</td>\n",
              "      <td>2.749619</td>\n",
              "      <td>13.756427</td>\n",
              "      <td>72.178301</td>\n",
              "      <td>37.803691</td>\n",
              "      <td>542.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>104.993031</td>\n",
              "      <td>233.392815</td>\n",
              "      <td>3.366624</td>\n",
              "      <td>17.536710</td>\n",
              "      <td>164.935104</td>\n",
              "      <td>48.652571</td>\n",
              "      <td>850.095093</td>\n",
              "      <td>213.010849</td>\n",
              "      <td>1285.447998</td>\n",
              "      <td>337.880554</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85.908348</td>\n",
              "      <td>96.869648</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>3.818478</td>\n",
              "      <td>16.744036</td>\n",
              "      <td>141.783073</td>\n",
              "      <td>49.502325</td>\n",
              "      <td>709.535919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>119.365871</td>\n",
              "      <td>534.327866</td>\n",
              "      <td>4.407233</td>\n",
              "      <td>20.364126</td>\n",
              "      <td>288.819977</td>\n",
              "      <td>57.714706</td>\n",
              "      <td>1315.555298</td>\n",
              "      <td>243.944450</td>\n",
              "      <td>2126.954407</td>\n",
              "      <td>387.001335</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>93.025692</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>392.749786</td>\n",
              "      <td>4.693509</td>\n",
              "      <td>19.832062</td>\n",
              "      <td>244.960835</td>\n",
              "      <td>60.872105</td>\n",
              "      <td>1082.120544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>194.763153</td>\n",
              "      <td>3380.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>32.303604</td>\n",
              "      <td>2090.000000</td>\n",
              "      <td>117.949997</td>\n",
              "      <td>5449.500000</td>\n",
              "      <td>448.481049</td>\n",
              "      <td>11965.000000</td>\n",
              "      <td>621.632751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>176.800690</td>\n",
              "      <td>3920.000000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>32.381100</td>\n",
              "      <td>1972.887695</td>\n",
              "      <td>178.303329</td>\n",
              "      <td>4840.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 53 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d11b257f-1822-4837-8c68-1d4df10b62b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d11b257f-1822-4837-8c68-1d4df10b62b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d11b257f-1822-4837-8c68-1d4df10b62b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9f9f14c8-6992-45b2-928c-5092412e4a98\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9f9f14c8-6992-45b2-928c-5092412e4a98')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9f9f14c8-6992-45b2-928c-5092412e4a98 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "wW88GR7baDnw"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "#Normalizer\n",
        "def normalizer(x: int, ):\n",
        "\n",
        "  normalizer_dict = {\n",
        "      1: preprocessing.StandardScaler(),\n",
        "      2: preprocessing.MinMaxScaler(),\n",
        "      3: preprocessing.RobustScaler(),\n",
        "      4: preprocessing.PowerTransformer(),\n",
        "      5: preprocessing.QuantileTransformer(output_distribution='normal', ),\n",
        "  }\n",
        "  return normalizer_dict.get(x)\n",
        "\n",
        "\n",
        "norm_pipeline = Pipeline(steps=[\n",
        "      (\"transformer\", normalizer(5)),\n",
        "      (\"scaler\", normalizer(2)),\n",
        "])\n",
        "\n",
        "# columns_to_scale = ['Ca', 'Cl', 'K', 'Mg', 'Na','SO42', 'TDS',\n",
        "#                     'CO32', 'HCO3', 'ALKT', 'CNDT', 'HARD', 'pH','avg_water_Temp',\n",
        "#                     'avg_air_temp', 'avg_precip', 'avg_discharge', 'avg_stage', 'max_Wx', 'max_Wy',\n",
        "#                     'CCMEWQI']\n",
        "\n",
        "columns_to_scale_x = ['Ca_X', 'Cl_X', 'K_X', 'Mg_X', 'Na_X','SO42_X', 'TDS_X',\n",
        "                      'ALKT_X', 'CNDT_X', 'HARD_X', 'pH_X', 'CO32_X', 'HCO3_X',\n",
        "                      'avg_water_Temp_X', 'avg_discharge_X', 'avg_stage_X',\n",
        "                      'avg_air_temp_X', 'avg_precip_X',\n",
        "                      # 'CCMEWQI_X',\n",
        "                      'Ca_y', 'Cl_y', 'K_y', 'Mg_y', 'Na_y', 'SO42_y', 'TDS_y']\n",
        "categorical_columns = [\n",
        "    # \"major_basin\",\n",
        "    # \"rank\",\n",
        "    # \"date_year\",\n",
        "    # \"date_season\",\n",
        "  ]\n",
        "categories=[\n",
        "    # ['West Don River', 'East Don River', 'Lower Don River'],\n",
        "    # [\"Poor\", \"Marginal\", \"Fair\", \"Good\", \"Excellent\",],\n",
        "    # ['1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974',\n",
        "    #  '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984',\n",
        "    #  '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994',\n",
        "    #  '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',\n",
        "    #  '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014',\n",
        "    #  '2015', '2016', '2017', '2018', '2019', '2020', '2021'],\n",
        "    # [\"winter\",\"spring\", \"summer\", \"autumn\",],\n",
        "    ]\n",
        "# ordinal_encoder = OrdinalEncoder(handle_unknown='error', categories=categories)\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "pipeline = make_pipeline (\n",
        "    ColumnTransformer(\n",
        "        transformers=[\n",
        "            # (\"categorical\", ordinal_encoder, categorical_columns),\n",
        "            (\"one-hot-encoder\", one_hot_encoder, ['major_basin_X']),\n",
        "            (\"numerical\", normalizer(1), columns_to_scale_x)\n",
        "        ],\n",
        "        remainder=\"passthrough\",\n",
        "        verbose_feature_names_out=False,\n",
        "    )\n",
        ").set_output(transform=\"pandas\")\n",
        "\n",
        "# dataset_scaled = pipeline.fit_transform(dataset_to_model)\n",
        "dataset_scaled_x = pipeline.fit_transform(dataset_to_model_x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_scaled_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "BIwOcTpVWYDp",
        "outputId": "038ff5fc-8c86-4d8a-f5da-5219f5e02dbe"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            major_basin_X_East Don River  major_basin_X_West Don River  \\\n",
              "date                                                                     \n",
              "1965-01-31                           0.0                           1.0   \n",
              "1965-02-28                           0.0                           1.0   \n",
              "1965-03-31                           0.0                           1.0   \n",
              "1965-04-30                           0.0                           1.0   \n",
              "1965-05-31                           0.0                           1.0   \n",
              "...                                  ...                           ...   \n",
              "2021-08-31                           1.0                           0.0   \n",
              "2021-09-30                           1.0                           0.0   \n",
              "2021-10-31                           1.0                           0.0   \n",
              "2021-11-30                           1.0                           0.0   \n",
              "2021-12-31                           1.0                           0.0   \n",
              "\n",
              "                Ca_X      Cl_X       K_X      Mg_X      Na_X    SO42_X  \\\n",
              "date                                                                     \n",
              "1965-01-31 -0.086246 -0.556061 -0.098163 -0.560814 -0.644739  0.078290   \n",
              "1965-02-28 -0.418118 -0.289938  1.072168 -0.232686 -0.044370 -1.681405   \n",
              "1965-03-31 -0.697797  1.373693  0.570225 -1.181394  0.717623 -0.906386   \n",
              "1965-04-30 -0.002157  0.906231 -0.383899  1.599306 -0.348399  0.206965   \n",
              "1965-05-31 -0.385303 -0.225698 -1.024740 -1.400408 -0.507641 -0.163641   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2021-08-31  0.275840 -0.386375 -0.359091  1.280498 -0.495232 -0.208449   \n",
              "2021-09-30 -2.708212 -0.802205 -0.620081 -2.854942 -0.815853 -1.863678   \n",
              "2021-10-31 -1.131326 -0.508881 -0.563480 -1.080949 -0.553991 -0.790779   \n",
              "2021-11-30  0.211390 -0.342328 -0.374813  0.710493 -0.451801 -0.000222   \n",
              "2021-12-31  0.340291  0.146316 -0.648381  0.501104  0.043821  0.624459   \n",
              "\n",
              "               TDS_X    ALKT_X  ...  avg_precip_X      Ca_y      Cl_y  \\\n",
              "date                            ...                                     \n",
              "1965-01-31  0.781088 -1.143963  ...      2.042412  2.083869  0.025846   \n",
              "1965-02-28 -1.348616  0.593358  ...      0.775129 -0.533659  0.442964   \n",
              "1965-03-31  1.087932 -0.344984  ...      1.698642  0.423670 -0.134126   \n",
              "1965-04-30  0.520725 -1.545488  ...      0.126516 -0.622532  0.122027   \n",
              "1965-05-31  1.676623  1.837518  ...      0.312919  0.499158  2.525907   \n",
              "...              ...       ...  ...           ...       ...       ...   \n",
              "2021-08-31 -0.399202  0.826915  ...     -0.444555 -0.943215 -0.324993   \n",
              "2021-09-30 -1.050932 -2.148911  ...      2.144143 -1.724737 -0.681228   \n",
              "2021-10-31 -0.604185 -0.111701  ...      0.641133 -1.150639 -0.761134   \n",
              "2021-11-30 -0.281577  1.339293  ...     -0.882906 -0.680923 -0.201540   \n",
              "2021-12-31  0.201569  1.492560  ...     -0.185359  0.455229  0.255063   \n",
              "\n",
              "                 K_y      Mg_y      Na_y    SO42_y     TDS_y   max_Wx_X  \\\n",
              "date                                                                      \n",
              "1965-01-31 -1.570310  1.928149  0.196730 -0.028084 -0.172889  19.193562   \n",
              "1965-02-28  0.640847 -0.995475  0.826922 -0.237556 -0.544599   6.191867   \n",
              "1965-03-31  0.859866  0.450620  1.124707  1.290526  0.453862   8.004473   \n",
              "1965-04-30 -0.000870 -0.045935  0.075672 -0.057534  1.840223  22.366516   \n",
              "1965-05-31 -1.709134  0.251294  0.327740  1.607857  0.144994  25.116323   \n",
              "...              ...       ...       ...       ...       ...        ...   \n",
              "2021-08-31 -0.050399 -0.106444 -0.251992 -1.439031 -0.286482  29.485376   \n",
              "2021-09-30 -0.592962 -2.200679 -0.664370  0.742819 -0.981111  35.004150   \n",
              "2021-10-31  0.024659 -0.379131 -0.254536 -0.105436 -0.347248  25.289892   \n",
              "2021-11-30 -0.001075  0.100798 -0.164758 -0.508566 -0.101396  34.616508   \n",
              "2021-12-31  0.301302  0.711616  0.275152  0.350844  0.631143  43.743031   \n",
              "\n",
              "             max_Wy_X  \n",
              "date                   \n",
              "1965-01-31   7.145782  \n",
              "1965-02-28   2.399321  \n",
              "1965-03-31   4.470599  \n",
              "1965-04-30  10.401231  \n",
              "1965-05-31  11.292035  \n",
              "...               ...  \n",
              "2021-08-31  12.738286  \n",
              "2021-09-30  17.572309  \n",
              "2021-10-31  10.915476  \n",
              "2021-11-30  16.895031  \n",
              "2021-12-31  18.904181  \n",
              "\n",
              "[1368 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9df58e12-6226-4911-8efb-a029ff8ea879\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>major_basin_X_East Don River</th>\n",
              "      <th>major_basin_X_West Don River</th>\n",
              "      <th>Ca_X</th>\n",
              "      <th>Cl_X</th>\n",
              "      <th>K_X</th>\n",
              "      <th>Mg_X</th>\n",
              "      <th>Na_X</th>\n",
              "      <th>SO42_X</th>\n",
              "      <th>TDS_X</th>\n",
              "      <th>ALKT_X</th>\n",
              "      <th>...</th>\n",
              "      <th>avg_precip_X</th>\n",
              "      <th>Ca_y</th>\n",
              "      <th>Cl_y</th>\n",
              "      <th>K_y</th>\n",
              "      <th>Mg_y</th>\n",
              "      <th>Na_y</th>\n",
              "      <th>SO42_y</th>\n",
              "      <th>TDS_y</th>\n",
              "      <th>max_Wx_X</th>\n",
              "      <th>max_Wy_X</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1965-01-31</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.086246</td>\n",
              "      <td>-0.556061</td>\n",
              "      <td>-0.098163</td>\n",
              "      <td>-0.560814</td>\n",
              "      <td>-0.644739</td>\n",
              "      <td>0.078290</td>\n",
              "      <td>0.781088</td>\n",
              "      <td>-1.143963</td>\n",
              "      <td>...</td>\n",
              "      <td>2.042412</td>\n",
              "      <td>2.083869</td>\n",
              "      <td>0.025846</td>\n",
              "      <td>-1.570310</td>\n",
              "      <td>1.928149</td>\n",
              "      <td>0.196730</td>\n",
              "      <td>-0.028084</td>\n",
              "      <td>-0.172889</td>\n",
              "      <td>19.193562</td>\n",
              "      <td>7.145782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965-02-28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.418118</td>\n",
              "      <td>-0.289938</td>\n",
              "      <td>1.072168</td>\n",
              "      <td>-0.232686</td>\n",
              "      <td>-0.044370</td>\n",
              "      <td>-1.681405</td>\n",
              "      <td>-1.348616</td>\n",
              "      <td>0.593358</td>\n",
              "      <td>...</td>\n",
              "      <td>0.775129</td>\n",
              "      <td>-0.533659</td>\n",
              "      <td>0.442964</td>\n",
              "      <td>0.640847</td>\n",
              "      <td>-0.995475</td>\n",
              "      <td>0.826922</td>\n",
              "      <td>-0.237556</td>\n",
              "      <td>-0.544599</td>\n",
              "      <td>6.191867</td>\n",
              "      <td>2.399321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965-03-31</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.697797</td>\n",
              "      <td>1.373693</td>\n",
              "      <td>0.570225</td>\n",
              "      <td>-1.181394</td>\n",
              "      <td>0.717623</td>\n",
              "      <td>-0.906386</td>\n",
              "      <td>1.087932</td>\n",
              "      <td>-0.344984</td>\n",
              "      <td>...</td>\n",
              "      <td>1.698642</td>\n",
              "      <td>0.423670</td>\n",
              "      <td>-0.134126</td>\n",
              "      <td>0.859866</td>\n",
              "      <td>0.450620</td>\n",
              "      <td>1.124707</td>\n",
              "      <td>1.290526</td>\n",
              "      <td>0.453862</td>\n",
              "      <td>8.004473</td>\n",
              "      <td>4.470599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965-04-30</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.002157</td>\n",
              "      <td>0.906231</td>\n",
              "      <td>-0.383899</td>\n",
              "      <td>1.599306</td>\n",
              "      <td>-0.348399</td>\n",
              "      <td>0.206965</td>\n",
              "      <td>0.520725</td>\n",
              "      <td>-1.545488</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126516</td>\n",
              "      <td>-0.622532</td>\n",
              "      <td>0.122027</td>\n",
              "      <td>-0.000870</td>\n",
              "      <td>-0.045935</td>\n",
              "      <td>0.075672</td>\n",
              "      <td>-0.057534</td>\n",
              "      <td>1.840223</td>\n",
              "      <td>22.366516</td>\n",
              "      <td>10.401231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965-05-31</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.385303</td>\n",
              "      <td>-0.225698</td>\n",
              "      <td>-1.024740</td>\n",
              "      <td>-1.400408</td>\n",
              "      <td>-0.507641</td>\n",
              "      <td>-0.163641</td>\n",
              "      <td>1.676623</td>\n",
              "      <td>1.837518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.312919</td>\n",
              "      <td>0.499158</td>\n",
              "      <td>2.525907</td>\n",
              "      <td>-1.709134</td>\n",
              "      <td>0.251294</td>\n",
              "      <td>0.327740</td>\n",
              "      <td>1.607857</td>\n",
              "      <td>0.144994</td>\n",
              "      <td>25.116323</td>\n",
              "      <td>11.292035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-08-31</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.275840</td>\n",
              "      <td>-0.386375</td>\n",
              "      <td>-0.359091</td>\n",
              "      <td>1.280498</td>\n",
              "      <td>-0.495232</td>\n",
              "      <td>-0.208449</td>\n",
              "      <td>-0.399202</td>\n",
              "      <td>0.826915</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.444555</td>\n",
              "      <td>-0.943215</td>\n",
              "      <td>-0.324993</td>\n",
              "      <td>-0.050399</td>\n",
              "      <td>-0.106444</td>\n",
              "      <td>-0.251992</td>\n",
              "      <td>-1.439031</td>\n",
              "      <td>-0.286482</td>\n",
              "      <td>29.485376</td>\n",
              "      <td>12.738286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-09-30</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.708212</td>\n",
              "      <td>-0.802205</td>\n",
              "      <td>-0.620081</td>\n",
              "      <td>-2.854942</td>\n",
              "      <td>-0.815853</td>\n",
              "      <td>-1.863678</td>\n",
              "      <td>-1.050932</td>\n",
              "      <td>-2.148911</td>\n",
              "      <td>...</td>\n",
              "      <td>2.144143</td>\n",
              "      <td>-1.724737</td>\n",
              "      <td>-0.681228</td>\n",
              "      <td>-0.592962</td>\n",
              "      <td>-2.200679</td>\n",
              "      <td>-0.664370</td>\n",
              "      <td>0.742819</td>\n",
              "      <td>-0.981111</td>\n",
              "      <td>35.004150</td>\n",
              "      <td>17.572309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-31</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.131326</td>\n",
              "      <td>-0.508881</td>\n",
              "      <td>-0.563480</td>\n",
              "      <td>-1.080949</td>\n",
              "      <td>-0.553991</td>\n",
              "      <td>-0.790779</td>\n",
              "      <td>-0.604185</td>\n",
              "      <td>-0.111701</td>\n",
              "      <td>...</td>\n",
              "      <td>0.641133</td>\n",
              "      <td>-1.150639</td>\n",
              "      <td>-0.761134</td>\n",
              "      <td>0.024659</td>\n",
              "      <td>-0.379131</td>\n",
              "      <td>-0.254536</td>\n",
              "      <td>-0.105436</td>\n",
              "      <td>-0.347248</td>\n",
              "      <td>25.289892</td>\n",
              "      <td>10.915476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-30</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.211390</td>\n",
              "      <td>-0.342328</td>\n",
              "      <td>-0.374813</td>\n",
              "      <td>0.710493</td>\n",
              "      <td>-0.451801</td>\n",
              "      <td>-0.000222</td>\n",
              "      <td>-0.281577</td>\n",
              "      <td>1.339293</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.882906</td>\n",
              "      <td>-0.680923</td>\n",
              "      <td>-0.201540</td>\n",
              "      <td>-0.001075</td>\n",
              "      <td>0.100798</td>\n",
              "      <td>-0.164758</td>\n",
              "      <td>-0.508566</td>\n",
              "      <td>-0.101396</td>\n",
              "      <td>34.616508</td>\n",
              "      <td>16.895031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-31</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.340291</td>\n",
              "      <td>0.146316</td>\n",
              "      <td>-0.648381</td>\n",
              "      <td>0.501104</td>\n",
              "      <td>0.043821</td>\n",
              "      <td>0.624459</td>\n",
              "      <td>0.201569</td>\n",
              "      <td>1.492560</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.185359</td>\n",
              "      <td>0.455229</td>\n",
              "      <td>0.255063</td>\n",
              "      <td>0.301302</td>\n",
              "      <td>0.711616</td>\n",
              "      <td>0.275152</td>\n",
              "      <td>0.350844</td>\n",
              "      <td>0.631143</td>\n",
              "      <td>43.743031</td>\n",
              "      <td>18.904181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1368 rows × 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9df58e12-6226-4911-8efb-a029ff8ea879')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9df58e12-6226-4911-8efb-a029ff8ea879 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9df58e12-6226-4911-8efb-a029ff8ea879');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a1dc500c-72ea-4ae2-a069-b05cced8e266\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a1dc500c-72ea-4ae2-a069-b05cced8e266')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a1dc500c-72ea-4ae2-a069-b05cced8e266 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ggplot(dataset_scaled_x, aes(y='Cl_y')) + geom_violin(quantiles=[.25, .5, .75], quantile_lines=True) + ggtitle(\"Simplest example\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "IZs2vTCfmAOz",
        "outputId": "28cdae27-8a3d-4ca5-f321-5a32537d0255"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lets_plot.plot.core.PlotSpec at 0x7d6bb8eb12a0>"
            ],
            "text/html": [
              "<html lang=\"en\">\n",
              "   <head>\n",
              "       <script type=\"text/javascript\" data-lets-plot-script=\"library\" src=\"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.0/js-package/distr/lets-plot.min.js\"></script>\n",
              "   </head>\n",
              "   <body>\n",
              "          <div id=\"bdqOfQ\"></div>\n",
              "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
              "       var plotSpec={\n",
              "\"data\":{\n",
              "},\n",
              "\"mapping\":{\n",
              "\"y\":\"Cl_y\"\n",
              "},\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"ggtitle\":{\n",
              "\"text\":\"Simplest example\"\n",
              "},\n",
              "\"kind\":\"plot\",\n",
              "\"scales\":[],\n",
              "\"layers\":[{\n",
              "\"geom\":\"violin\",\n",
              "\"mapping\":{\n",
              "},\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"quantiles\":[0.25,0.5,0.75],\n",
              "\"quantile_lines\":true,\n",
              "\"data\":{\n",
              "\"..x..\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
              "\"..quantile..\":[0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\n",
              "\"Cl_y\":[-0.8737627267837524,-0.8543032046166885,-0.8348436824496245,-0.8153841602825606,-0.7959246381154965,-0.7764651159484326,-0.7570055937813686,-0.7375460716143046,-0.7180865494472406,-0.6986270272801767,-0.6791675051131127,-0.6597079829460487,-0.6402484607789848,-0.6207889386119207,-0.6013294164448568,-0.5818698942777929,-0.5624103721107289,-0.5429508499436648,-0.5429508499436648,-0.5234913277766009,-0.504031805609537,-0.48457228344247294,-0.465112761275409,-0.44565323910834503,-0.42619371694128105,-0.4067341947742171,-0.3872746726071531,-0.36781515044008917,-0.34835562827302513,-0.34835562827302513,-0.3288961061059612,-0.30943658393889717,-0.28997706177183324,-0.2705175396047692,-0.2510580174377053,-0.23159849527064136,-0.21213897310357732,-0.1926794509365134,-0.17321992876944936,-0.15376040660238544,-0.1343008844353214,-0.11484136226825747,-0.09538184010119344,-0.07592231793412951,-0.05646279576706559,-0.03700327360000155,-0.017543751432937627,0.0019157707341264096,0.021375292901190335,0.04083481506825437,0.060294337235318296,0.07975385940238222,0.09921338156944626,0.11867290373651018,0.11867290373651018,0.1381324259035741,0.15759194807063825,0.17705147023770218,0.1965109924047661,0.21597051457183003,0.23543003673889396,0.2548895589059581,0.274349081073022,0.29380860324008595,0.3132681254071499,0.332727647574214,0.35218716974127795,0.3716466919083419,0.3911062140754058,0.4105657362424697,0.4300252584095339,0.4494847805765978,0.4689443027436617,0.48840382491072565,0.5078633470777898,0.5273228692448537,0.5467823914119176,0.5662419135789816,0.5857014357460455,0.6051609579131096,0.6246204800801736,0.6440800022472375,0.6635395244143014,0.6829990465813656,0.7024585687484295,0.7219180909154934,0.7413776130825573,0.7608371352496213,0.7802966574166854,0.7997561795837493,0.8192157017508133,0.8386752239178772,0.8581347460849411,0.8775942682520053,0.8970537904190692,0.9165133125861331,0.935972834753197,0.9554323569202612,0.9748918790873251,0.994351401254389,1.013810923421453,1.0332704455885169,1.052729967755581,1.072189489922645,1.0916490120897089,1.1111085342567728,1.130568056423837,1.1500275785909007,1.1694871007579648,1.188946622925029,1.2084061450920927,1.2278656672591568,1.2473251894262205,1.2667847115932847,1.2862442337603488,1.3057037559274125,1.3251632780944766,1.3446228002615404,1.3640823224286045,1.3835418445956686,1.4030013667627323,1.4224608889297965,1.4419204110968606,1.4613799332639243,1.4808394554309885,1.5002989775980522,1.5197584997651163,1.5392180219321805,1.5586775440992442,1.5781370662663083,1.597596588433372,1.6170561106004362,1.6365156327675003,1.655975154934564,1.6754346771016282,1.694894199268692,1.714353721435756,1.7338132436028202,1.753272765769884,1.772732287936948,1.7921918101040117,1.8116513322710759,1.83111085443814,1.8505703766052037,1.8700298987722679,1.889489420939332,1.9089489431063957,1.9284084652734599,1.9478679874405236,1.9673275096075877,1.9867870317746519,2.0062465539417156,2.0257060761087797,2.0451655982758434,2.0646251204429076,2.0840846426099717,2.1035441647770354,2.1230036869440996,2.1424632091111633,2.1619227312782274,2.1813822534452916,2.2008417756123553,2.2203012977794194,2.2397608199464836,2.2592203421135473,2.2786798642806114,2.298139386447675,2.3175989086147393,2.3370584307818034,2.356517952948867,2.3759774751159313,2.395436997282995,2.414896519450059,2.4343560416171233,2.453815563784187,2.473275085951251,2.492734608118315,2.512194130285379,2.531653652452443,2.551113174619507,2.570572696786571,2.5900322189536347,2.609491741120699,2.628951263287763,2.6484107854548267,2.667870307621891,2.687329829788955,2.7067893519560187,2.726248874123083,2.7457083962901465,2.7651679184572107,2.784627440624275,2.8040869627913385,2.8235464849584027,2.8430060071254664,2.8624655292925305,2.8819250514595947,2.9013845736266584,2.9208440957937225,2.940303617960786,2.9597631401278504,2.9792226622949145,2.998682184461978,3.0181417066290424,3.037601228796106,3.05706075096317,3.0765202731302344,3.095979795297298,3.115439317464362,3.1348988396314263,3.15435836179849,3.1738178839655538,3.1932774061326183,3.212736928299682,3.2321964504667458,3.2516559726338103,3.271115494800874,3.2905750169679377,3.3100345391350023,3.329494061302066,3.3489535834691297,3.3684131056361934,3.387872627803258,3.4073321499703217,3.4267916721373854,3.44625119430445,3.4657107164715137,3.4851702386385774,3.504629760805642,3.5240892829727057,3.5435488051397694,3.563008327306833,3.5824678494738977,3.6019273716409614,3.621386893808025,3.6408464159750897,3.6603059381421534,3.679765460309217,3.6992249824762817,3.7186845046433454,3.738144026810409,3.7576035489774737,3.7770630711445374,3.796522593311601,3.815982115478665,3.8354416376457294,3.854901159812793,3.874360681979857,3.8938202041469214,3.913279726313985,3.932739248481049,3.9521987706481134,3.971658292815177,3.991117814982241,4.0105773371493045,4.030036859316369,4.049496381483433,4.0689559036504965,4.088415425817561,4.107874947984625,4.1273344701516885,4.146793992318753,4.166253514485817,4.1857130366528805,4.205172558819945,4.224632080987009,4.2440916031540725,4.263551125321136,4.283010647488201,4.3024701696552645,4.321929691822328,4.341389213989393,4.3608487361564565,4.38030825832352,4.399767780490585,4.4192273026576485,4.438686824824712,4.458146346991776,4.4776058691588405,4.497065391325904,4.516524913492968,4.5359844356600325,4.555443957827096,4.57490347999416,4.5943630021612245,4.613822524328288,4.633282046495352,4.6527415686624165,4.67220109082948,4.691660612996544,4.711120135163608,4.730579657330672,4.750039179497736,4.7694987016648,4.788958223831864,4.808417745998928,4.827877268165992,4.847336790333056,4.86679631250012,4.886255834667184,4.905715356834247,4.925174879001312,4.944634401168376,4.964093923335439,4.983553445502504,5.003012967669568,5.022472489836631,5.041932012003696,5.06139153417076,5.080851056337823,5.100310578504888,5.119770100671952,5.139229622839015,5.158689145006079,5.178148667173144,5.197608189340207,5.217067711507271,5.236527233674336,5.255986755841399,5.275446278008463,5.294905800175528,5.314365322342591,5.333824844509655,5.35328436667672,5.372743888843783,5.392203411010847,5.411662933177911,5.431122455344975,5.450581977512039,5.470041499679103,5.489501021846167,5.508960544013231,5.528420066180295,5.547879588347359,5.567339110514423,5.586798632681487,5.60625815484855,5.625717677015615,5.645177199182679,5.664636721349742,5.684096243516807,5.703555765683871,5.723015287850934,5.742474810017999,5.761934332185063,5.781393854352126,5.800853376519191,5.820312898686255,5.839772420853318,5.859231943020382,5.878691465187447,5.89815098735451,5.917610509521574,5.937070031688639,5.956529553855702,5.975989076022766,5.995448598189831,6.014908120356894,6.034367642523958,6.053827164691022,6.073286686858086,6.09274620902515,6.112205731192214,6.131665253359278,6.151124775526342,6.170584297693406,6.19004381986047,6.209503342027534,6.228962864194598,6.248422386361662,6.267881908528726,6.28734143069579,6.3068009528628535,6.326260475029918,6.345719997196982,6.3651795193640455,6.38463904153111,6.404098563698174,6.4235580858652375,6.443017608032302,6.462477130199366,6.4819366523664295,6.501396174533493,6.520855696700558,6.5403152188676215,6.559774741034685,6.57923426320175,6.5986937853688135,6.618153307535877,6.637612829702942,6.6570723518700055,6.676531874037069,6.695991396204134,6.7154509183711975,6.734910440538261,6.754369962705325,6.7738294848723895,6.793289007039453,6.812748529206517,6.8322080513735814,6.851667573540645,6.871127095707709,6.8905866178747734,6.910046140041837,6.929505662208901,6.9489651843759646,6.968424706543029,6.987884228710093,7.0073437508771566,7.026803273044221,7.046262795211285,7.0657223173783485,7.085181839545413,7.104641361712477,7.1241008838795405,7.143560406046605,7.163019928213668,7.1824794503807325,7.201938972547797,7.22139849471486,7.2408580168819245,7.260317539048989,7.279777061216052,7.2992365833831165,7.318696105550181,7.338155627717244,7.3576151498843085,7.377074672051373,7.396534194218436,7.4159937163855005,7.435453238552565,7.454912760719628,7.4743722828866925,7.493831805053757,7.51329132722082,7.5327508493878845,7.552210371554947,7.571669893722012,7.5911294158890765,7.610588938056139,7.630048460223204,7.6495079823902685,7.668967504557331,7.688427026724396,7.7078865488914605,7.727346071058523,7.746805593225588,7.7662651153926525,7.785724637559715,7.80518415972678,7.8246436818938445,7.844103204060907,7.863562726227972,7.8830222483950365,7.902481770562099,7.921941292729164,7.9414008148962285,7.960860337063291,7.980319859230356,7.999779381397419,8.019238903564483,8.038698425731548,8.05815794789861,8.077617470065675,8.09707699223274,8.116536514399803,8.135996036566867,8.155455558733932,8.174915080900995,8.19437460306806,8.213834125235124,8.233293647402187,8.252753169569251,8.272212691736316,8.291672213903379,8.311131736070443,8.330591258237508,8.35005078040457,8.369510302571635,8.3889698247387,8.408429346905763,8.427888869072827,8.44734839123989,8.466807913406955,8.48626743557402,8.505726957741082,8.525186479908147,8.544646002075211,8.564105524242274,8.583565046409339,8.603024568576403,8.622484090743466,8.64194361291053,8.661403135077595,8.680862657244658,8.700322179411723,8.719781701578787,8.73924122374585,8.758700745912915,8.77816026807998,8.797619790247042,8.817079312414107,8.836538834581171,8.855998356748234,8.875457878915299,8.894917401082362,8.914376923249426,8.93383644541649,8.953295967583554,8.972755489750618,8.992215011917683,9.011674534084746,9.03113405625181,9.050593578418875,9.070053100585938],\n",
              "\"..violinwidth..\":[0.1218702842161008,0.143905023352906,0.16945446447905987,0.19940698605123422,0.23470079957921386,0.2761847759718211,0.324455892215744,0.3796975358798937,0.44154551885068594,0.5090065469356442,0.5804474013642535,0.6536632865110075,0.7260222460958683,0.7946710412219619,0.8567782218244094,0.9097839081393336,0.9516241885610011,0.9809014994628615,0.9809014994628615,0.9969804562382826,1.0,0.990805329794892,0.970814531328277,0.9418429855552385,0.9059122229154211,0.8650686335735032,0.8212321526030506,0.776087257048008,0.7310201691076993,0.7310201691076993,0.6870987194361202,0.6450860610375404,0.6054768756343173,0.5685447902375416,0.5343918192780808,0.5029938387843662,0.47423936299818287,0.44796136994937363,0.4239631328873337,0.40203898512239433,0.38199016549964115,0.3636350969533059,0.34681330285628875,0.33138294946758273,0.3172134817402739,0.30417633279323186,0.29213745189362045,0.280954888198524,0.27048290060884883,0.2605816241527066,0.2511291394941405,0.24203173945456877,0.23322870348449967,0.22468978772591497,0.22468978772591497,0.21640617864862255,0.20837787594302898,0.20060157022150765,0.19306274616718025,0.18573421772768647,0.17858123848811952,0.17157147145982318,0.16468697945676403,0.1579351632409558,0.15135603891314883,0.1450240606296459,0.13904357476830476,0.13353784400694507,0.12863247663559801,0.12443512446418203,0.12101440211031785,0.11838183245125018,0.11648079942412232,0.11518563418619697,0.11431204052369658,0.11363749049471789,0.11292775848945351,0.11196425469298199,0.11056683247423635,0.10860830281154096,0.10601943852782467,0.10278587485136909,0.09894010446547631,0.09455219169257434,0.08972192130986997,0.08457338750018364,0.07925129692161627,0.07391719718488546,0.06874379078396979,0.06390636092677611,0.05957168055409994,0.0558860216456474,0.052964540913601414,0.05088415781965254,0.04968114287299816,0.04935334076366761,0.049865732519559974,0.05115731889872634,0.053147317602486234,0.0557393735760792,0.058823597059664307,0.06227733872308861,0.0659662746593316,0.06974736581989698,0.07347459088316939,0.07700728542671093,0.08021985287472563,0.08301093774047928,0.0853101166636098,0.08708078338450201,0.08831896015033354,0.08904888134886722,0.08931696019458804,0.08918587989566383,0.08872999015526878,0.0880321450492153,0.08718099952307082,0.08626706142563156,0.08537583328541933,0.08457727281177228,0.0839123345581232,0.08337904320940383,0.08292179269156078,0.08242784852232328,0.08173409895642958,0.08064506386728798,0.07896049804142878,0.0765083270998598,0.07317687752962217,0.0689399642910324,0.06386956133444134,0.05813323443786889,0.051976619645224154,0.04569417052873214,0.039593449842826055,0.03395899277444889,0.029021214470815147,0.024934321047997397,0.02176524063773333,0.019493727845471185,0.01802234613406494,0.017194107519217742,0.01681507901593912,0.016679107426281477,0.016591851955132488,0.016391512325397584,0.01596403575194223,0.01525123399255407,0.014251146251729616,0.013011056075251085,0.011614631983137048,0.010165494614519153,0.008769934906017783,0.007521435279351195,0.006489121510946405,0.005711442546847662,0.005195430840800823,0.004921012575433492,0.004849135670341072,0.004932024316795117,0.005123674180368576,0.005388780718131554,0.005708644601031285,0.006083197448586536,0.006529058861066235,0.0070743269182174315,0.007751430054783109,0.008589653901480686,0.009608813692588253,0.010815019460518766,0.012198762222639294,0.013734895376485706,0.015383732186944735,0.017092539011385015,0.018797110590587966,0.020423652418235107,0.02189159117806146,0.02311797616466658,0.024023769752539545,0.024541682343937672,0.024624537688382478,0.024252718026702806,0.023439194310175476,0.022230997815209194,0.02070660328910357,0.01896937922369479,0.017137855474293293,0.015333980248949238,0.013670782941334761,0.012240955144676496,0.011107821913091238,0.010299973728551152,0.009810423632180288,0.009600530008205405,0.009608144133504588,0.009758650323571781,0.009976967686829274,0.010198365882303568,0.010376214257685926,0.01048549643887692,0.0105219019144947,0.01049728473611007,0.010432989291157522,0.010352806961766138,0.010277115596739914,0.010219184540816028,0.010183914558381784,0.01016864897447006,0.010165298995872487,0.010162925344427353,0.01015006211695444,0.010116349934623488,0.010053350389222945,0.0099546625990776,0.009815622323384935,0.009632936575184972,0.009404605362273182,0.009130412740749563,0.008813126887626142,0.008460331866535066,0.008086545015740831,0.007715013587492827,0.0073784227264955225,0.007117775413862925,0.006978975371030673,0.007007134855543178,0.007239237646578628,0.007696351325495739,0.008376929704537614,0.009252749123848809,0.010268643890071316,0.011346514532724106,0.01239323540392584,0.0133112895112052,0.014010402295406446,0.014418262564503431,0.014488639644142945,0.014205758727806753,0.013584530909878493,0.012666969886567035,0.011515706690913277,0.010205843495053508,0.008816451221453935,0.007422860943062873,0.0060906061797244,0.004871523003514402,0.00380216751282115,0.0029044002256620005,0.0021877301354730795,0.0016528157271493177,0.0012953947385169766,0.0011098707417868078,0.0010918366801373437,0.0012389757639099139,0.0015500534609762403,0.002022088573707604,0.0026462258175202376,0.0034032492029630883,0.004259964381153191,0.0051677197473118655,0.006064045648442064,0.006877766425015173,0.007537092679307812,0.007979346847966794,0.008160376547600275,0.008061586093069674,0.007692952758355821,0.007091296114212207,0.0063141819796008365,0.005430838882317256,0.004512058442060835,0.0036210970666416703,0.0028071327398772506,0.002102048438098992,0.0015204743431516297,0.0010623598783945207,7.170013165678716E-4,4.674374177030265E-4,2.9436195861723657E-4,1.7905811340777756E-4,1.0521067826959185E-4,5.9714319741547764E-5,3.273779217136882E-5,1.7336929806395443E-5,8.868467165095739E-6,4.382201428107207E-6,2.0921412506804226E-6,9.662206607253774E-7,4.3484739699036176E-7,1.989300414848217E-7,1.1246670283174157E-7,1.1738668277251654E-7,2.1463677946055324E-7,4.628295658939072E-7,1.0029035118326216E-6,2.113733706831345E-6,4.3090785888859946E-6,8.489703149527322E-6,1.616411701496716E-5,2.9744368780201873E-5,5.290680400059178E-5,9.097990776504453E-5,1.5128386993105885E-4,2.4330618121076478E-4,3.7856737635697435E-4,5.700284280465691E-4,8.309331540801304E-4,0.0011730739645760063,0.0016046098605449674,0.002127724203151724,0.002736542319203823,0.003415787271206055,0.004140601521585178,0.004877797744566051,0.005588554115409807,0.006232295339896955,0.006771264418188563,0.007175139913459787,0.007425008892847066,0.0075160603551737825,0.007458500634151521,0.007276398746726717,0.007004442715759169,0.006682920683311853,0.006351601091031681,0.006043503033402852,0.00577971929827381,0.0055663832944660135,0.005394512534404436,0.005242861628417252,0.005083217860045835,0.004886970803273432,0.0046314720143614905,0.004304777832396855,0.0039078189741635,0.003453725067310362,0.0029647399758233237,0.0024676891420853473,0.001989172354452961,0.0015515351009179822,0.0011703040196301435,8.533027458372934E-4,6.01242351309206E-4,4.093087362984461E-4,2.6918275900837457E-4,1.7100015273365746E-4,1.049227989013292E-4,6.217951141994187E-5,3.558892912001476E-5,1.96726597083188E-5,1.0502316521738953E-5,5.414724378859322E-6,2.696085240729974E-6,1.2964466863444321E-6,6.020574488007524E-7,2.70011560125195E-7,1.169464887811751E-7,4.8916108655987916E-8,1.9759508574386563E-8,7.708318505446909E-9,2.9040377840469533E-9,1.056583568089506E-9,3.712481824893603E-10,1.2597479368816968E-10,4.1282098949355436E-11,1.3064676246406754E-11,3.9929537586207475E-12,1.1785517120430423E-12,3.3594000897377564E-13,9.247695757902601E-14,2.458465114950362E-14,6.311800691104415E-15,1.5649526815677922E-15,3.74720843119689E-16,8.665092344609982E-17,1.935072514699885E-17,4.1733037633616706E-18,8.692034728332676E-19,1.7483227615916787E-19,3.396100028672907E-20,6.370859653453348E-21,1.1541820212851972E-21,2.0193391938510238E-22,3.411952267909004E-23,5.567437131811815E-24,8.773369061960647E-25,1.335168810057836E-25,1.9622966422012822E-26,2.785170876901295E-27,3.8176642617545977E-28,5.053616966652562E-29,6.460492311187451E-30,7.976045240436393E-31,9.509733119868627E-32,1.0949894830482109E-32,1.2181953702388288E-33,1.3674216398135331E-34,7.200176058117723E-35,5.523810299868273E-34,5.017533724582898E-33,4.412117415184931E-32,3.7469133970962454E-31,3.0729753030943603E-30,2.4339025642957842E-29,1.8616841833415794E-28,1.3752052836780039E-27,9.810423554214516E-27,6.758754723525113E-26,4.4968077076719535E-25,2.8893533040391264E-24,1.7928984228202154E-23,1.0744084692709673E-22,6.217873230948157E-22,3.475145608893676E-21,1.8756977244830113E-20,9.777130214011146E-20,4.921739794267635E-19,2.392679904151329E-18,1.123334913509182E-17,5.0932220965777554E-17,2.230153080102564E-16,9.43051558014764E-16,3.85118961426379E-15,1.5188436472346424E-14,5.784821182038177E-14,2.1277740644525137E-13,7.558224466378014E-13,2.5928223560744303E-12,8.58982749436467E-12,2.7482410616527428E-11,8.491490118760977E-11,2.53379628720535E-10,7.301602475214295E-10,2.0319984695217304E-9,5.461189803790648E-9,1.4174569163429757E-8,3.5529669402320955E-8,8.600647889576897E-8,2.0106194156003463E-7,4.5392831979167853E-7,9.896995831606522E-7,2.0839063531642165E-6,4.237519526769398E-6,8.32154428471335E-6,1.5781738661560724E-5,2.8904433931682578E-5,5.112493773480143E-5,8.732926764242673E-5,1.4406070858065812E-4,2.2950385112645508E-4,3.530962545708975E-4,5.246322050655513E-4,7.527928489612158E-4,0.0010431691274050913,0.0013960233062531595,0.0018042192591285115,0.0022518769857163136,0.00271430541386831,0.0031595953490224935,0.0035519179198957728,0.0038561425411271976,0.004042983224950211,0.004093638628680345,0.004002909461618178,0.0037800777695988267,0.0034473422733617524,0.003036174843066283,0.0025824258031835225,0.002121229361499472,0.0016826977334236058,0.0012890901562234269,9.537162375517111E-4,6.814182549905939E-4,4.701831418193836E-4,3.1331345502971596E-4,2.0162748840898878E-4,1.2530809782656548E-4,7.520855382209611E-5,4.359272772132137E-5,2.4401671806571306E-5,1.3191190388670588E-5,6.8866518793846225E-6,3.4721408175097333E-6,1.6907694035194017E-6,7.955566890163825E-7,3.627411461613936E-7,1.6305957997535678E-7,7.944203091930584E-8,5.864794536060342E-8,8.67959871750035E-8,1.8260403698595435E-7,4.0646229563948236E-7,8.878405666554963E-7,1.877806920702952E-6,3.837173957747717E-6,7.572877603707183E-6,1.4433577618013454E-5,2.6567246474746183E-5,4.722564840445006E-5,8.107146807406734E-5,1.344054413524679E-4,2.1519113102492886E-4,3.3272894851609354E-4,4.96838806001189E-4,7.164718532589731E-4,9.977953150706298E-4,0.0013419690188254368,0.001743019401743616,0.0021863548760071762,0.00264848667307704,0.0030983725112614124,0.003500484608150676,0.0038192792754369524,0.004024327927990852,0.004095095284503327]\n",
              "}\n",
              "}],\n",
              "\"metainfo_list\":[]\n",
              "};\n",
              "       var plotContainer = document.getElementById(\"bdqOfQ\");\n",
              "       LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
              "   </script>\n",
              "   </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iW47cpuaHDp"
      },
      "outputs": [],
      "source": [
        "# #@title Plot Transformed Data\n",
        "\n",
        "# def plot_histogram(data: pd.DataFrame):\n",
        "#     data = data.melt(var_name='param',\n",
        "#                      value_name='value',\n",
        "#                      ignore_index=False)\n",
        "\n",
        "#     return ggplot(data, aes(x='value')) + geom_histogram(aes(y='..density..')) \\\n",
        "#         + geom_density(alpha=.2, color=\"#de2d26\", fill=\"#ff6666\") \\\n",
        "#         + facet_wrap(facets='param', ncol=3, order=1, scales='free', dir='h', )\n",
        "\n",
        "# plot_histogram(dataset_scaled[columns_to_scale])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku9A09XvapXS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "outputId": "565805a4-6649-449f-eefc-b4b604c89c2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lets_plot.plot.core.PlotSpec at 0x7a7350626d10>"
            ],
            "text/html": [
              "<html lang=\"en\">\n",
              "   <head>\n",
              "       <script type=\"text/javascript\" data-lets-plot-script=\"library\" src=\"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.0/js-package/distr/lets-plot.min.js\"></script>\n",
              "   </head>\n",
              "   <body>\n",
              "          <div id=\"xxYKRt\"></div>\n",
              "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
              "       var plotSpec={\n",
              "\"data\":{\n",
              "},\n",
              "\"kind\":\"plot\",\n",
              "\"scales\":[{\n",
              "\"aesthetic\":\"size\",\n",
              "\"scale_mapper_kind\":\"identity\",\n",
              "\"na_value\":0,\n",
              "\"guide\":\"none\"\n",
              "},{\n",
              "\"aesthetic\":\"x\",\n",
              "\"discrete\":true,\n",
              "\"breaks\":[\"Ca\",\"Cl\",\"K\",\"Mg\",\"Na\",\"SO42\",\"TDS\",\"CO32\",\"HCO3\",\"ALKT\",\"CNDT\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"limits\":[\"Ca\",\"Cl\",\"K\",\"Mg\",\"Na\",\"SO42\",\"TDS\",\"CO32\",\"HCO3\",\"ALKT\",\"CNDT\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"expand\":[0.0,0.0]\n",
              "},{\n",
              "\"aesthetic\":\"y\",\n",
              "\"discrete\":true,\n",
              "\"breaks\":[\"Ca\",\"Cl\",\"K\",\"Mg\",\"Na\",\"SO42\",\"TDS\",\"CO32\",\"HCO3\",\"ALKT\",\"CNDT\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"limits\":[\"CCMEWQI\",\"max_Wy\",\"max_Wx\",\"avg_stage\",\"avg_discharge\",\"avg_precip\",\"avg_air_temp\",\"avg_water_Temp\",\"pH\",\"HARD\",\"CNDT\",\"ALKT\",\"HCO3\",\"CO32\",\"TDS\",\"SO42\",\"Na\",\"Mg\",\"K\",\"Cl\",\"Ca\"],\n",
              "\"expand\":[0.0,0.0]\n",
              "},{\n",
              "\"aesthetic\":\"color\",\n",
              "\"name\":\"\",\n",
              "\"breaks\":[-1.0,-0.5,0.0,0.5,1.0],\n",
              "\"limits\":[-1.0,1.0],\n",
              "\"na_value\":\"rgba(0,0,0,0)\",\n",
              "\"scale_mapper_kind\":\"color_gradient2\",\n",
              "\"low\":\"#B3412C\",\n",
              "\"mid\":\"#EDEDED\",\n",
              "\"high\":\"#326C81\"\n",
              "},{\n",
              "\"aesthetic\":\"fill\",\n",
              "\"name\":\"\",\n",
              "\"breaks\":[-1.0,-0.5,0.0,0.5,1.0],\n",
              "\"limits\":[-1.0,1.0],\n",
              "\"na_value\":\"rgba(0,0,0,0)\",\n",
              "\"scale_mapper_kind\":\"color_gradient2\",\n",
              "\"low\":\"#B3412C\",\n",
              "\"mid\":\"#EDEDED\",\n",
              "\"high\":\"#326C81\"\n",
              "}],\n",
              "\"layers\":[{\n",
              "\"na_text\":\"\",\n",
              "\"label_format\":\".2f\",\n",
              "\"show_legend\":true,\n",
              "\"tooltips\":{\n",
              "\"lines\":[\"@corr\"],\n",
              "\"formats\":[{\n",
              "\"field\":\"@corr\",\n",
              "\"format\":\".2f\"\n",
              "}]\n",
              "},\n",
              "\"sampling\":{\n",
              "\"name\":\"none\"\n",
              "},\n",
              "\"geom\":\"tile\",\n",
              "\"data\":{\n",
              "\"x\":[\"Ca\",\"Ca\",\"Ca\",\"Ca\",\"Ca\",\"Cl\",\"Cl\",\"Cl\",\"Cl\",\"Cl\",\"K\",\"Mg\",\"Mg\",\"Mg\",\"Mg\",\"Na\",\"Na\",\"Na\",\"Na\",\"SO42\",\"TDS\",\"TDS\",\"CO32\",\"CO32\",\"HCO3\",\"HCO3\",\"HCO3\",\"ALKT\",\"ALKT\",\"CNDT\",\"CNDT\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wx\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"y\":[\"Ca\",\"Mg\",\"HCO3\",\"ALKT\",\"HARD\",\"Cl\",\"Na\",\"TDS\",\"CNDT\",\"CCMEWQI\",\"K\",\"Mg\",\"HCO3\",\"ALKT\",\"HARD\",\"Na\",\"TDS\",\"CNDT\",\"CCMEWQI\",\"SO42\",\"TDS\",\"CNDT\",\"CO32\",\"pH\",\"HCO3\",\"ALKT\",\"HARD\",\"ALKT\",\"HARD\",\"CNDT\",\"CCMEWQI\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wy\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"corr\":[1.0,0.8451300933658015,0.6064556759341931,0.6212793342841968,0.9798953644928396,1.0,0.8391562519416534,0.5065219015204949,0.9752172323353717,-0.8428918538182926,1.0,1.0,0.5629748437816114,0.5746612116462292,0.8498981085929748,1.0,0.5665386842986212,0.8373745099916003,-0.7609895100931565,1.0,1.0,0.5249712756176073,1.0,-0.8394839064075326,1.0,0.9837804211040901,0.6307186187543025,1.0,0.6474419436409902,1.0,-0.8361361598495411,1.0,1.0,1.0,0.6539659592566867,1.0,1.0,1.0,1.0,1.0,0.9788382496337047,1.0,1.0]\n",
              "},\n",
              "\"mapping\":{\n",
              "\"x\":\"x\",\n",
              "\"y\":\"y\",\n",
              "\"fill\":\"corr\"\n",
              "},\n",
              "\"size\":0.0,\n",
              "\"width\":1.002,\n",
              "\"height\":1.002\n",
              "},{\n",
              "\"na_text\":\"\",\n",
              "\"label_format\":\".2f\",\n",
              "\"show_legend\":true,\n",
              "\"tooltips\":{\n",
              "\"lines\":[\"@corr\"],\n",
              "\"formats\":[{\n",
              "\"field\":\"@corr\",\n",
              "\"format\":\".2f\"\n",
              "}]\n",
              "},\n",
              "\"sampling\":{\n",
              "\"name\":\"none\"\n",
              "},\n",
              "\"geom\":\"text\",\n",
              "\"data\":{\n",
              "\"x\":[\"Ca\",\"Ca\",\"Ca\",\"Ca\",\"Ca\",\"Cl\",\"Cl\",\"Cl\",\"Cl\",\"Cl\",\"K\",\"Mg\",\"Mg\",\"Mg\",\"Mg\",\"Na\",\"Na\",\"Na\",\"Na\",\"SO42\",\"TDS\",\"TDS\",\"CO32\",\"CO32\",\"HCO3\",\"HCO3\",\"HCO3\",\"ALKT\",\"ALKT\",\"CNDT\",\"CNDT\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wx\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"y\":[\"Ca\",\"Mg\",\"HCO3\",\"ALKT\",\"HARD\",\"Cl\",\"Na\",\"TDS\",\"CNDT\",\"CCMEWQI\",\"K\",\"Mg\",\"HCO3\",\"ALKT\",\"HARD\",\"Na\",\"TDS\",\"CNDT\",\"CCMEWQI\",\"SO42\",\"TDS\",\"CNDT\",\"CO32\",\"pH\",\"HCO3\",\"ALKT\",\"HARD\",\"ALKT\",\"HARD\",\"CNDT\",\"CCMEWQI\",\"HARD\",\"pH\",\"avg_water_Temp\",\"avg_air_temp\",\"avg_air_temp\",\"avg_precip\",\"avg_discharge\",\"avg_stage\",\"max_Wx\",\"max_Wy\",\"max_Wy\",\"CCMEWQI\"],\n",
              "\"corr\":[1.0,0.8451300933658015,0.6064556759341931,0.6212793342841968,0.9798953644928396,1.0,0.8391562519416534,0.5065219015204949,0.9752172323353717,-0.8428918538182926,1.0,1.0,0.5629748437816114,0.5746612116462292,0.8498981085929748,1.0,0.5665386842986212,0.8373745099916003,-0.7609895100931565,1.0,1.0,0.5249712756176073,1.0,-0.8394839064075326,1.0,0.9837804211040901,0.6307186187543025,1.0,0.6474419436409902,1.0,-0.8361361598495411,1.0,1.0,1.0,0.6539659592566867,1.0,1.0,1.0,1.0,1.0,0.9788382496337047,1.0,1.0],\n",
              "\"corr_abs\":[1.0,0.8451300933658015,0.6064556759341931,0.6212793342841968,0.9798953644928396,1.0,0.8391562519416534,0.5065219015204949,0.9752172323353717,0.8428918538182926,1.0,1.0,0.5629748437816114,0.5746612116462292,0.8498981085929748,1.0,0.5665386842986212,0.8373745099916003,0.7609895100931565,1.0,1.0,0.5249712756176073,1.0,0.8394839064075326,1.0,0.9837804211040901,0.6307186187543025,1.0,0.6474419436409902,1.0,0.8361361598495411,1.0,1.0,1.0,0.6539659592566867,1.0,1.0,1.0,1.0,1.0,0.9788382496337047,1.0,1.0]\n",
              "},\n",
              "\"mapping\":{\n",
              "\"x\":\"x\",\n",
              "\"y\":\"y\",\n",
              "\"label\":\"corr\",\n",
              "\"size\":\"corr_abs\",\n",
              "\"color\":\"corr\"\n",
              "},\n",
              "\"size_unit\":\"x\",\n",
              "\"color\":\"black\"\n",
              "}],\n",
              "\"metainfo_list\":[],\n",
              "\"coord\":{\n",
              "\"name\":\"fixed\",\n",
              "\"xlim\":[-0.6,20.6],\n",
              "\"ylim\":[-0.6,20.6]\n",
              "},\n",
              "\"ggsize\":{\n",
              "\"width\":849,\n",
              "\"height\":739\n",
              "},\n",
              "\"theme\":{\n",
              "\"axis_title\":{\n",
              "\"blank\":true\n",
              "},\n",
              "\"axis_line\":{\n",
              "\"blank\":true\n",
              "},\n",
              "\"panel_grid\":{\n",
              "\"blank\":true\n",
              "},\n",
              "\"axis_ticks_x\":{\n",
              "\"blank\":false\n",
              "},\n",
              "\"axis_ticks_y\":{\n",
              "\"blank\":false\n",
              "}\n",
              "}\n",
              "};\n",
              "       var plotContainer = document.getElementById(\"xxYKRt\");\n",
              "       LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
              "   </script>\n",
              "   </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# from lets_plot.bistro.corr import *\n",
        "# _ = dataset_scaled[columns_to_scale]\n",
        "# corr_plot(_, threshold=.5).labels(type='lower', diag=True, map_size=True, color='black').tiles(type='lower', diag=True).build()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_scaled_x.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VWoYBSZWnJE",
        "outputId": "06ca6c69-b258-43ad-b472-fc726f4e0694"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 1368 entries, 1965-01-31 to 2021-12-31\n",
            "Data columns (total 29 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   major_basin_X_East Don River  1368 non-null   float64\n",
            " 1   major_basin_X_West Don River  1368 non-null   float64\n",
            " 2   Ca_X                          1368 non-null   float32\n",
            " 3   Cl_X                          1368 non-null   float32\n",
            " 4   K_X                           1368 non-null   float32\n",
            " 5   Mg_X                          1368 non-null   float32\n",
            " 6   Na_X                          1368 non-null   float32\n",
            " 7   SO42_X                        1368 non-null   float32\n",
            " 8   TDS_X                         1368 non-null   float32\n",
            " 9   ALKT_X                        1368 non-null   float32\n",
            " 10  CNDT_X                        1368 non-null   float32\n",
            " 11  HARD_X                        1368 non-null   float32\n",
            " 12  pH_X                          1368 non-null   float32\n",
            " 13  CO32_X                        1368 non-null   float32\n",
            " 14  HCO3_X                        1368 non-null   float32\n",
            " 15  avg_water_Temp_X              1368 non-null   float32\n",
            " 16  avg_discharge_X               1368 non-null   float32\n",
            " 17  avg_stage_X                   1368 non-null   float32\n",
            " 18  avg_air_temp_X                1368 non-null   float32\n",
            " 19  avg_precip_X                  1368 non-null   float32\n",
            " 20  Ca_y                          1368 non-null   float32\n",
            " 21  Cl_y                          1368 non-null   float32\n",
            " 22  K_y                           1368 non-null   float32\n",
            " 23  Mg_y                          1368 non-null   float32\n",
            " 24  Na_y                          1368 non-null   float32\n",
            " 25  SO42_y                        1368 non-null   float32\n",
            " 26  TDS_y                         1368 non-null   float32\n",
            " 27  max_Wx_X                      1368 non-null   float32\n",
            " 28  max_Wy_X                      1368 non-null   float32\n",
            "dtypes: float32(27), float64(2)\n",
            "memory usage: 176.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_scaled_x"
      ],
      "metadata": {
        "id": "6j-NEZ3fXjaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "for target in ['Ca_y', 'Cl_y', 'K_y', 'Mg_y', 'Na_y','SO42_y', 'TDS_y',]:\n",
        "  # target = 'Ca_y'\n",
        "  # X = dataset_scaled.copy().drop('rank', axis=1)\n",
        "  # y = X[target] #X.pop(target)\n",
        "  # X = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "  # X = dataset_scaled_x[[column for column in dataset_scaled_x.columns if column.endswith(\"_X\")]].copy()\n",
        "  X = dataset_scaled_x.copy()\n",
        "  y = X[target]\n",
        "  n_features = X.shape[1]\n",
        "  X = X.values.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "\n",
        "  sequence_length = 1\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, shuffle=False)\n",
        "  # X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "  loss_fn = 'mse' #tf.keras.losses.Huber(delta=0.5)\n",
        "  n_months = 25\n",
        "  model_name = \"RNN\"\n",
        "  metric = ['mae', 'mse']\n",
        "\n",
        "  # sLNN Model----------------------------------------------------------------------------------------------------------\n",
        "  # def build_model():\n",
        "  #   \"\"\"\n",
        "  #   Builds a single layered neural model.\n",
        "  #   \"\"\"\n",
        "  #   # Create the model\n",
        "  #   inputs = tf.keras.Input(shape=(X.shape[1:]))\n",
        "  #   x = inputs\n",
        "  #   x = tf.keras.layers.Flatten()(x)\n",
        "  #   # Add the input layer\n",
        "  #   x = tf.keras.layers.Dense(units=16,activation='elu')(x)\n",
        "  #   # Add dropout\n",
        "  #   x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "  #   outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "  #   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  #   # Compile the model\n",
        "  #   model.compile(loss=loss_fn,\n",
        "  #                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "  #                 optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "  #                 metrics=[metric]\n",
        "  #                 )\n",
        "  #   return model\n",
        "\n",
        "  # DNN Model----------------------------------------------------------------------------------------------------------\n",
        "  # def build_model():\n",
        "  #   \"\"\"\n",
        "  #   Builds a multi-layered neural network model.\n",
        "  #   \"\"\"\n",
        "  #   # Create the model\n",
        "  #   inputs = tf.keras.Input(shape=(X.shape[1:]))\n",
        "  #   x = inputs\n",
        "  #   x = tf.keras.layers.Flatten()(x)\n",
        "  #   x = tf.keras.layers.Dense(units=64, activation=\"elu\", name=f\"dense_L0\")(x)\n",
        "  #   x = tf.keras.layers.Dense(units=64, activation=\"elu\", name=f\"dense_L1\")(x)\n",
        "  #   # Tune whether to use dropout before passing it to the output layer.\n",
        "  #   x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "  #   outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "  #   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  #   model.compile(loss=loss_fn,\n",
        "  #                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "  #                 optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "  #                 metrics=[metric]\n",
        "  #   )\n",
        "  #   return model\n",
        "\n",
        "  # CNN Model----------------------------------------------------------------------------------------------------------\n",
        "  # def build_model():\n",
        "  #   \"\"\"\n",
        "  #   Builds a convolutional neural network model.\n",
        "  #   \"\"\"\n",
        "  #   inputs = tf.keras.Input(shape=(X.shape[1:]))\n",
        "  #   x = inputs\n",
        "  #   # x = tf.keras.layers.Flatten()(x)\n",
        "  #   x = tf.keras.layers.Conv1D(\n",
        "  #         filters=32,\n",
        "  #         kernel_size=12,\n",
        "  #         activation='elu',\n",
        "  #         padding=\"causal\", name=\"conv1d_L0\",)(x)\n",
        "  #   x = tf.keras.layers.MaxPooling1D()(x)\n",
        "  #   x = tf.keras.layers.Conv1D(\n",
        "  #         filters=32,\n",
        "  #         kernel_size=6,\n",
        "  #         activation='elu',\n",
        "  #         padding=\"causal\", name=\"conv1d_L1\",)(x)\n",
        "  #   x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "  #   # x = tf.keras.layers.Dense(units=16, activation=\"elu\", name=f\"dense_L0\")(x)\n",
        "  #   # Tune whether to use dropout before passing it to the output layer.\n",
        "  #   x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  #   outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "\n",
        "  #   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  #   model.compile(loss=loss_fn,\n",
        "  #                 #optimizer=trial.suggest_categorical(\"optimizer\", ['rmsprop', 'adam']),\n",
        "  #                 optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "  #                 metrics=[metric]\n",
        "  #   )\n",
        "  #   return model\n",
        "\n",
        "  # AENN Model----------------------------------------------------------------------------------------------------------\n",
        "  # def build_model():\n",
        "  #   \"\"\"\n",
        "  #   Builds a multi-layered neural network model.\n",
        "  #   \"\"\"\n",
        "  #   # Create the model\n",
        "  #   inputs = tf.keras.Input(shape=(n_features,))\n",
        "  #   x = inputs\n",
        "  #   x = tf.keras.layers.Flatten()(x)\n",
        "  #   x = tf.keras.layers.Dense(units=64, activation=\"elu\", name=f\"dense_L0\")(x)\n",
        "  #   x = tf.keras.layers.Dense(units=32, activation=\"elu\", name=f\"dense_L1\")(x)\n",
        "  #   x = tf.keras.layers.Dense(units=16, activation=\"elu\", name=f\"dense_L2\")(x)\n",
        "  #   x = tf.keras.layers.Dense(units=32, activation=\"elu\", name=f\"dense_L3\")(x)\n",
        "  #   x = tf.keras.layers.Dense(units=64, activation=\"elu\", name=f\"dense_L4\")(x)\n",
        "  #   # Tune whether to use dropout before passing it to the output layer.\n",
        "  #   x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "  #   outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "  #   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  #   model.compile(loss=loss_fn,\n",
        "  #                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "  #                 optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "  #                 metrics=[metric]\n",
        "  #   )\n",
        "  #   return model\n",
        "\n",
        "  # RNN Model----------------------------------------------------------------------------------------------------------\n",
        "  def build_model():\n",
        "    \"\"\"\n",
        "    Builds a recurrent neural network model.\n",
        "    \"\"\"\n",
        "    inputs = tf.keras.Input(shape=(X.shape[1:]))\n",
        "    x = inputs\n",
        "    x = tf.keras.layers.LSTM(\n",
        "            units=16,\n",
        "            return_sequences=True,\n",
        "            # recurrent_dropout=0.2,\n",
        "            )(x)\n",
        "    x = tf.keras.layers.LSTM(\n",
        "            units=16,\n",
        "            return_sequences=False,\n",
        "            # recurrent_dropout=0.2,\n",
        "            )(x)\n",
        "    x = tf.keras.layers.Dense(units=32, activation='elu', name=\"dense_L0\")(x)\n",
        "    x = tf.keras.layers.Dense(units=16, activation='elu', name=\"dense_L1\")(x)\n",
        "    x = tf.keras.layers.Dense(units=32, activation='elu', name=\"dense_L2\")(x)\n",
        "    x = tf.keras.layers.Dropout(rate=0.5, name=\"dropout\")(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(loss=loss_fn,\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=metric\n",
        "    )\n",
        "    return model\n",
        "\n",
        "  def get_callbacks(patience, lr_factor):\n",
        "    '''\n",
        "    Callbacks used for early stopping and learning rate scheduling.\n",
        "    '''\n",
        "    return [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            mode=\"min\",\n",
        "            patience = patience,\n",
        "            verbose=1),\n",
        "        # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n",
        "        # for a number of epochs set with 'patience/2' var.\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\",\n",
        "            mode=\"min\",\n",
        "            factor=lr_factor,\n",
        "            min_lr=1e-6,\n",
        "            patience=patience//2,\n",
        "            verbose=1)\n",
        "        ]\n",
        "\n",
        "  model = build_model()\n",
        "\n",
        "  history = model.fit(\n",
        "      x=X_train,\n",
        "      y=y_train,\n",
        "      batch_size=128,\n",
        "      epochs=2000,\n",
        "      verbose='auto',\n",
        "      callbacks=get_callbacks(patience=5, lr_factor=0.3),\n",
        "      validation_split=0.2,\n",
        "      shuffle=True,\n",
        "  )\n",
        "  val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "  best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
        "  print(f\"Best epoch: {best_epoch}\")\n",
        "\n",
        "  # Evaluate the model____________________________________________________________________________________\n",
        "  eval_result = model.evaluate(X_test, y_test)\n",
        "  print(f\"[Test loss, Test {metric}]: {eval_result}\")\n",
        "\n",
        "  # Plot the model's losses________________________________________________________________________________\n",
        "  metrics = pd.DataFrame(history.history)\n",
        "  metrics['epoch'] = history.epoch\n",
        "\n",
        "  loss = metrics.iloc[best_epoch, metrics.columns.get_loc('loss')]\n",
        "  val_loss = metrics.iloc[best_epoch, metrics.columns.get_loc('val_loss')]\n",
        "\n",
        "  data = metrics[['epoch','loss','val_loss']].melt(value_name=\"value\", id_vars=\"epoch\", var_name=\"loss\")\n",
        "\n",
        "  plot_1 = (\n",
        "      ggplot(data)\n",
        "    + geom_line(aes(x=\"epoch\", y='value', color=\"loss\",), size=0.75)\n",
        "    + theme(legend_position=[1, 1], legend_justification=[1, 1],)\n",
        "    + geom_vline(xintercept=best_epoch, color=\"red\", linetype=\"dashed\", size=1)\n",
        "    + labs(title=f\"Suitable Model Result for {target} - ({model_name}/{loss_fn})\",\n",
        "          subtitle=f\"train loss:{loss:.4f}, val_loss:{val_loss:.4f}, test loss:{eval_result[0]:.4f}\",\n",
        "          x=\"Epoch\", y='Error [%]', color=\"\",\n",
        "          caption=\"red dash line: Optimal epoch \\n Master Thesis @ Alfred Ofosu, 08/2023\")\n",
        "    + ggsize(800, 400)\n",
        "    + scale_y_log10()\n",
        "  )\n",
        "  ggsave(plot_1, f'losses_{target}_{model_name}_{loss_fn}.png',\n",
        "        path=f'{local_path}/data/images/CCMEWQI', scale=1.0)\n",
        "\n",
        "\n",
        "  # Retrain model on the full training data_______________________________________________________________\n",
        "  hypermodel = build_model()\n",
        "  hypermodel.fit(\n",
        "      x=X_train,\n",
        "      y=y_train,\n",
        "      batch_size=None,\n",
        "      epochs=int(best_epoch*1.2),\n",
        "      verbose='auto',\n",
        "      callbacks=None,\n",
        "      validation_data=None,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  hypermodel.save(f'/content/colab_bucket/hyperOpt/CCMEWQI/{target}_{model_name}_{loss_fn}_model.keras')\n",
        "\n",
        "  # Save best model architecture\n",
        "  tf.keras.utils.plot_model(hypermodel,\n",
        "                            f\"{local_path}/data/images/CCMEWQI/archiT_{target}_{model_name}_{loss_fn}.jpeg\",\n",
        "                            show_shapes=True)\n",
        "\n",
        "  # Get the X and y values from the test BatchDataset and predict the last 12 months\n",
        "  # X = np.concatenate([x for x, y in test_dataset], axis=0)\n",
        "  # y = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "  y_pred = hypermodel.predict(X_test).flatten()\n",
        "\n",
        "  data = pd.DataFrame({\"y\":y_test, \"y_pred\":y_pred},).reset_index()\n",
        "  data['date'] = pd.to_datetime(data['date'])\n",
        "  scaler = normalizer(1)\n",
        "  scaler.fit(dataset_to_model_x[[target]])\n",
        "  data['y_invs'] = scaler.inverse_transform(data[['y']])\n",
        "  data['y_pred_invs'] = scaler.inverse_transform(data[['y_pred']])\n",
        "  plot_2 = (\n",
        "      ggplot(data.tail(n_months))\n",
        "    + geom_line(aes(x=\"date\", y=\"y_invs\"),size=1, alpha=1, show_legend=True, color='#1985a1')\n",
        "    + geom_line(aes(x=\"date\", y=\"y_pred_invs\", ),size=0.8, alpha=0.6, color=\"red\",\n",
        "                linetype=\"dashed\",\n",
        "                show_legend=True)\n",
        "    + labs(title=f\"Actual vs. Predicted Values of {target} - ({model_name}/{loss_fn})\",\n",
        "          subtitle=f\"Predicited Water Quality over the last {n_months - 1} months\",\n",
        "        x=\"\", y=f'{target} in [%]', color=\"\",\n",
        "              caption=\"black line: Actual Values, red dash line: Predicted Values \\n Master Thesis @ Alfred Ofosu, 08/2023\")\n",
        "  + scale_x_datetime(format=\"%b %Y\")\n",
        "  + ggsize(800, 400)\n",
        "  )\n",
        "  ggsave(plot_2, f'predicted_{target}_{model_name}_{loss_fn}_{n_months - 1}mons.png',\n",
        "                    path=f'{local_path}/data/images/CCMEWQI', scale=1.0)\n",
        "  # Plot model 84 Months_______________________________________________________________\n",
        "  plot_3 = (\n",
        "      ggplot(data.tail(85))\n",
        "    + geom_line(aes(x=\"date\", y=\"y_invs\"),size=1, alpha=1, show_legend=True, color='#1985a1')\n",
        "    + geom_line(aes(x=\"date\", y=\"y_pred_invs\", ),size=0.8, alpha=0.6, color=\"red\",\n",
        "                linetype=\"dashed\",\n",
        "                show_legend=True)\n",
        "    + labs(title=f\"True vs. Predicted Values of {target} - ({model_name}/{loss_fn})\",\n",
        "          subtitle=f\"Predicited Water Quality over the last {85 - 1} months\",\n",
        "        x=\"\", y=f'{target} in [%]', color=\"\",\n",
        "              caption=\"black line: Actual Values, red dash line: Predicted Values \\n Master Thesis @ Alfred Ofosu, 08/2023\")\n",
        "  + scale_x_datetime(format=\"%b %Y\")\n",
        "  + ggsize(800, 400)\n",
        "  )\n",
        "\n",
        "  ggsave(plot_3, f'predicted_{target}_{model_name}_{loss_fn}_84mons.png',\n",
        "                    path=f'{local_path}/data/images/CCMEWQI', scale=1.0)\n",
        "\n",
        "  # w, h = 800, 400\n",
        "  # bunch = GGBunch()\n",
        "  # bunch.add_plot(plot_1, 0, 0)\n",
        "  # bunch.add_plot(plot_2, w, 0)\n",
        "  # bunch.show()\n",
        "\n",
        "  del hypermodel\n",
        "  del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXhEwbEzr-5T",
        "outputId": "1ef9277e-d6dc-4393-d697-efd9c2a6d67b"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1149 - mae: 0.1983 - mse: 0.1149 - val_loss: 0.0788 - val_mae: 0.2079 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 278/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0959 - mae: 0.1979 - mse: 0.0959 - val_loss: 0.0788 - val_mae: 0.2078 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 279/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1036 - mae: 0.2019 - mse: 0.1036 - val_loss: 0.0788 - val_mae: 0.2078 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 280/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1023 - mae: 0.1920 - mse: 0.1023 - val_loss: 0.0788 - val_mae: 0.2078 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 281/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1146 - mae: 0.1940 - mse: 0.1146 - val_loss: 0.0788 - val_mae: 0.2078 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 282/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1036 - mae: 0.1972 - mse: 0.1036 - val_loss: 0.0788 - val_mae: 0.2078 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 283/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0825 - mae: 0.1905 - mse: 0.0825 - val_loss: 0.0788 - val_mae: 0.2077 - val_mse: 0.0788 - lr: 1.0000e-06\n",
            "Epoch 284/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0999 - mae: 0.1964 - mse: 0.0999 - val_loss: 0.0787 - val_mae: 0.2077 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 285/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1092 - mae: 0.1969 - mse: 0.1092 - val_loss: 0.0787 - val_mae: 0.2077 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 286/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1159 - mae: 0.1986 - mse: 0.1159 - val_loss: 0.0787 - val_mae: 0.2077 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 287/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0850 - mae: 0.1921 - mse: 0.0850 - val_loss: 0.0787 - val_mae: 0.2077 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 288/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0918 - mae: 0.1883 - mse: 0.0918 - val_loss: 0.0787 - val_mae: 0.2076 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 289/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1034 - mae: 0.1934 - mse: 0.1034 - val_loss: 0.0787 - val_mae: 0.2076 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 290/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1096 - mae: 0.1951 - mse: 0.1096 - val_loss: 0.0787 - val_mae: 0.2076 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 291/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0943 - mae: 0.1937 - mse: 0.0943 - val_loss: 0.0787 - val_mae: 0.2076 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 292/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0985 - mae: 0.1948 - mse: 0.0985 - val_loss: 0.0787 - val_mae: 0.2076 - val_mse: 0.0787 - lr: 1.0000e-06\n",
            "Epoch 293/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0795 - mae: 0.1909 - mse: 0.0795 - val_loss: 0.0786 - val_mae: 0.2075 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 294/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0784 - mae: 0.1873 - mse: 0.0784 - val_loss: 0.0786 - val_mae: 0.2075 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 295/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1033 - mae: 0.1985 - mse: 0.1033 - val_loss: 0.0786 - val_mae: 0.2075 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 296/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0777 - mae: 0.1881 - mse: 0.0777 - val_loss: 0.0786 - val_mae: 0.2075 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 297/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0930 - mae: 0.1933 - mse: 0.0930 - val_loss: 0.0786 - val_mae: 0.2075 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 298/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1065 - mae: 0.2011 - mse: 0.1065 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 299/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0872 - mae: 0.1921 - mse: 0.0872 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 300/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1250 - mae: 0.1988 - mse: 0.1250 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 301/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0972 - mae: 0.1954 - mse: 0.0972 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 302/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0916 - mae: 0.1964 - mse: 0.0916 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 303/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1016 - mae: 0.2005 - mse: 0.1016 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 304/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1058 - mae: 0.1935 - mse: 0.1058 - val_loss: 0.0786 - val_mae: 0.2074 - val_mse: 0.0786 - lr: 1.0000e-06\n",
            "Epoch 305/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1239 - mae: 0.2002 - mse: 0.1239 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 306/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0958 - mae: 0.1974 - mse: 0.0958 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 307/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1110 - mae: 0.1972 - mse: 0.1110 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 308/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1076 - mae: 0.2022 - mse: 0.1076 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 309/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1030 - mae: 0.1924 - mse: 0.1030 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 310/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0963 - mae: 0.1923 - mse: 0.0963 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 311/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0866 - mae: 0.1929 - mse: 0.0866 - val_loss: 0.0785 - val_mae: 0.2072 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 312/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1006 - mae: 0.1987 - mse: 0.1006 - val_loss: 0.0785 - val_mae: 0.2072 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 313/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1118 - mae: 0.1977 - mse: 0.1118 - val_loss: 0.0785 - val_mae: 0.2072 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 314/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0978 - mae: 0.1983 - mse: 0.0978 - val_loss: 0.0785 - val_mae: 0.2072 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 315/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1247 - mae: 0.2046 - mse: 0.1247 - val_loss: 0.0785 - val_mae: 0.2072 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 316/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0884 - mae: 0.1954 - mse: 0.0884 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 317/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0792 - mae: 0.1918 - mse: 0.0792 - val_loss: 0.0785 - val_mae: 0.2073 - val_mse: 0.0785 - lr: 1.0000e-06\n",
            "Epoch 318/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1009 - mae: 0.1975 - mse: 0.1009 - val_loss: 0.0784 - val_mae: 0.2072 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 319/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1108 - mae: 0.2004 - mse: 0.1108 - val_loss: 0.0784 - val_mae: 0.2072 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 320/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0670 - mae: 0.1871 - mse: 0.0670 - val_loss: 0.0784 - val_mae: 0.2072 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 321/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0918 - mae: 0.1947 - mse: 0.0918 - val_loss: 0.0784 - val_mae: 0.2072 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 322/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1089 - mae: 0.2015 - mse: 0.1089 - val_loss: 0.0784 - val_mae: 0.2072 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 323/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0921 - mae: 0.1912 - mse: 0.0921 - val_loss: 0.0784 - val_mae: 0.2072 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 324/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0884 - mae: 0.1942 - mse: 0.0884 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 325/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0914 - mae: 0.1952 - mse: 0.0914 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 326/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0909 - mae: 0.1961 - mse: 0.0909 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 327/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0832 - mae: 0.1913 - mse: 0.0832 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 328/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0908 - mae: 0.1895 - mse: 0.0908 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 329/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0704 - mae: 0.1872 - mse: 0.0704 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 330/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0783 - mae: 0.1906 - mse: 0.0783 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 331/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1353 - mae: 0.2023 - mse: 0.1353 - val_loss: 0.0784 - val_mae: 0.2071 - val_mse: 0.0784 - lr: 1.0000e-06\n",
            "Epoch 332/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0801 - mae: 0.1922 - mse: 0.0801 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 333/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1480 - mae: 0.1999 - mse: 0.1480 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 334/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0723 - mae: 0.1882 - mse: 0.0723 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 335/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0812 - mae: 0.1941 - mse: 0.0812 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 336/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1089 - mae: 0.1915 - mse: 0.1089 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 337/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1151 - mae: 0.2035 - mse: 0.1151 - val_loss: 0.0783 - val_mae: 0.2072 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 338/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0838 - mae: 0.1941 - mse: 0.0838 - val_loss: 0.0783 - val_mae: 0.2072 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 339/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0869 - mae: 0.1920 - mse: 0.0869 - val_loss: 0.0783 - val_mae: 0.2072 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 340/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0913 - mae: 0.1982 - mse: 0.0913 - val_loss: 0.0783 - val_mae: 0.2072 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 341/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1152 - mae: 0.1995 - mse: 0.1152 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 342/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1166 - mae: 0.1980 - mse: 0.1166 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 343/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1138 - mae: 0.1949 - mse: 0.1138 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 344/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1092 - mae: 0.2025 - mse: 0.1092 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 345/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0826 - mae: 0.1857 - mse: 0.0826 - val_loss: 0.0783 - val_mae: 0.2071 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 346/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1009 - mae: 0.1974 - mse: 0.1009 - val_loss: 0.0783 - val_mae: 0.2070 - val_mse: 0.0783 - lr: 1.0000e-06\n",
            "Epoch 347/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1178 - mae: 0.1977 - mse: 0.1178 - val_loss: 0.0782 - val_mae: 0.2070 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 348/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0932 - mae: 0.1988 - mse: 0.0932 - val_loss: 0.0782 - val_mae: 0.2070 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 349/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0934 - mae: 0.1875 - mse: 0.0934 - val_loss: 0.0782 - val_mae: 0.2070 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 350/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0723 - mae: 0.1850 - mse: 0.0723 - val_loss: 0.0782 - val_mae: 0.2070 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 351/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0795 - mae: 0.1895 - mse: 0.0795 - val_loss: 0.0782 - val_mae: 0.2070 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 352/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0888 - mae: 0.1914 - mse: 0.0888 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 353/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0888 - mae: 0.1968 - mse: 0.0888 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 354/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0914 - mae: 0.1896 - mse: 0.0914 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 355/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1003 - mae: 0.1919 - mse: 0.1003 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 356/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1178 - mae: 0.1923 - mse: 0.1178 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 357/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0820 - mae: 0.1877 - mse: 0.0820 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 358/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0822 - mae: 0.1934 - mse: 0.0822 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 359/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0763 - mae: 0.1933 - mse: 0.0763 - val_loss: 0.0782 - val_mae: 0.2069 - val_mse: 0.0782 - lr: 1.0000e-06\n",
            "Epoch 360/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0947 - mae: 0.2030 - mse: 0.0947 - val_loss: 0.0781 - val_mae: 0.2069 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 361/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0827 - mae: 0.1910 - mse: 0.0827 - val_loss: 0.0781 - val_mae: 0.2069 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 362/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0963 - mae: 0.1938 - mse: 0.0963 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 363/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0817 - mae: 0.1948 - mse: 0.0817 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 364/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0877 - mae: 0.1957 - mse: 0.0877 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 365/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0971 - mae: 0.1929 - mse: 0.0971 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 366/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1005 - mae: 0.1969 - mse: 0.1005 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 367/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0800 - mae: 0.1911 - mse: 0.0800 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 368/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0889 - mae: 0.1962 - mse: 0.0889 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 369/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1054 - mae: 0.1994 - mse: 0.1054 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 370/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0979 - mae: 0.1972 - mse: 0.0979 - val_loss: 0.0781 - val_mae: 0.2068 - val_mse: 0.0781 - lr: 1.0000e-06\n",
            "Epoch 371/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0818 - mae: 0.1869 - mse: 0.0818 - val_loss: 0.0780 - val_mae: 0.2067 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 372/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0738 - mae: 0.1890 - mse: 0.0738 - val_loss: 0.0780 - val_mae: 0.2067 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 373/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1282 - mae: 0.2001 - mse: 0.1282 - val_loss: 0.0780 - val_mae: 0.2067 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 374/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0954 - mae: 0.1951 - mse: 0.0954 - val_loss: 0.0780 - val_mae: 0.2067 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 375/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1003 - mae: 0.2017 - mse: 0.1003 - val_loss: 0.0780 - val_mae: 0.2067 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 376/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0942 - mae: 0.1907 - mse: 0.0942 - val_loss: 0.0780 - val_mae: 0.2067 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 377/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1080 - mae: 0.1977 - mse: 0.1080 - val_loss: 0.0780 - val_mae: 0.2066 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 378/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0973 - mae: 0.1947 - mse: 0.0973 - val_loss: 0.0780 - val_mae: 0.2066 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 379/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1010 - mae: 0.1988 - mse: 0.1010 - val_loss: 0.0780 - val_mae: 0.2066 - val_mse: 0.0780 - lr: 1.0000e-06\n",
            "Epoch 380/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0935 - mae: 0.1954 - mse: 0.0935 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 381/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1060 - mae: 0.1973 - mse: 0.1060 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 382/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0991 - mae: 0.1941 - mse: 0.0991 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 383/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0940 - mae: 0.1963 - mse: 0.0940 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 384/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0929 - mae: 0.1988 - mse: 0.0929 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 385/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0864 - mae: 0.1916 - mse: 0.0864 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 386/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0808 - mae: 0.1914 - mse: 0.0808 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 387/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1056 - mae: 0.2014 - mse: 0.1056 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 388/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0754 - mae: 0.1840 - mse: 0.0754 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 389/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0739 - mae: 0.1912 - mse: 0.0739 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 390/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0764 - mae: 0.1941 - mse: 0.0764 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 391/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1335 - mae: 0.1944 - mse: 0.1335 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 392/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1028 - mae: 0.2000 - mse: 0.1028 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 393/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0972 - mae: 0.1929 - mse: 0.0972 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 394/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0830 - mae: 0.1893 - mse: 0.0830 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 395/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1046 - mae: 0.2065 - mse: 0.1046 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 396/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0804 - mae: 0.1898 - mse: 0.0804 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 397/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1120 - mae: 0.1993 - mse: 0.1120 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 398/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1050 - mae: 0.1934 - mse: 0.1050 - val_loss: 0.0779 - val_mae: 0.2064 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 399/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1062 - mae: 0.1995 - mse: 0.1062 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 400/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0745 - mae: 0.1845 - mse: 0.0745 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 401/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1018 - mae: 0.1978 - mse: 0.1018 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 402/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0982 - mae: 0.1965 - mse: 0.0982 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 403/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0870 - mae: 0.1909 - mse: 0.0870 - val_loss: 0.0779 - val_mae: 0.2065 - val_mse: 0.0779 - lr: 1.0000e-06\n",
            "Epoch 404/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1174 - mae: 0.1922 - mse: 0.1174 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 405/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0861 - mae: 0.1962 - mse: 0.0861 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 406/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1061 - mae: 0.1985 - mse: 0.1061 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 407/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1068 - mae: 0.1994 - mse: 0.1068 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 408/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0945 - mae: 0.1957 - mse: 0.0945 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 409/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0833 - mae: 0.1885 - mse: 0.0833 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 410/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0650 - mae: 0.1813 - mse: 0.0650 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 411/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0904 - mae: 0.1953 - mse: 0.0904 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 412/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0928 - mae: 0.1936 - mse: 0.0928 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 413/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0883 - mae: 0.1946 - mse: 0.0883 - val_loss: 0.0778 - val_mae: 0.2065 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 414/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1138 - mae: 0.2023 - mse: 0.1138 - val_loss: 0.0778 - val_mae: 0.2064 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 415/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0790 - mae: 0.1883 - mse: 0.0790 - val_loss: 0.0778 - val_mae: 0.2064 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 416/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0958 - mae: 0.2011 - mse: 0.0958 - val_loss: 0.0778 - val_mae: 0.2064 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 417/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0920 - mae: 0.1903 - mse: 0.0920 - val_loss: 0.0778 - val_mae: 0.2064 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 418/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0889 - mae: 0.1971 - mse: 0.0889 - val_loss: 0.0778 - val_mae: 0.2064 - val_mse: 0.0778 - lr: 1.0000e-06\n",
            "Epoch 419/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1009 - mae: 0.1919 - mse: 0.1009 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 420/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0940 - mae: 0.1991 - mse: 0.0940 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 421/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0804 - mae: 0.1886 - mse: 0.0804 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 422/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1048 - mae: 0.1929 - mse: 0.1048 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 423/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0889 - mae: 0.1927 - mse: 0.0889 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 424/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1030 - mae: 0.1969 - mse: 0.1030 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 425/2000\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0823 - mae: 0.1948 - mse: 0.0823 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 426/2000\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0988 - mae: 0.1986 - mse: 0.0988 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 427/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1012 - mae: 0.1998 - mse: 0.1012 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 428/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0795 - mae: 0.1890 - mse: 0.0795 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 429/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1007 - mae: 0.1970 - mse: 0.1007 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 430/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0944 - mae: 0.1939 - mse: 0.0944 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 431/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0981 - mae: 0.1931 - mse: 0.0981 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 432/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1055 - mae: 0.1963 - mse: 0.1055 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 433/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0857 - mae: 0.1898 - mse: 0.0857 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 434/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0731 - mae: 0.1903 - mse: 0.0731 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 435/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0980 - mae: 0.1913 - mse: 0.0980 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 436/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0994 - mae: 0.1945 - mse: 0.0994 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 437/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0769 - mae: 0.1904 - mse: 0.0769 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 438/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0760 - mae: 0.1862 - mse: 0.0760 - val_loss: 0.0777 - val_mae: 0.2062 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 439/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0760 - mae: 0.1840 - mse: 0.0760 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 440/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0922 - mae: 0.1992 - mse: 0.0922 - val_loss: 0.0777 - val_mae: 0.2063 - val_mse: 0.0777 - lr: 1.0000e-06\n",
            "Epoch 441/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0966 - mae: 0.1925 - mse: 0.0966 - val_loss: 0.0776 - val_mae: 0.2063 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 442/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0779 - mae: 0.1902 - mse: 0.0779 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 443/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0955 - mae: 0.1931 - mse: 0.0955 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 444/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0823 - mae: 0.1914 - mse: 0.0823 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 445/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1191 - mae: 0.1974 - mse: 0.1191 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 446/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1026 - mae: 0.1965 - mse: 0.1026 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 447/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0895 - mae: 0.1904 - mse: 0.0895 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 448/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0907 - mae: 0.1971 - mse: 0.0907 - val_loss: 0.0776 - val_mae: 0.2061 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 449/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0937 - mae: 0.1911 - mse: 0.0937 - val_loss: 0.0776 - val_mae: 0.2061 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 450/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0769 - mae: 0.1884 - mse: 0.0769 - val_loss: 0.0776 - val_mae: 0.2061 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 451/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0849 - mae: 0.1874 - mse: 0.0849 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 452/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0756 - mae: 0.1858 - mse: 0.0756 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 453/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0935 - mae: 0.1950 - mse: 0.0935 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 454/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0789 - mae: 0.1917 - mse: 0.0789 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 455/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0871 - mae: 0.1926 - mse: 0.0871 - val_loss: 0.0776 - val_mae: 0.2062 - val_mse: 0.0776 - lr: 1.0000e-06\n",
            "Epoch 456/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0880 - mae: 0.1944 - mse: 0.0880 - val_loss: 0.0775 - val_mae: 0.2062 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 457/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0841 - mae: 0.1936 - mse: 0.0841 - val_loss: 0.0775 - val_mae: 0.2061 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 458/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0836 - mae: 0.1874 - mse: 0.0836 - val_loss: 0.0775 - val_mae: 0.2061 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 459/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0803 - mae: 0.1860 - mse: 0.0803 - val_loss: 0.0775 - val_mae: 0.2061 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 460/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0993 - mae: 0.1963 - mse: 0.0993 - val_loss: 0.0775 - val_mae: 0.2061 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 461/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1045 - mae: 0.2008 - mse: 0.1045 - val_loss: 0.0775 - val_mae: 0.2061 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 462/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0879 - mae: 0.1919 - mse: 0.0879 - val_loss: 0.0775 - val_mae: 0.2061 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 463/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0922 - mae: 0.1916 - mse: 0.0922 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 464/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1119 - mae: 0.1972 - mse: 0.1119 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 465/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0943 - mae: 0.1996 - mse: 0.0943 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 466/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0782 - mae: 0.1902 - mse: 0.0782 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 467/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0740 - mae: 0.1870 - mse: 0.0740 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 468/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0991 - mae: 0.1955 - mse: 0.0991 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 469/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1016 - mae: 0.1956 - mse: 0.1016 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 470/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0932 - mae: 0.1911 - mse: 0.0932 - val_loss: 0.0775 - val_mae: 0.2060 - val_mse: 0.0775 - lr: 1.0000e-06\n",
            "Epoch 471/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0863 - mae: 0.1911 - mse: 0.0863 - val_loss: 0.0774 - val_mae: 0.2060 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 472/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0945 - mae: 0.1982 - mse: 0.0945 - val_loss: 0.0774 - val_mae: 0.2060 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 473/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1096 - mae: 0.1956 - mse: 0.1096 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 474/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0948 - mae: 0.1973 - mse: 0.0948 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 475/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0921 - mae: 0.1925 - mse: 0.0921 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 476/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0837 - mae: 0.1893 - mse: 0.0837 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 477/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0964 - mae: 0.1888 - mse: 0.0964 - val_loss: 0.0774 - val_mae: 0.2058 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 478/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1485 - mae: 0.2099 - mse: 0.1485 - val_loss: 0.0774 - val_mae: 0.2058 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 479/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0818 - mae: 0.1883 - mse: 0.0818 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 480/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0922 - mae: 0.1922 - mse: 0.0922 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 481/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1066 - mae: 0.1996 - mse: 0.1066 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 482/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0858 - mae: 0.1910 - mse: 0.0858 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 483/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0910 - mae: 0.1908 - mse: 0.0910 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 484/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1006 - mae: 0.1968 - mse: 0.1006 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 485/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0941 - mae: 0.1890 - mse: 0.0941 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 486/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1047 - mae: 0.1872 - mse: 0.1047 - val_loss: 0.0774 - val_mae: 0.2059 - val_mse: 0.0774 - lr: 1.0000e-06\n",
            "Epoch 486: early stopping\n",
            "Best epoch: 481\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 0.1997 - mae: 0.2473 - mse: 0.1997\n",
            "[Test loss, Test ['mae', 'mse']]: [0.19968144595623016, 0.24725662171840668, 0.19968144595623016]\n",
            "Epoch 1/577\n",
            "35/35 [==============================] - 5s 6ms/step - loss: 0.8362 - mae: 0.6243 - mse: 0.8362\n",
            "Epoch 2/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.8195 - mae: 0.5993 - mse: 0.8195\n",
            "Epoch 3/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.8187 - mae: 0.5876 - mse: 0.8187\n",
            "Epoch 4/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8128 - mae: 0.5792 - mse: 0.8128\n",
            "Epoch 5/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8138 - mae: 0.5730 - mse: 0.8138\n",
            "Epoch 6/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8103 - mae: 0.5704 - mse: 0.8103\n",
            "Epoch 7/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8064 - mae: 0.5683 - mse: 0.8064\n",
            "Epoch 8/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7994 - mae: 0.5633 - mse: 0.7994\n",
            "Epoch 9/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7952 - mae: 0.5584 - mse: 0.7952\n",
            "Epoch 10/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7928 - mae: 0.5541 - mse: 0.7928\n",
            "Epoch 11/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7829 - mae: 0.5514 - mse: 0.7829\n",
            "Epoch 12/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7744 - mae: 0.5465 - mse: 0.7744\n",
            "Epoch 13/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7610 - mae: 0.5401 - mse: 0.7610\n",
            "Epoch 14/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7470 - mae: 0.5301 - mse: 0.7470\n",
            "Epoch 15/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7276 - mae: 0.5185 - mse: 0.7276\n",
            "Epoch 16/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6928 - mae: 0.5030 - mse: 0.6928\n",
            "Epoch 17/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6554 - mae: 0.4848 - mse: 0.6554\n",
            "Epoch 18/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6233 - mae: 0.4618 - mse: 0.6233\n",
            "Epoch 19/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5738 - mae: 0.4364 - mse: 0.5738\n",
            "Epoch 20/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5170 - mae: 0.4142 - mse: 0.5170\n",
            "Epoch 21/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4615 - mae: 0.3918 - mse: 0.4615\n",
            "Epoch 22/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4347 - mae: 0.3812 - mse: 0.4347\n",
            "Epoch 23/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4034 - mae: 0.3738 - mse: 0.4034\n",
            "Epoch 24/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3712 - mae: 0.3638 - mse: 0.3712\n",
            "Epoch 25/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3385 - mae: 0.3530 - mse: 0.3385\n",
            "Epoch 26/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3255 - mae: 0.3546 - mse: 0.3255\n",
            "Epoch 27/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2904 - mae: 0.3403 - mse: 0.2904\n",
            "Epoch 28/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.3243 - mae: 0.3406 - mse: 0.3243\n",
            "Epoch 29/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2934 - mae: 0.3267 - mse: 0.2934\n",
            "Epoch 30/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2597 - mae: 0.3152 - mse: 0.2597\n",
            "Epoch 31/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2641 - mae: 0.3167 - mse: 0.2641\n",
            "Epoch 32/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2427 - mae: 0.3066 - mse: 0.2427\n",
            "Epoch 33/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2436 - mae: 0.3043 - mse: 0.2436\n",
            "Epoch 34/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2264 - mae: 0.2938 - mse: 0.2264\n",
            "Epoch 35/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2085 - mae: 0.2978 - mse: 0.2085\n",
            "Epoch 36/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2072 - mae: 0.2832 - mse: 0.2072\n",
            "Epoch 37/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1835 - mae: 0.2784 - mse: 0.1835\n",
            "Epoch 38/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1795 - mae: 0.2684 - mse: 0.1795\n",
            "Epoch 39/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2013 - mae: 0.2726 - mse: 0.2013\n",
            "Epoch 40/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1820 - mae: 0.2713 - mse: 0.1820\n",
            "Epoch 41/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1911 - mae: 0.2737 - mse: 0.1911\n",
            "Epoch 42/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1431 - mae: 0.2499 - mse: 0.1431\n",
            "Epoch 43/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1606 - mae: 0.2519 - mse: 0.1606\n",
            "Epoch 44/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1710 - mae: 0.2545 - mse: 0.1710\n",
            "Epoch 45/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1407 - mae: 0.2459 - mse: 0.1407\n",
            "Epoch 46/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1583 - mae: 0.2488 - mse: 0.1583\n",
            "Epoch 47/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1348 - mae: 0.2389 - mse: 0.1348\n",
            "Epoch 48/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1376 - mae: 0.2418 - mse: 0.1376\n",
            "Epoch 49/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1380 - mae: 0.2367 - mse: 0.1380\n",
            "Epoch 50/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1216 - mae: 0.2329 - mse: 0.1216\n",
            "Epoch 51/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1331 - mae: 0.2307 - mse: 0.1331\n",
            "Epoch 52/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1380 - mae: 0.2326 - mse: 0.1380\n",
            "Epoch 53/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1367 - mae: 0.2258 - mse: 0.1367\n",
            "Epoch 54/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1107 - mae: 0.2235 - mse: 0.1107\n",
            "Epoch 55/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1242 - mae: 0.2199 - mse: 0.1242\n",
            "Epoch 56/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1096 - mae: 0.2116 - mse: 0.1096\n",
            "Epoch 57/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1439 - mae: 0.2208 - mse: 0.1439\n",
            "Epoch 58/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1245 - mae: 0.2099 - mse: 0.1245\n",
            "Epoch 59/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1028 - mae: 0.2080 - mse: 0.1028\n",
            "Epoch 60/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1148 - mae: 0.2115 - mse: 0.1148\n",
            "Epoch 61/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0963 - mae: 0.1983 - mse: 0.0963\n",
            "Epoch 62/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0783 - mae: 0.1934 - mse: 0.0783\n",
            "Epoch 63/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0874 - mae: 0.1995 - mse: 0.0874\n",
            "Epoch 64/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1167 - mae: 0.2065 - mse: 0.1167\n",
            "Epoch 65/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0916 - mae: 0.1940 - mse: 0.0916\n",
            "Epoch 66/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1050 - mae: 0.2015 - mse: 0.1050\n",
            "Epoch 67/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0974 - mae: 0.1946 - mse: 0.0974\n",
            "Epoch 68/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0867 - mae: 0.1921 - mse: 0.0867\n",
            "Epoch 69/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1154 - mae: 0.2015 - mse: 0.1154\n",
            "Epoch 70/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0947 - mae: 0.1944 - mse: 0.0947\n",
            "Epoch 71/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0940 - mae: 0.1895 - mse: 0.0940\n",
            "Epoch 72/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1173 - mae: 0.1999 - mse: 0.1173\n",
            "Epoch 73/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0884 - mae: 0.1876 - mse: 0.0884\n",
            "Epoch 74/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0962 - mae: 0.1838 - mse: 0.0962\n",
            "Epoch 75/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0922 - mae: 0.1883 - mse: 0.0922\n",
            "Epoch 76/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0731 - mae: 0.1812 - mse: 0.0731\n",
            "Epoch 77/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1001 - mae: 0.1816 - mse: 0.1001\n",
            "Epoch 78/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0755 - mae: 0.1766 - mse: 0.0755\n",
            "Epoch 79/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0767 - mae: 0.1778 - mse: 0.0767\n",
            "Epoch 80/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0845 - mae: 0.1830 - mse: 0.0845\n",
            "Epoch 81/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0753 - mae: 0.1785 - mse: 0.0753\n",
            "Epoch 82/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0842 - mae: 0.1812 - mse: 0.0842\n",
            "Epoch 83/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0713 - mae: 0.1688 - mse: 0.0713\n",
            "Epoch 84/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0798 - mae: 0.1769 - mse: 0.0798\n",
            "Epoch 85/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0802 - mae: 0.1745 - mse: 0.0802\n",
            "Epoch 86/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0607 - mae: 0.1703 - mse: 0.0607\n",
            "Epoch 87/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0723 - mae: 0.1719 - mse: 0.0723\n",
            "Epoch 88/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0668 - mae: 0.1669 - mse: 0.0668\n",
            "Epoch 89/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0636 - mae: 0.1680 - mse: 0.0636\n",
            "Epoch 90/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0890 - mae: 0.1738 - mse: 0.0890\n",
            "Epoch 91/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0598 - mae: 0.1653 - mse: 0.0598\n",
            "Epoch 92/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0610 - mae: 0.1622 - mse: 0.0610\n",
            "Epoch 93/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0742 - mae: 0.1662 - mse: 0.0742\n",
            "Epoch 94/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0630 - mae: 0.1624 - mse: 0.0630\n",
            "Epoch 95/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0733 - mae: 0.1637 - mse: 0.0733\n",
            "Epoch 96/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0539 - mae: 0.1571 - mse: 0.0539\n",
            "Epoch 97/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0759 - mae: 0.1658 - mse: 0.0759\n",
            "Epoch 98/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0623 - mae: 0.1617 - mse: 0.0623\n",
            "Epoch 99/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0666 - mae: 0.1647 - mse: 0.0666\n",
            "Epoch 100/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0599 - mae: 0.1645 - mse: 0.0599\n",
            "Epoch 101/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0615 - mae: 0.1630 - mse: 0.0615\n",
            "Epoch 102/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0775 - mae: 0.1525 - mse: 0.0775\n",
            "Epoch 103/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0503 - mae: 0.1505 - mse: 0.0503\n",
            "Epoch 104/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0629 - mae: 0.1595 - mse: 0.0629\n",
            "Epoch 105/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0820 - mae: 0.1594 - mse: 0.0820\n",
            "Epoch 106/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0608 - mae: 0.1601 - mse: 0.0608\n",
            "Epoch 107/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0690 - mae: 0.1550 - mse: 0.0690\n",
            "Epoch 108/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0541 - mae: 0.1536 - mse: 0.0541\n",
            "Epoch 109/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0722 - mae: 0.1507 - mse: 0.0722\n",
            "Epoch 110/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0522 - mae: 0.1542 - mse: 0.0522\n",
            "Epoch 111/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0607 - mae: 0.1521 - mse: 0.0607\n",
            "Epoch 112/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0632 - mae: 0.1563 - mse: 0.0632\n",
            "Epoch 113/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0439 - mae: 0.1446 - mse: 0.0439\n",
            "Epoch 114/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1479 - mse: 0.0532\n",
            "Epoch 115/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0395 - mae: 0.1429 - mse: 0.0395\n",
            "Epoch 116/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0601 - mae: 0.1548 - mse: 0.0601\n",
            "Epoch 117/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0620 - mae: 0.1550 - mse: 0.0620\n",
            "Epoch 118/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0665 - mae: 0.1479 - mse: 0.0665\n",
            "Epoch 119/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0546 - mae: 0.1519 - mse: 0.0546\n",
            "Epoch 120/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0481 - mae: 0.1445 - mse: 0.0481\n",
            "Epoch 121/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0433 - mae: 0.1397 - mse: 0.0433\n",
            "Epoch 122/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0462 - mae: 0.1449 - mse: 0.0462\n",
            "Epoch 123/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.1477 - mse: 0.0604\n",
            "Epoch 124/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0577 - mae: 0.1471 - mse: 0.0577\n",
            "Epoch 125/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0637 - mae: 0.1515 - mse: 0.0637\n",
            "Epoch 126/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0490 - mae: 0.1462 - mse: 0.0490\n",
            "Epoch 127/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0399 - mae: 0.1386 - mse: 0.0399\n",
            "Epoch 128/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0473 - mae: 0.1431 - mse: 0.0473\n",
            "Epoch 129/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0436 - mae: 0.1421 - mse: 0.0436\n",
            "Epoch 130/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0424 - mae: 0.1367 - mse: 0.0424\n",
            "Epoch 131/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0401 - mae: 0.1382 - mse: 0.0401\n",
            "Epoch 132/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0491 - mae: 0.1445 - mse: 0.0491\n",
            "Epoch 133/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0429 - mae: 0.1374 - mse: 0.0429\n",
            "Epoch 134/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0641 - mae: 0.1370 - mse: 0.0641\n",
            "Epoch 135/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0422 - mae: 0.1362 - mse: 0.0422\n",
            "Epoch 136/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0572 - mae: 0.1441 - mse: 0.0572\n",
            "Epoch 137/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0542 - mae: 0.1355 - mse: 0.0542\n",
            "Epoch 138/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0385 - mae: 0.1320 - mse: 0.0385\n",
            "Epoch 139/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0441 - mae: 0.1397 - mse: 0.0441\n",
            "Epoch 140/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0587 - mae: 0.1438 - mse: 0.0587\n",
            "Epoch 141/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0488 - mae: 0.1355 - mse: 0.0488\n",
            "Epoch 142/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0441 - mae: 0.1376 - mse: 0.0441\n",
            "Epoch 143/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0517 - mae: 0.1368 - mse: 0.0517\n",
            "Epoch 144/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0521 - mae: 0.1366 - mse: 0.0521\n",
            "Epoch 145/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0471 - mae: 0.1362 - mse: 0.0471\n",
            "Epoch 146/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0353 - mae: 0.1278 - mse: 0.0353\n",
            "Epoch 147/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0486 - mae: 0.1353 - mse: 0.0486\n",
            "Epoch 148/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0382 - mae: 0.1332 - mse: 0.0382\n",
            "Epoch 149/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0448 - mae: 0.1343 - mse: 0.0448\n",
            "Epoch 150/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0439 - mae: 0.1357 - mse: 0.0439\n",
            "Epoch 151/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0602 - mae: 0.1358 - mse: 0.0602\n",
            "Epoch 152/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0378 - mae: 0.1307 - mse: 0.0378\n",
            "Epoch 153/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0375 - mae: 0.1291 - mse: 0.0375\n",
            "Epoch 154/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0363 - mae: 0.1302 - mse: 0.0363\n",
            "Epoch 155/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0397 - mae: 0.1294 - mse: 0.0397\n",
            "Epoch 156/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0546 - mae: 0.1308 - mse: 0.0546\n",
            "Epoch 157/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0444 - mae: 0.1308 - mse: 0.0444\n",
            "Epoch 158/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0536 - mae: 0.1373 - mse: 0.0536\n",
            "Epoch 159/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0495 - mae: 0.1313 - mse: 0.0495\n",
            "Epoch 160/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0459 - mae: 0.1332 - mse: 0.0459\n",
            "Epoch 161/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0526 - mae: 0.1348 - mse: 0.0526\n",
            "Epoch 162/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0447 - mae: 0.1312 - mse: 0.0447\n",
            "Epoch 163/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0345 - mae: 0.1260 - mse: 0.0345\n",
            "Epoch 164/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0488 - mae: 0.1313 - mse: 0.0488\n",
            "Epoch 165/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0417 - mae: 0.1271 - mse: 0.0417\n",
            "Epoch 166/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0413 - mae: 0.1246 - mse: 0.0413\n",
            "Epoch 167/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0382 - mae: 0.1292 - mse: 0.0382\n",
            "Epoch 168/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.1280 - mse: 0.0380\n",
            "Epoch 169/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0456 - mae: 0.1303 - mse: 0.0456\n",
            "Epoch 170/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0490 - mae: 0.1263 - mse: 0.0490\n",
            "Epoch 171/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0526 - mae: 0.1256 - mse: 0.0526\n",
            "Epoch 172/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0470 - mae: 0.1268 - mse: 0.0470\n",
            "Epoch 173/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0366 - mae: 0.1254 - mse: 0.0366\n",
            "Epoch 174/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0406 - mae: 0.1247 - mse: 0.0406\n",
            "Epoch 175/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1234 - mse: 0.0346\n",
            "Epoch 176/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0449 - mae: 0.1261 - mse: 0.0449\n",
            "Epoch 177/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0403 - mae: 0.1267 - mse: 0.0403\n",
            "Epoch 178/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0376 - mae: 0.1219 - mse: 0.0376\n",
            "Epoch 179/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0414 - mae: 0.1277 - mse: 0.0414\n",
            "Epoch 180/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0431 - mae: 0.1237 - mse: 0.0431\n",
            "Epoch 181/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0331 - mae: 0.1182 - mse: 0.0331\n",
            "Epoch 182/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0426 - mae: 0.1221 - mse: 0.0426\n",
            "Epoch 183/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.1226 - mse: 0.0341\n",
            "Epoch 184/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0418 - mae: 0.1259 - mse: 0.0418\n",
            "Epoch 185/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0393 - mae: 0.1241 - mse: 0.0393\n",
            "Epoch 186/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0404 - mae: 0.1241 - mse: 0.0404\n",
            "Epoch 187/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0505 - mae: 0.1289 - mse: 0.0505\n",
            "Epoch 188/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0371 - mae: 0.1190 - mse: 0.0371\n",
            "Epoch 189/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0324 - mae: 0.1213 - mse: 0.0324\n",
            "Epoch 190/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0291 - mae: 0.1142 - mse: 0.0291\n",
            "Epoch 191/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0471 - mae: 0.1232 - mse: 0.0471\n",
            "Epoch 192/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0308 - mae: 0.1190 - mse: 0.0308\n",
            "Epoch 193/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0339 - mae: 0.1176 - mse: 0.0339\n",
            "Epoch 194/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0511 - mae: 0.1276 - mse: 0.0511\n",
            "Epoch 195/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0388 - mae: 0.1238 - mse: 0.0388\n",
            "Epoch 196/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0451 - mae: 0.1301 - mse: 0.0451\n",
            "Epoch 197/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0312 - mae: 0.1156 - mse: 0.0312\n",
            "Epoch 198/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0340 - mae: 0.1184 - mse: 0.0340\n",
            "Epoch 199/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0361 - mae: 0.1246 - mse: 0.0361\n",
            "Epoch 200/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0307 - mae: 0.1158 - mse: 0.0307\n",
            "Epoch 201/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0497 - mae: 0.1212 - mse: 0.0497\n",
            "Epoch 202/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.1187 - mse: 0.0311\n",
            "Epoch 203/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0369 - mae: 0.1201 - mse: 0.0369\n",
            "Epoch 204/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0373 - mae: 0.1184 - mse: 0.0373\n",
            "Epoch 205/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0320 - mae: 0.1161 - mse: 0.0320\n",
            "Epoch 206/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0321 - mae: 0.1160 - mse: 0.0321\n",
            "Epoch 207/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0326 - mae: 0.1176 - mse: 0.0326\n",
            "Epoch 208/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0452 - mae: 0.1218 - mse: 0.0452\n",
            "Epoch 209/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0362 - mae: 0.1191 - mse: 0.0362\n",
            "Epoch 210/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.1170 - mse: 0.0361\n",
            "Epoch 211/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.1130 - mse: 0.0324\n",
            "Epoch 212/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0386 - mae: 0.1185 - mse: 0.0386\n",
            "Epoch 213/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0408 - mae: 0.1236 - mse: 0.0408\n",
            "Epoch 214/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0384 - mae: 0.1178 - mse: 0.0384\n",
            "Epoch 215/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0336 - mae: 0.1191 - mse: 0.0336\n",
            "Epoch 216/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.1157 - mse: 0.0377\n",
            "Epoch 217/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.1187 - mse: 0.0341\n",
            "Epoch 218/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0314 - mae: 0.1138 - mse: 0.0314\n",
            "Epoch 219/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.1128 - mse: 0.0286\n",
            "Epoch 220/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1153 - mse: 0.0346\n",
            "Epoch 221/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0371 - mae: 0.1180 - mse: 0.0371\n",
            "Epoch 222/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0376 - mae: 0.1203 - mse: 0.0376\n",
            "Epoch 223/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0501 - mae: 0.1186 - mse: 0.0501\n",
            "Epoch 224/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0433 - mae: 0.1185 - mse: 0.0433\n",
            "Epoch 225/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.1103 - mse: 0.0304\n",
            "Epoch 226/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0433 - mae: 0.1182 - mse: 0.0433\n",
            "Epoch 227/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.1085 - mse: 0.0283\n",
            "Epoch 228/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0306 - mae: 0.1119 - mse: 0.0306\n",
            "Epoch 229/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0331 - mae: 0.1142 - mse: 0.0331\n",
            "Epoch 230/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0252 - mae: 0.1088 - mse: 0.0252\n",
            "Epoch 231/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0353 - mae: 0.1173 - mse: 0.0353\n",
            "Epoch 232/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0353 - mae: 0.1122 - mse: 0.0353\n",
            "Epoch 233/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0350 - mae: 0.1107 - mse: 0.0350\n",
            "Epoch 234/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0416 - mae: 0.1206 - mse: 0.0416\n",
            "Epoch 235/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.1137 - mse: 0.0341\n",
            "Epoch 236/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0450 - mae: 0.1227 - mse: 0.0450\n",
            "Epoch 237/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.1166 - mse: 0.0380\n",
            "Epoch 238/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1140 - mse: 0.0305\n",
            "Epoch 239/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0440 - mae: 0.1123 - mse: 0.0440\n",
            "Epoch 240/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1085 - mse: 0.0273\n",
            "Epoch 241/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0318 - mae: 0.1160 - mse: 0.0318\n",
            "Epoch 242/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0313 - mae: 0.1122 - mse: 0.0313\n",
            "Epoch 243/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1151 - mse: 0.0337\n",
            "Epoch 244/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.1172 - mse: 0.0368\n",
            "Epoch 245/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0320 - mae: 0.1106 - mse: 0.0320\n",
            "Epoch 246/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1120 - mse: 0.0337\n",
            "Epoch 247/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0335 - mae: 0.1101 - mse: 0.0335\n",
            "Epoch 248/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1106 - mse: 0.0337\n",
            "Epoch 249/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0291 - mae: 0.1122 - mse: 0.0291\n",
            "Epoch 250/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0296 - mae: 0.1133 - mse: 0.0296\n",
            "Epoch 251/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0377 - mae: 0.1139 - mse: 0.0377\n",
            "Epoch 252/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0357 - mae: 0.1103 - mse: 0.0357\n",
            "Epoch 253/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0323 - mae: 0.1146 - mse: 0.0323\n",
            "Epoch 254/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0500 - mae: 0.1155 - mse: 0.0500\n",
            "Epoch 255/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0331 - mae: 0.1078 - mse: 0.0331\n",
            "Epoch 256/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1074 - mse: 0.0274\n",
            "Epoch 257/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0342 - mae: 0.1131 - mse: 0.0342\n",
            "Epoch 258/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0436 - mae: 0.1165 - mse: 0.0436\n",
            "Epoch 259/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0431 - mae: 0.1164 - mse: 0.0431\n",
            "Epoch 260/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1055 - mse: 0.0337\n",
            "Epoch 261/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.1097 - mse: 0.0341\n",
            "Epoch 262/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1137 - mse: 0.0346\n",
            "Epoch 263/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0381 - mae: 0.1115 - mse: 0.0381\n",
            "Epoch 264/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0298 - mae: 0.1084 - mse: 0.0298\n",
            "Epoch 265/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0523 - mae: 0.1106 - mse: 0.0523\n",
            "Epoch 266/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1041 - mse: 0.0242\n",
            "Epoch 267/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0278 - mae: 0.1082 - mse: 0.0278\n",
            "Epoch 268/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0252 - mae: 0.1037 - mse: 0.0252\n",
            "Epoch 269/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.1069 - mse: 0.0304\n",
            "Epoch 270/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1060 - mse: 0.0274\n",
            "Epoch 271/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0378 - mae: 0.1118 - mse: 0.0378\n",
            "Epoch 272/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0351 - mae: 0.1119 - mse: 0.0351\n",
            "Epoch 273/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0302 - mae: 0.1064 - mse: 0.0302\n",
            "Epoch 274/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0291 - mae: 0.1078 - mse: 0.0291\n",
            "Epoch 275/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0519 - mae: 0.1132 - mse: 0.0519\n",
            "Epoch 276/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1069 - mse: 0.0281\n",
            "Epoch 277/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0314 - mae: 0.1084 - mse: 0.0314\n",
            "Epoch 278/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0353 - mae: 0.1111 - mse: 0.0353\n",
            "Epoch 279/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.1075 - mse: 0.0301\n",
            "Epoch 280/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0279 - mae: 0.1063 - mse: 0.0279\n",
            "Epoch 281/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.1061 - mse: 0.0328\n",
            "Epoch 282/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1075 - mse: 0.0334\n",
            "Epoch 283/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.1072 - mse: 0.0283\n",
            "Epoch 284/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0364 - mae: 0.1128 - mse: 0.0364\n",
            "Epoch 285/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0389 - mae: 0.1164 - mse: 0.0389\n",
            "Epoch 286/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1097 - mse: 0.0334\n",
            "Epoch 287/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.1059 - mse: 0.0294\n",
            "Epoch 288/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0243 - mae: 0.1038 - mse: 0.0243\n",
            "Epoch 289/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1051 - mse: 0.0245\n",
            "Epoch 290/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.1080 - mse: 0.0304\n",
            "Epoch 291/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0290 - mae: 0.1043 - mse: 0.0290\n",
            "Epoch 292/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0423 - mae: 0.1113 - mse: 0.0423\n",
            "Epoch 293/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0229 - mae: 0.1007 - mse: 0.0229\n",
            "Epoch 294/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0345 - mae: 0.1076 - mse: 0.0345\n",
            "Epoch 295/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0273 - mae: 0.1022 - mse: 0.0273\n",
            "Epoch 296/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0995 - mse: 0.0268\n",
            "Epoch 297/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0485 - mae: 0.1106 - mse: 0.0485\n",
            "Epoch 298/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.1055 - mse: 0.0264\n",
            "Epoch 299/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1085 - mse: 0.0305\n",
            "Epoch 300/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0308 - mae: 0.1077 - mse: 0.0308\n",
            "Epoch 301/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.1041 - mse: 0.0286\n",
            "Epoch 302/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0432 - mae: 0.1124 - mse: 0.0432\n",
            "Epoch 303/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.1048 - mse: 0.0294\n",
            "Epoch 304/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0234 - mae: 0.1034 - mse: 0.0234\n",
            "Epoch 305/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0299 - mae: 0.1084 - mse: 0.0299\n",
            "Epoch 306/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0272 - mae: 0.1023 - mse: 0.0272\n",
            "Epoch 307/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0276 - mae: 0.1059 - mse: 0.0276\n",
            "Epoch 308/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0445 - mae: 0.1071 - mse: 0.0445\n",
            "Epoch 309/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.1078 - mse: 0.0368\n",
            "Epoch 310/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0382 - mae: 0.1054 - mse: 0.0382\n",
            "Epoch 311/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.1060 - mse: 0.0250\n",
            "Epoch 312/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0280 - mae: 0.1055 - mse: 0.0280\n",
            "Epoch 313/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0263 - mae: 0.1034 - mse: 0.0263\n",
            "Epoch 314/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0378 - mae: 0.1115 - mse: 0.0378\n",
            "Epoch 315/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0327 - mae: 0.1105 - mse: 0.0327\n",
            "Epoch 316/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0292 - mae: 0.1021 - mse: 0.0292\n",
            "Epoch 317/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0369 - mae: 0.1071 - mse: 0.0369\n",
            "Epoch 318/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0295 - mae: 0.1041 - mse: 0.0295\n",
            "Epoch 319/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0254 - mae: 0.0994 - mse: 0.0254\n",
            "Epoch 320/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.1036 - mse: 0.0330\n",
            "Epoch 321/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1059 - mse: 0.0305\n",
            "Epoch 322/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.1072 - mse: 0.0300\n",
            "Epoch 323/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1018 - mse: 0.0273\n",
            "Epoch 324/577\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0303 - mae: 0.0991 - mse: 0.0303\n",
            "Epoch 325/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1058 - mse: 0.0346\n",
            "Epoch 326/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.1043 - mse: 0.0297\n",
            "Epoch 327/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0335 - mae: 0.1081 - mse: 0.0335\n",
            "Epoch 328/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0275 - mae: 0.1038 - mse: 0.0275\n",
            "Epoch 329/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.1039 - mse: 0.0286\n",
            "Epoch 330/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0357 - mae: 0.1095 - mse: 0.0357\n",
            "Epoch 331/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0356 - mae: 0.1092 - mse: 0.0356\n",
            "Epoch 332/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0217 - mae: 0.0983 - mse: 0.0217\n",
            "Epoch 333/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0240 - mae: 0.1013 - mse: 0.0240\n",
            "Epoch 334/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1052 - mse: 0.0305\n",
            "Epoch 335/577\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0301 - mae: 0.1044 - mse: 0.0301\n",
            "Epoch 336/577\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0272 - mae: 0.0989 - mse: 0.0272\n",
            "Epoch 337/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0295 - mae: 0.1020 - mse: 0.0295\n",
            "Epoch 338/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0257 - mae: 0.1018 - mse: 0.0257\n",
            "Epoch 339/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.0994 - mse: 0.0281\n",
            "Epoch 340/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0256 - mae: 0.1009 - mse: 0.0256\n",
            "Epoch 341/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.1055 - mse: 0.0303\n",
            "Epoch 342/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.1048 - mse: 0.0283\n",
            "Epoch 343/577\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0297 - mae: 0.0988 - mse: 0.0297\n",
            "Epoch 344/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0296 - mae: 0.1052 - mse: 0.0296\n",
            "Epoch 345/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0219 - mae: 0.0983 - mse: 0.0219\n",
            "Epoch 346/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0269 - mae: 0.1024 - mse: 0.0269\n",
            "Epoch 347/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0247 - mae: 0.0991 - mse: 0.0247\n",
            "Epoch 348/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0272 - mae: 0.0998 - mse: 0.0272\n",
            "Epoch 349/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0392 - mae: 0.1079 - mse: 0.0392\n",
            "Epoch 350/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0425 - mae: 0.1053 - mse: 0.0425\n",
            "Epoch 351/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0334 - mae: 0.1027 - mse: 0.0334\n",
            "Epoch 352/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0371 - mae: 0.1066 - mse: 0.0371\n",
            "Epoch 353/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0271 - mae: 0.1002 - mse: 0.0271\n",
            "Epoch 354/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0360 - mae: 0.1080 - mse: 0.0360\n",
            "Epoch 355/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0228 - mae: 0.0949 - mse: 0.0228\n",
            "Epoch 356/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0465 - mae: 0.1090 - mse: 0.0465\n",
            "Epoch 357/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1059 - mse: 0.0346\n",
            "Epoch 358/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0221 - mae: 0.0958 - mse: 0.0221\n",
            "Epoch 359/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0356 - mae: 0.1063 - mse: 0.0356\n",
            "Epoch 360/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0230 - mae: 0.0954 - mse: 0.0230\n",
            "Epoch 361/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0298 - mae: 0.1036 - mse: 0.0298\n",
            "Epoch 362/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1092 - mse: 0.0346\n",
            "Epoch 363/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0274 - mae: 0.1065 - mse: 0.0274\n",
            "Epoch 364/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1055 - mse: 0.0334\n",
            "Epoch 365/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1025 - mse: 0.0281\n",
            "Epoch 366/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.1023 - mse: 0.0330\n",
            "Epoch 367/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0374 - mae: 0.1072 - mse: 0.0374\n",
            "Epoch 368/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0321 - mae: 0.1022 - mse: 0.0321\n",
            "Epoch 369/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0296 - mae: 0.1035 - mse: 0.0296\n",
            "Epoch 370/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.1035 - mse: 0.0330\n",
            "Epoch 371/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0445 - mae: 0.1054 - mse: 0.0445\n",
            "Epoch 372/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.0965 - mse: 0.0287\n",
            "Epoch 373/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1020 - mse: 0.0337\n",
            "Epoch 374/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0270 - mae: 0.1009 - mse: 0.0270\n",
            "Epoch 375/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0323 - mae: 0.1014 - mse: 0.0323\n",
            "Epoch 376/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0240 - mae: 0.0990 - mse: 0.0240\n",
            "Epoch 377/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0352 - mae: 0.1002 - mse: 0.0352\n",
            "Epoch 378/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0335 - mae: 0.1016 - mse: 0.0335\n",
            "Epoch 379/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0289 - mae: 0.1023 - mse: 0.0289\n",
            "Epoch 380/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0298 - mae: 0.0999 - mse: 0.0298\n",
            "Epoch 381/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0277 - mae: 0.1017 - mse: 0.0277\n",
            "Epoch 382/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0222 - mae: 0.0994 - mse: 0.0222\n",
            "Epoch 383/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0216 - mae: 0.0945 - mse: 0.0216\n",
            "Epoch 384/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0455 - mae: 0.1061 - mse: 0.0455\n",
            "Epoch 385/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.1055 - mse: 0.0350\n",
            "Epoch 386/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0268 - mae: 0.0984 - mse: 0.0268\n",
            "Epoch 387/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0293 - mae: 0.1013 - mse: 0.0293\n",
            "Epoch 388/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0267 - mae: 0.1020 - mse: 0.0267\n",
            "Epoch 389/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0338 - mae: 0.1051 - mse: 0.0338\n",
            "Epoch 390/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0236 - mae: 0.1005 - mse: 0.0236\n",
            "Epoch 391/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0316 - mae: 0.1036 - mse: 0.0316\n",
            "Epoch 392/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0297 - mae: 0.0996 - mse: 0.0297\n",
            "Epoch 393/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0325 - mae: 0.1009 - mse: 0.0325\n",
            "Epoch 394/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0295 - mae: 0.1023 - mse: 0.0295\n",
            "Epoch 395/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0381 - mae: 0.1038 - mse: 0.0381\n",
            "Epoch 396/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0295 - mae: 0.1054 - mse: 0.0295\n",
            "Epoch 397/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0292 - mae: 0.1025 - mse: 0.0292\n",
            "Epoch 398/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0247 - mae: 0.0988 - mse: 0.0247\n",
            "Epoch 399/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0308 - mae: 0.1031 - mse: 0.0308\n",
            "Epoch 400/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0252 - mae: 0.0973 - mse: 0.0252\n",
            "Epoch 401/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.0998 - mse: 0.0329\n",
            "Epoch 402/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.0987 - mse: 0.0232\n",
            "Epoch 403/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0249 - mae: 0.1009 - mse: 0.0249\n",
            "Epoch 404/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.1019 - mse: 0.0297\n",
            "Epoch 405/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.0991 - mse: 0.0273\n",
            "Epoch 406/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0249 - mae: 0.0985 - mse: 0.0249\n",
            "Epoch 407/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0260 - mae: 0.1022 - mse: 0.0260\n",
            "Epoch 408/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0235 - mae: 0.0988 - mse: 0.0235\n",
            "Epoch 409/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0215 - mae: 0.0961 - mse: 0.0215\n",
            "Epoch 410/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0383 - mae: 0.1027 - mse: 0.0383\n",
            "Epoch 411/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0249 - mae: 0.0977 - mse: 0.0249\n",
            "Epoch 412/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0291 - mae: 0.1026 - mse: 0.0291\n",
            "Epoch 413/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0310 - mae: 0.0981 - mse: 0.0310\n",
            "Epoch 414/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0404 - mae: 0.1001 - mse: 0.0404\n",
            "Epoch 415/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0260 - mae: 0.1004 - mse: 0.0260\n",
            "Epoch 416/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0279 - mae: 0.1011 - mse: 0.0279\n",
            "Epoch 417/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0282 - mae: 0.1012 - mse: 0.0282\n",
            "Epoch 418/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0263 - mae: 0.0988 - mse: 0.0263\n",
            "Epoch 419/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0285 - mae: 0.0995 - mse: 0.0285\n",
            "Epoch 420/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0439 - mae: 0.1004 - mse: 0.0439\n",
            "Epoch 421/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0246 - mae: 0.0943 - mse: 0.0246\n",
            "Epoch 422/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0346 - mae: 0.1040 - mse: 0.0346\n",
            "Epoch 423/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0395 - mae: 0.1042 - mse: 0.0395\n",
            "Epoch 424/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0225 - mae: 0.0988 - mse: 0.0225\n",
            "Epoch 425/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.0960 - mse: 0.0287\n",
            "Epoch 426/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0254 - mae: 0.0968 - mse: 0.0254\n",
            "Epoch 427/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0307 - mae: 0.1021 - mse: 0.0307\n",
            "Epoch 428/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.0961 - mse: 0.0286\n",
            "Epoch 429/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0250 - mae: 0.0976 - mse: 0.0250\n",
            "Epoch 430/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.1008 - mse: 0.0287\n",
            "Epoch 431/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.0986 - mse: 0.0287\n",
            "Epoch 432/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0253 - mae: 0.1018 - mse: 0.0253\n",
            "Epoch 433/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.1006 - mse: 0.0294\n",
            "Epoch 434/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0299 - mae: 0.0989 - mse: 0.0299\n",
            "Epoch 435/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0255 - mae: 0.0978 - mse: 0.0255\n",
            "Epoch 436/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0373 - mae: 0.1080 - mse: 0.0373\n",
            "Epoch 437/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0280 - mae: 0.0959 - mse: 0.0280\n",
            "Epoch 438/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.1045 - mse: 0.0330\n",
            "Epoch 439/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0405 - mae: 0.0991 - mse: 0.0405\n",
            "Epoch 440/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.0980 - mse: 0.0300\n",
            "Epoch 441/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.0964 - mse: 0.0287\n",
            "Epoch 442/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.1016 - mse: 0.0306\n",
            "Epoch 443/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0194 - mae: 0.0913 - mse: 0.0194\n",
            "Epoch 444/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0316 - mae: 0.1006 - mse: 0.0316\n",
            "Epoch 445/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0263 - mae: 0.1007 - mse: 0.0263\n",
            "Epoch 446/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0278 - mae: 0.0951 - mse: 0.0278\n",
            "Epoch 447/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.0963 - mse: 0.0244\n",
            "Epoch 448/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0266 - mae: 0.0973 - mse: 0.0266\n",
            "Epoch 449/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0203 - mae: 0.0927 - mse: 0.0203\n",
            "Epoch 450/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.0992 - mse: 0.0300\n",
            "Epoch 451/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0289 - mae: 0.0997 - mse: 0.0289\n",
            "Epoch 452/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.0997 - mse: 0.0286\n",
            "Epoch 453/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0292 - mae: 0.0997 - mse: 0.0292\n",
            "Epoch 454/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.0959 - mse: 0.0264\n",
            "Epoch 455/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0338 - mae: 0.1034 - mse: 0.0338\n",
            "Epoch 456/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0223 - mae: 0.0935 - mse: 0.0223\n",
            "Epoch 457/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.0961 - mse: 0.0281\n",
            "Epoch 458/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0317 - mae: 0.0979 - mse: 0.0317\n",
            "Epoch 459/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0240 - mae: 0.0987 - mse: 0.0240\n",
            "Epoch 460/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0303 - mae: 0.0986 - mse: 0.0303\n",
            "Epoch 461/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0249 - mae: 0.1008 - mse: 0.0249\n",
            "Epoch 462/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0293 - mae: 0.1014 - mse: 0.0293\n",
            "Epoch 463/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0253 - mae: 0.0997 - mse: 0.0253\n",
            "Epoch 464/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0276 - mae: 0.0974 - mse: 0.0276\n",
            "Epoch 465/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.0993 - mse: 0.0297\n",
            "Epoch 466/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.0959 - mse: 0.0301\n",
            "Epoch 467/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.0996 - mse: 0.0287\n",
            "Epoch 468/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0249 - mae: 0.0961 - mse: 0.0249\n",
            "Epoch 469/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0239 - mae: 0.0912 - mse: 0.0239\n",
            "Epoch 470/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.1003 - mse: 0.0324\n",
            "Epoch 471/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0265 - mae: 0.0989 - mse: 0.0265\n",
            "Epoch 472/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.0951 - mse: 0.0232\n",
            "Epoch 473/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0247 - mae: 0.0984 - mse: 0.0247\n",
            "Epoch 474/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.0957 - mse: 0.0264\n",
            "Epoch 475/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.0985 - mse: 0.0300\n",
            "Epoch 476/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0262 - mae: 0.0942 - mse: 0.0262\n",
            "Epoch 477/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0345 - mae: 0.0994 - mse: 0.0345\n",
            "Epoch 478/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0288 - mae: 0.0982 - mse: 0.0288\n",
            "Epoch 479/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0282 - mae: 0.0980 - mse: 0.0282\n",
            "Epoch 480/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0271 - mae: 0.0991 - mse: 0.0271\n",
            "Epoch 481/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0340 - mae: 0.1001 - mse: 0.0340\n",
            "Epoch 482/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.1004 - mse: 0.0306\n",
            "Epoch 483/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0231 - mae: 0.0943 - mse: 0.0231\n",
            "Epoch 484/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0257 - mae: 0.0952 - mse: 0.0257\n",
            "Epoch 485/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0443 - mae: 0.1055 - mse: 0.0443\n",
            "Epoch 486/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0187 - mae: 0.0915 - mse: 0.0187\n",
            "Epoch 487/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0339 - mae: 0.0993 - mse: 0.0339\n",
            "Epoch 488/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.0978 - mse: 0.0294\n",
            "Epoch 489/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0323 - mae: 0.0975 - mse: 0.0323\n",
            "Epoch 490/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.0946 - mse: 0.0300\n",
            "Epoch 491/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0215 - mae: 0.0903 - mse: 0.0215\n",
            "Epoch 492/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.0982 - mse: 0.0283\n",
            "Epoch 493/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0258 - mae: 0.0918 - mse: 0.0258\n",
            "Epoch 494/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0434 - mae: 0.1011 - mse: 0.0434\n",
            "Epoch 495/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0255 - mae: 0.0984 - mse: 0.0255\n",
            "Epoch 496/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.0995 - mse: 0.0304\n",
            "Epoch 497/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0237 - mae: 0.0933 - mse: 0.0237\n",
            "Epoch 498/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.0947 - mse: 0.0306\n",
            "Epoch 499/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0259 - mae: 0.0947 - mse: 0.0259\n",
            "Epoch 500/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0278 - mae: 0.0945 - mse: 0.0278\n",
            "Epoch 501/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0451 - mae: 0.0998 - mse: 0.0451\n",
            "Epoch 502/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0370 - mae: 0.0983 - mse: 0.0370\n",
            "Epoch 503/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0213 - mae: 0.0938 - mse: 0.0213\n",
            "Epoch 504/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.0974 - mse: 0.0303\n",
            "Epoch 505/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.0887 - mse: 0.0200\n",
            "Epoch 506/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0317 - mae: 0.1020 - mse: 0.0317\n",
            "Epoch 507/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.0929 - mse: 0.0241\n",
            "Epoch 508/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0231 - mae: 0.0940 - mse: 0.0231\n",
            "Epoch 509/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0298 - mae: 0.0984 - mse: 0.0298\n",
            "Epoch 510/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0222 - mae: 0.0953 - mse: 0.0222\n",
            "Epoch 511/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.0974 - mse: 0.0294\n",
            "Epoch 512/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0255 - mae: 0.0949 - mse: 0.0255\n",
            "Epoch 513/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0369 - mae: 0.1012 - mse: 0.0369\n",
            "Epoch 514/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0341 - mae: 0.1025 - mse: 0.0341\n",
            "Epoch 515/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.1021 - mse: 0.0424\n",
            "Epoch 516/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0267 - mae: 0.0970 - mse: 0.0267\n",
            "Epoch 517/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0240 - mae: 0.0956 - mse: 0.0240\n",
            "Epoch 518/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0203 - mae: 0.0924 - mse: 0.0203\n",
            "Epoch 519/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0280 - mae: 0.0952 - mse: 0.0280\n",
            "Epoch 520/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.0893 - mse: 0.0200\n",
            "Epoch 521/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0316 - mae: 0.0962 - mse: 0.0316\n",
            "Epoch 522/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0219 - mae: 0.0942 - mse: 0.0219\n",
            "Epoch 523/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0236 - mae: 0.0926 - mse: 0.0236\n",
            "Epoch 524/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.0944 - mse: 0.0304\n",
            "Epoch 525/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0265 - mae: 0.1016 - mse: 0.0265\n",
            "Epoch 526/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0291 - mae: 0.0947 - mse: 0.0291\n",
            "Epoch 527/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0257 - mae: 0.0945 - mse: 0.0257\n",
            "Epoch 528/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.0973 - mse: 0.0330\n",
            "Epoch 529/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0236 - mae: 0.0915 - mse: 0.0236\n",
            "Epoch 530/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.0959 - mse: 0.0283\n",
            "Epoch 531/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.0997 - mse: 0.0355\n",
            "Epoch 532/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0258 - mae: 0.0942 - mse: 0.0258\n",
            "Epoch 533/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0323 - mae: 0.0973 - mse: 0.0323\n",
            "Epoch 534/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0291 - mae: 0.0981 - mse: 0.0291\n",
            "Epoch 535/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.0907 - mse: 0.0242\n",
            "Epoch 536/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0237 - mae: 0.0943 - mse: 0.0237\n",
            "Epoch 537/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.1020 - mse: 0.0422\n",
            "Epoch 538/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0260 - mae: 0.0943 - mse: 0.0260\n",
            "Epoch 539/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0218 - mae: 0.0889 - mse: 0.0218\n",
            "Epoch 540/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.0983 - mse: 0.0297\n",
            "Epoch 541/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.0955 - mse: 0.0303\n",
            "Epoch 542/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0222 - mae: 0.0899 - mse: 0.0222\n",
            "Epoch 543/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0243 - mae: 0.0916 - mse: 0.0243\n",
            "Epoch 544/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.0966 - mse: 0.0329\n",
            "Epoch 545/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0279 - mae: 0.0922 - mse: 0.0279\n",
            "Epoch 546/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0267 - mae: 0.0978 - mse: 0.0267\n",
            "Epoch 547/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0270 - mae: 0.0951 - mse: 0.0270\n",
            "Epoch 548/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.0927 - mse: 0.0264\n",
            "Epoch 549/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0323 - mae: 0.0971 - mse: 0.0323\n",
            "Epoch 550/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0302 - mae: 0.0965 - mse: 0.0302\n",
            "Epoch 551/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.0978 - mse: 0.0324\n",
            "Epoch 552/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0249 - mae: 0.0961 - mse: 0.0249\n",
            "Epoch 553/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0373 - mae: 0.0980 - mse: 0.0373\n",
            "Epoch 554/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0279 - mae: 0.0997 - mse: 0.0279\n",
            "Epoch 555/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.0919 - mse: 0.0232\n",
            "Epoch 556/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0260 - mae: 0.0959 - mse: 0.0260\n",
            "Epoch 557/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0276 - mae: 0.0943 - mse: 0.0276\n",
            "Epoch 558/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0316 - mae: 0.1014 - mse: 0.0316\n",
            "Epoch 559/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0227 - mae: 0.0946 - mse: 0.0227\n",
            "Epoch 560/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0290 - mae: 0.0979 - mse: 0.0290\n",
            "Epoch 561/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0269 - mae: 0.0956 - mse: 0.0269\n",
            "Epoch 562/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0255 - mae: 0.0924 - mse: 0.0255\n",
            "Epoch 563/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0223 - mae: 0.0920 - mse: 0.0223\n",
            "Epoch 564/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.0926 - mse: 0.0244\n",
            "Epoch 565/577\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.0919 - mse: 0.0241\n",
            "Epoch 566/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0278 - mae: 0.0955 - mse: 0.0278\n",
            "Epoch 567/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0251 - mae: 0.0935 - mse: 0.0251\n",
            "Epoch 568/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.0926 - mse: 0.0241\n",
            "Epoch 569/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0227 - mae: 0.0969 - mse: 0.0227\n",
            "Epoch 570/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.0967 - mse: 0.0304\n",
            "Epoch 571/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0268 - mae: 0.0920 - mse: 0.0268\n",
            "Epoch 572/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0308 - mae: 0.1002 - mse: 0.0308\n",
            "Epoch 573/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.0972 - mse: 0.0328\n",
            "Epoch 574/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.0886 - mse: 0.0329\n",
            "Epoch 575/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0263 - mae: 0.0916 - mse: 0.0263\n",
            "Epoch 576/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0262 - mae: 0.0975 - mse: 0.0262\n",
            "Epoch 577/577\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.0934 - mse: 0.0283\n",
            "9/9 [==============================] - 1s 3ms/step\n",
            "Epoch 1/2000\n",
            "7/7 [==============================] - 7s 371ms/step - loss: 1.0355 - mae: 0.7964 - mse: 1.0355 - val_loss: 0.9605 - val_mae: 0.7852 - val_mse: 0.9605 - lr: 1.0000e-04\n",
            "Epoch 2/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0339 - mae: 0.7952 - mse: 1.0339 - val_loss: 0.9603 - val_mae: 0.7851 - val_mse: 0.9603 - lr: 1.0000e-04\n",
            "Epoch 3/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0327 - mae: 0.7954 - mse: 1.0327 - val_loss: 0.9604 - val_mae: 0.7852 - val_mse: 0.9604 - lr: 1.0000e-04\n",
            "Epoch 4/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0330 - mae: 0.7951 - mse: 1.0330 - val_loss: 0.9601 - val_mae: 0.7851 - val_mse: 0.9601 - lr: 1.0000e-04\n",
            "Epoch 5/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0304 - mae: 0.7941 - mse: 1.0304 - val_loss: 0.9593 - val_mae: 0.7847 - val_mse: 0.9593 - lr: 1.0000e-04\n",
            "Epoch 6/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0279 - mae: 0.7928 - mse: 1.0279 - val_loss: 0.9581 - val_mae: 0.7842 - val_mse: 0.9581 - lr: 1.0000e-04\n",
            "Epoch 7/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0300 - mae: 0.7943 - mse: 1.0300 - val_loss: 0.9570 - val_mae: 0.7837 - val_mse: 0.9570 - lr: 1.0000e-04\n",
            "Epoch 8/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0280 - mae: 0.7934 - mse: 1.0280 - val_loss: 0.9567 - val_mae: 0.7836 - val_mse: 0.9567 - lr: 1.0000e-04\n",
            "Epoch 9/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0275 - mae: 0.7928 - mse: 1.0275 - val_loss: 0.9559 - val_mae: 0.7833 - val_mse: 0.9559 - lr: 1.0000e-04\n",
            "Epoch 10/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0279 - mae: 0.7931 - mse: 1.0279 - val_loss: 0.9550 - val_mae: 0.7829 - val_mse: 0.9550 - lr: 1.0000e-04\n",
            "Epoch 11/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0244 - mae: 0.7917 - mse: 1.0244 - val_loss: 0.9539 - val_mae: 0.7824 - val_mse: 0.9539 - lr: 1.0000e-04\n",
            "Epoch 12/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0229 - mae: 0.7912 - mse: 1.0229 - val_loss: 0.9532 - val_mae: 0.7821 - val_mse: 0.9532 - lr: 1.0000e-04\n",
            "Epoch 13/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0228 - mae: 0.7910 - mse: 1.0228 - val_loss: 0.9522 - val_mae: 0.7817 - val_mse: 0.9522 - lr: 1.0000e-04\n",
            "Epoch 14/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0199 - mae: 0.7902 - mse: 1.0199 - val_loss: 0.9507 - val_mae: 0.7811 - val_mse: 0.9507 - lr: 1.0000e-04\n",
            "Epoch 15/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0192 - mae: 0.7898 - mse: 1.0192 - val_loss: 0.9492 - val_mae: 0.7804 - val_mse: 0.9492 - lr: 1.0000e-04\n",
            "Epoch 16/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0173 - mae: 0.7893 - mse: 1.0173 - val_loss: 0.9479 - val_mae: 0.7798 - val_mse: 0.9479 - lr: 1.0000e-04\n",
            "Epoch 17/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0177 - mae: 0.7893 - mse: 1.0177 - val_loss: 0.9460 - val_mae: 0.7790 - val_mse: 0.9460 - lr: 1.0000e-04\n",
            "Epoch 18/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0153 - mae: 0.7880 - mse: 1.0153 - val_loss: 0.9449 - val_mae: 0.7785 - val_mse: 0.9449 - lr: 1.0000e-04\n",
            "Epoch 19/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0150 - mae: 0.7878 - mse: 1.0150 - val_loss: 0.9439 - val_mae: 0.7781 - val_mse: 0.9439 - lr: 1.0000e-04\n",
            "Epoch 20/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0111 - mae: 0.7866 - mse: 1.0111 - val_loss: 0.9423 - val_mae: 0.7774 - val_mse: 0.9423 - lr: 1.0000e-04\n",
            "Epoch 21/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0116 - mae: 0.7866 - mse: 1.0116 - val_loss: 0.9404 - val_mae: 0.7765 - val_mse: 0.9404 - lr: 1.0000e-04\n",
            "Epoch 22/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0075 - mae: 0.7856 - mse: 1.0075 - val_loss: 0.9383 - val_mae: 0.7755 - val_mse: 0.9383 - lr: 1.0000e-04\n",
            "Epoch 23/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0046 - mae: 0.7833 - mse: 1.0046 - val_loss: 0.9365 - val_mae: 0.7747 - val_mse: 0.9365 - lr: 1.0000e-04\n",
            "Epoch 24/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0049 - mae: 0.7832 - mse: 1.0049 - val_loss: 0.9341 - val_mae: 0.7737 - val_mse: 0.9341 - lr: 1.0000e-04\n",
            "Epoch 25/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0005 - mae: 0.7818 - mse: 1.0005 - val_loss: 0.9311 - val_mae: 0.7723 - val_mse: 0.9311 - lr: 1.0000e-04\n",
            "Epoch 26/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9975 - mae: 0.7815 - mse: 0.9975 - val_loss: 0.9288 - val_mae: 0.7713 - val_mse: 0.9288 - lr: 1.0000e-04\n",
            "Epoch 27/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9927 - mae: 0.7791 - mse: 0.9927 - val_loss: 0.9259 - val_mae: 0.7700 - val_mse: 0.9259 - lr: 1.0000e-04\n",
            "Epoch 28/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9906 - mae: 0.7781 - mse: 0.9906 - val_loss: 0.9223 - val_mae: 0.7684 - val_mse: 0.9223 - lr: 1.0000e-04\n",
            "Epoch 29/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9856 - mae: 0.7761 - mse: 0.9856 - val_loss: 0.9188 - val_mae: 0.7669 - val_mse: 0.9188 - lr: 1.0000e-04\n",
            "Epoch 30/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9766 - mae: 0.7730 - mse: 0.9766 - val_loss: 0.9150 - val_mae: 0.7652 - val_mse: 0.9150 - lr: 1.0000e-04\n",
            "Epoch 31/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9729 - mae: 0.7705 - mse: 0.9729 - val_loss: 0.9099 - val_mae: 0.7630 - val_mse: 0.9099 - lr: 1.0000e-04\n",
            "Epoch 32/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9713 - mae: 0.7703 - mse: 0.9713 - val_loss: 0.9056 - val_mae: 0.7611 - val_mse: 0.9056 - lr: 1.0000e-04\n",
            "Epoch 33/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9617 - mae: 0.7669 - mse: 0.9617 - val_loss: 0.8999 - val_mae: 0.7585 - val_mse: 0.8999 - lr: 1.0000e-04\n",
            "Epoch 34/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9543 - mae: 0.7637 - mse: 0.9543 - val_loss: 0.8942 - val_mae: 0.7560 - val_mse: 0.8942 - lr: 1.0000e-04\n",
            "Epoch 35/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9476 - mae: 0.7604 - mse: 0.9476 - val_loss: 0.8874 - val_mae: 0.7530 - val_mse: 0.8874 - lr: 1.0000e-04\n",
            "Epoch 36/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9422 - mae: 0.7569 - mse: 0.9422 - val_loss: 0.8800 - val_mae: 0.7497 - val_mse: 0.8800 - lr: 1.0000e-04\n",
            "Epoch 37/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9292 - mae: 0.7525 - mse: 0.9292 - val_loss: 0.8714 - val_mae: 0.7458 - val_mse: 0.8714 - lr: 1.0000e-04\n",
            "Epoch 38/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9158 - mae: 0.7474 - mse: 0.9158 - val_loss: 0.8623 - val_mae: 0.7417 - val_mse: 0.8623 - lr: 1.0000e-04\n",
            "Epoch 39/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.9082 - mae: 0.7443 - mse: 0.9082 - val_loss: 0.8540 - val_mae: 0.7379 - val_mse: 0.8540 - lr: 1.0000e-04\n",
            "Epoch 40/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.9010 - mae: 0.7401 - mse: 0.9010 - val_loss: 0.8441 - val_mae: 0.7334 - val_mse: 0.8441 - lr: 1.0000e-04\n",
            "Epoch 41/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8843 - mae: 0.7330 - mse: 0.8843 - val_loss: 0.8332 - val_mae: 0.7283 - val_mse: 0.8332 - lr: 1.0000e-04\n",
            "Epoch 42/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8697 - mae: 0.7281 - mse: 0.8697 - val_loss: 0.8204 - val_mae: 0.7226 - val_mse: 0.8204 - lr: 1.0000e-04\n",
            "Epoch 43/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8540 - mae: 0.7215 - mse: 0.8540 - val_loss: 0.8067 - val_mae: 0.7163 - val_mse: 0.8067 - lr: 1.0000e-04\n",
            "Epoch 44/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.8465 - mae: 0.7142 - mse: 0.8465 - val_loss: 0.7939 - val_mae: 0.7104 - val_mse: 0.7939 - lr: 1.0000e-04\n",
            "Epoch 45/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8203 - mae: 0.7054 - mse: 0.8203 - val_loss: 0.7782 - val_mae: 0.7031 - val_mse: 0.7782 - lr: 1.0000e-04\n",
            "Epoch 46/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7949 - mae: 0.6960 - mse: 0.7949 - val_loss: 0.7613 - val_mae: 0.6950 - val_mse: 0.7613 - lr: 1.0000e-04\n",
            "Epoch 47/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7817 - mae: 0.6869 - mse: 0.7817 - val_loss: 0.7441 - val_mae: 0.6865 - val_mse: 0.7441 - lr: 1.0000e-04\n",
            "Epoch 48/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7620 - mae: 0.6805 - mse: 0.7620 - val_loss: 0.7260 - val_mae: 0.6774 - val_mse: 0.7260 - lr: 1.0000e-04\n",
            "Epoch 49/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7318 - mae: 0.6661 - mse: 0.7318 - val_loss: 0.7058 - val_mae: 0.6670 - val_mse: 0.7058 - lr: 1.0000e-04\n",
            "Epoch 50/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7124 - mae: 0.6549 - mse: 0.7124 - val_loss: 0.6854 - val_mae: 0.6562 - val_mse: 0.6854 - lr: 1.0000e-04\n",
            "Epoch 51/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6811 - mae: 0.6414 - mse: 0.6811 - val_loss: 0.6602 - val_mae: 0.6428 - val_mse: 0.6602 - lr: 1.0000e-04\n",
            "Epoch 52/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6616 - mae: 0.6322 - mse: 0.6616 - val_loss: 0.6351 - val_mae: 0.6291 - val_mse: 0.6351 - lr: 1.0000e-04\n",
            "Epoch 53/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6309 - mae: 0.6163 - mse: 0.6309 - val_loss: 0.6124 - val_mae: 0.6164 - val_mse: 0.6124 - lr: 1.0000e-04\n",
            "Epoch 54/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5955 - mae: 0.6010 - mse: 0.5955 - val_loss: 0.5881 - val_mae: 0.6027 - val_mse: 0.5881 - lr: 1.0000e-04\n",
            "Epoch 55/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5790 - mae: 0.5853 - mse: 0.5790 - val_loss: 0.5611 - val_mae: 0.5873 - val_mse: 0.5611 - lr: 1.0000e-04\n",
            "Epoch 56/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.5511 - mae: 0.5740 - mse: 0.5511 - val_loss: 0.5329 - val_mae: 0.5712 - val_mse: 0.5329 - lr: 1.0000e-04\n",
            "Epoch 57/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5188 - mae: 0.5516 - mse: 0.5188 - val_loss: 0.5041 - val_mae: 0.5542 - val_mse: 0.5041 - lr: 1.0000e-04\n",
            "Epoch 58/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4982 - mae: 0.5442 - mse: 0.4982 - val_loss: 0.4755 - val_mae: 0.5372 - val_mse: 0.4755 - lr: 1.0000e-04\n",
            "Epoch 59/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.4510 - mae: 0.5155 - mse: 0.4510 - val_loss: 0.4463 - val_mae: 0.5201 - val_mse: 0.4463 - lr: 1.0000e-04\n",
            "Epoch 60/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4206 - mae: 0.4948 - mse: 0.4206 - val_loss: 0.4185 - val_mae: 0.5036 - val_mse: 0.4185 - lr: 1.0000e-04\n",
            "Epoch 61/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.4073 - mae: 0.4849 - mse: 0.4073 - val_loss: 0.3893 - val_mae: 0.4857 - val_mse: 0.3893 - lr: 1.0000e-04\n",
            "Epoch 62/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3658 - mae: 0.4594 - mse: 0.3658 - val_loss: 0.3618 - val_mae: 0.4676 - val_mse: 0.3618 - lr: 1.0000e-04\n",
            "Epoch 63/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3494 - mae: 0.4376 - mse: 0.3494 - val_loss: 0.3362 - val_mae: 0.4502 - val_mse: 0.3362 - lr: 1.0000e-04\n",
            "Epoch 64/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3258 - mae: 0.4300 - mse: 0.3258 - val_loss: 0.3114 - val_mae: 0.4334 - val_mse: 0.3114 - lr: 1.0000e-04\n",
            "Epoch 65/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3015 - mae: 0.4126 - mse: 0.3015 - val_loss: 0.2880 - val_mae: 0.4166 - val_mse: 0.2880 - lr: 1.0000e-04\n",
            "Epoch 66/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2674 - mae: 0.3835 - mse: 0.2674 - val_loss: 0.2650 - val_mae: 0.3991 - val_mse: 0.2650 - lr: 1.0000e-04\n",
            "Epoch 67/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2569 - mae: 0.3782 - mse: 0.2569 - val_loss: 0.2437 - val_mae: 0.3821 - val_mse: 0.2437 - lr: 1.0000e-04\n",
            "Epoch 68/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2371 - mae: 0.3619 - mse: 0.2371 - val_loss: 0.2247 - val_mae: 0.3659 - val_mse: 0.2247 - lr: 1.0000e-04\n",
            "Epoch 69/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2258 - mae: 0.3484 - mse: 0.2258 - val_loss: 0.2091 - val_mae: 0.3523 - val_mse: 0.2091 - lr: 1.0000e-04\n",
            "Epoch 70/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2060 - mae: 0.3321 - mse: 0.2060 - val_loss: 0.1932 - val_mae: 0.3382 - val_mse: 0.1932 - lr: 1.0000e-04\n",
            "Epoch 71/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1822 - mae: 0.3222 - mse: 0.1822 - val_loss: 0.1797 - val_mae: 0.3261 - val_mse: 0.1797 - lr: 1.0000e-04\n",
            "Epoch 72/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1865 - mae: 0.3142 - mse: 0.1865 - val_loss: 0.1691 - val_mae: 0.3164 - val_mse: 0.1691 - lr: 1.0000e-04\n",
            "Epoch 73/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1802 - mae: 0.3077 - mse: 0.1802 - val_loss: 0.1586 - val_mae: 0.3058 - val_mse: 0.1586 - lr: 1.0000e-04\n",
            "Epoch 74/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1712 - mae: 0.3047 - mse: 0.1712 - val_loss: 0.1526 - val_mae: 0.3008 - val_mse: 0.1526 - lr: 1.0000e-04\n",
            "Epoch 75/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1520 - mae: 0.2945 - mse: 0.1520 - val_loss: 0.1434 - val_mae: 0.2909 - val_mse: 0.1434 - lr: 1.0000e-04\n",
            "Epoch 76/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1722 - mae: 0.2941 - mse: 0.1722 - val_loss: 0.1377 - val_mae: 0.2848 - val_mse: 0.1377 - lr: 1.0000e-04\n",
            "Epoch 77/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1460 - mae: 0.2828 - mse: 0.1460 - val_loss: 0.1298 - val_mae: 0.2752 - val_mse: 0.1298 - lr: 1.0000e-04\n",
            "Epoch 78/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1635 - mae: 0.2900 - mse: 0.1635 - val_loss: 0.1260 - val_mae: 0.2717 - val_mse: 0.1260 - lr: 1.0000e-04\n",
            "Epoch 79/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1389 - mae: 0.2712 - mse: 0.1389 - val_loss: 0.1242 - val_mae: 0.2707 - val_mse: 0.1242 - lr: 1.0000e-04\n",
            "Epoch 80/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1310 - mae: 0.2697 - mse: 0.1310 - val_loss: 0.1190 - val_mae: 0.2643 - val_mse: 0.1190 - lr: 1.0000e-04\n",
            "Epoch 81/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1244 - mae: 0.2545 - mse: 0.1244 - val_loss: 0.1138 - val_mae: 0.2576 - val_mse: 0.1138 - lr: 1.0000e-04\n",
            "Epoch 82/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1327 - mae: 0.2613 - mse: 0.1327 - val_loss: 0.1100 - val_mae: 0.2533 - val_mse: 0.1100 - lr: 1.0000e-04\n",
            "Epoch 83/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1363 - mae: 0.2645 - mse: 0.1363 - val_loss: 0.1068 - val_mae: 0.2492 - val_mse: 0.1068 - lr: 1.0000e-04\n",
            "Epoch 84/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1229 - mae: 0.2527 - mse: 0.1229 - val_loss: 0.1036 - val_mae: 0.2449 - val_mse: 0.1036 - lr: 1.0000e-04\n",
            "Epoch 85/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1265 - mae: 0.2525 - mse: 0.1265 - val_loss: 0.1039 - val_mae: 0.2453 - val_mse: 0.1039 - lr: 1.0000e-04\n",
            "Epoch 86/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1309 - mae: 0.2574 - mse: 0.1309 - val_loss: 0.1020 - val_mae: 0.2426 - val_mse: 0.1020 - lr: 1.0000e-04\n",
            "Epoch 87/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1260 - mae: 0.2587 - mse: 0.1260 - val_loss: 0.0989 - val_mae: 0.2385 - val_mse: 0.0989 - lr: 1.0000e-04\n",
            "Epoch 88/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1256 - mae: 0.2574 - mse: 0.1256 - val_loss: 0.0975 - val_mae: 0.2366 - val_mse: 0.0975 - lr: 1.0000e-04\n",
            "Epoch 89/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1148 - mae: 0.2437 - mse: 0.1148 - val_loss: 0.0965 - val_mae: 0.2354 - val_mse: 0.0965 - lr: 1.0000e-04\n",
            "Epoch 90/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1219 - mae: 0.2462 - mse: 0.1219 - val_loss: 0.0935 - val_mae: 0.2315 - val_mse: 0.0935 - lr: 1.0000e-04\n",
            "Epoch 91/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1174 - mae: 0.2480 - mse: 0.1174 - val_loss: 0.0925 - val_mae: 0.2301 - val_mse: 0.0925 - lr: 1.0000e-04\n",
            "Epoch 92/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.1230 - mae: 0.2521 - mse: 0.1230 - val_loss: 0.0917 - val_mae: 0.2292 - val_mse: 0.0917 - lr: 1.0000e-04\n",
            "Epoch 93/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1230 - mae: 0.2474 - mse: 0.1230 - val_loss: 0.0912 - val_mae: 0.2289 - val_mse: 0.0912 - lr: 1.0000e-04\n",
            "Epoch 94/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1084 - mae: 0.2336 - mse: 0.1084 - val_loss: 0.0890 - val_mae: 0.2257 - val_mse: 0.0890 - lr: 1.0000e-04\n",
            "Epoch 95/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1172 - mae: 0.2452 - mse: 0.1172 - val_loss: 0.0870 - val_mae: 0.2231 - val_mse: 0.0870 - lr: 1.0000e-04\n",
            "Epoch 96/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1001 - mae: 0.2267 - mse: 0.1001 - val_loss: 0.0858 - val_mae: 0.2217 - val_mse: 0.0858 - lr: 1.0000e-04\n",
            "Epoch 97/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1081 - mae: 0.2396 - mse: 0.1081 - val_loss: 0.0848 - val_mae: 0.2204 - val_mse: 0.0848 - lr: 1.0000e-04\n",
            "Epoch 98/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1130 - mae: 0.2350 - mse: 0.1130 - val_loss: 0.0847 - val_mae: 0.2209 - val_mse: 0.0847 - lr: 1.0000e-04\n",
            "Epoch 99/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1019 - mae: 0.2331 - mse: 0.1019 - val_loss: 0.0826 - val_mae: 0.2172 - val_mse: 0.0826 - lr: 1.0000e-04\n",
            "Epoch 100/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0999 - mae: 0.2279 - mse: 0.0999 - val_loss: 0.0827 - val_mae: 0.2181 - val_mse: 0.0827 - lr: 1.0000e-04\n",
            "Epoch 101/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0799 - mae: 0.2139 - mse: 0.0799\n",
            "Epoch 101: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1160 - mae: 0.2384 - mse: 0.1160 - val_loss: 0.0836 - val_mae: 0.2209 - val_mse: 0.0836 - lr: 1.0000e-04\n",
            "Epoch 102/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0947 - mae: 0.2234 - mse: 0.0947 - val_loss: 0.0829 - val_mae: 0.2198 - val_mse: 0.0829 - lr: 3.0000e-05\n",
            "Epoch 103/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1031 - mae: 0.2328 - mse: 0.1031 - val_loss: 0.0823 - val_mae: 0.2188 - val_mse: 0.0823 - lr: 3.0000e-05\n",
            "Epoch 104/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0926 - mae: 0.2200 - mse: 0.0926 - val_loss: 0.0817 - val_mae: 0.2177 - val_mse: 0.0817 - lr: 3.0000e-05\n",
            "Epoch 105/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1223 - mae: 0.2456 - mse: 0.1223 - val_loss: 0.0806 - val_mae: 0.2155 - val_mse: 0.0806 - lr: 3.0000e-05\n",
            "Epoch 106/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1038 - mae: 0.2283 - mse: 0.1038 - val_loss: 0.0800 - val_mae: 0.2145 - val_mse: 0.0800 - lr: 3.0000e-05\n",
            "Epoch 107/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1080 - mae: 0.2308 - mse: 0.1080 - val_loss: 0.0797 - val_mae: 0.2142 - val_mse: 0.0797 - lr: 3.0000e-05\n",
            "Epoch 108/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1056 - mae: 0.2264 - mse: 0.1056 - val_loss: 0.0799 - val_mae: 0.2145 - val_mse: 0.0799 - lr: 3.0000e-05\n",
            "Epoch 109/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0891 - mae: 0.2287 - mse: 0.0891\n",
            "Epoch 109: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1013 - mae: 0.2219 - mse: 0.1013 - val_loss: 0.0801 - val_mae: 0.2149 - val_mse: 0.0801 - lr: 3.0000e-05\n",
            "Epoch 110/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0988 - mae: 0.2231 - mse: 0.0988 - val_loss: 0.0800 - val_mae: 0.2148 - val_mse: 0.0800 - lr: 9.0000e-06\n",
            "Epoch 111/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1880 - mae: 0.2427 - mse: 0.1880\n",
            "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1006 - mae: 0.2177 - mse: 0.1006 - val_loss: 0.0798 - val_mae: 0.2146 - val_mse: 0.0798 - lr: 9.0000e-06\n",
            "Epoch 112/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1044 - mae: 0.2311 - mse: 0.1044 - val_loss: 0.0798 - val_mae: 0.2146 - val_mse: 0.0798 - lr: 2.7000e-06\n",
            "Epoch 112: early stopping\n",
            "Best epoch: 107\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 0.1131 - mae: 0.2256 - mse: 0.1131\n",
            "[Test loss, Test ['mae', 'mse']]: [0.11313918977975845, 0.22561782598495483, 0.11313918977975845]\n",
            "Epoch 1/128\n",
            "35/35 [==============================] - 5s 5ms/step - loss: 1.0150 - mae: 0.7917 - mse: 1.0150\n",
            "Epoch 2/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0115 - mae: 0.7915 - mse: 1.0115\n",
            "Epoch 3/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0121 - mae: 0.7908 - mse: 1.0121\n",
            "Epoch 4/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0086 - mae: 0.7904 - mse: 1.0086\n",
            "Epoch 5/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0055 - mae: 0.7878 - mse: 1.0055\n",
            "Epoch 6/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 1.0006 - mae: 0.7864 - mse: 1.0006\n",
            "Epoch 7/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9940 - mae: 0.7845 - mse: 0.9940\n",
            "Epoch 8/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9899 - mae: 0.7826 - mse: 0.9899\n",
            "Epoch 9/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9742 - mae: 0.7758 - mse: 0.9742\n",
            "Epoch 10/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9655 - mae: 0.7717 - mse: 0.9655\n",
            "Epoch 11/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9476 - mae: 0.7646 - mse: 0.9476\n",
            "Epoch 12/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9258 - mae: 0.7532 - mse: 0.9258\n",
            "Epoch 13/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.9048 - mae: 0.7438 - mse: 0.9048\n",
            "Epoch 14/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8578 - mae: 0.7235 - mse: 0.8578\n",
            "Epoch 15/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8265 - mae: 0.7049 - mse: 0.8265\n",
            "Epoch 16/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7652 - mae: 0.6760 - mse: 0.7652\n",
            "Epoch 17/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6932 - mae: 0.6451 - mse: 0.6932\n",
            "Epoch 18/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6169 - mae: 0.6056 - mse: 0.6169\n",
            "Epoch 19/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5382 - mae: 0.5672 - mse: 0.5382\n",
            "Epoch 20/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4582 - mae: 0.5218 - mse: 0.4582\n",
            "Epoch 21/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3968 - mae: 0.4875 - mse: 0.3968\n",
            "Epoch 22/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3427 - mae: 0.4440 - mse: 0.3427\n",
            "Epoch 23/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2707 - mae: 0.4006 - mse: 0.2707\n",
            "Epoch 24/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2435 - mae: 0.3706 - mse: 0.2435\n",
            "Epoch 25/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2385 - mae: 0.3589 - mse: 0.2385\n",
            "Epoch 26/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2090 - mae: 0.3426 - mse: 0.2090\n",
            "Epoch 27/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2007 - mae: 0.3325 - mse: 0.2007\n",
            "Epoch 28/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2020 - mae: 0.3352 - mse: 0.2020\n",
            "Epoch 29/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1800 - mae: 0.3131 - mse: 0.1800\n",
            "Epoch 30/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1654 - mae: 0.2999 - mse: 0.1654\n",
            "Epoch 31/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1600 - mae: 0.2934 - mse: 0.1600\n",
            "Epoch 32/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1561 - mae: 0.2937 - mse: 0.1561\n",
            "Epoch 33/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1556 - mae: 0.2886 - mse: 0.1556\n",
            "Epoch 34/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1454 - mae: 0.2753 - mse: 0.1454\n",
            "Epoch 35/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1514 - mae: 0.2823 - mse: 0.1514\n",
            "Epoch 36/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1401 - mae: 0.2680 - mse: 0.1401\n",
            "Epoch 37/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1365 - mae: 0.2675 - mse: 0.1365\n",
            "Epoch 38/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1416 - mae: 0.2689 - mse: 0.1416\n",
            "Epoch 39/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1257 - mae: 0.2625 - mse: 0.1257\n",
            "Epoch 40/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1296 - mae: 0.2570 - mse: 0.1296\n",
            "Epoch 41/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1263 - mae: 0.2521 - mse: 0.1263\n",
            "Epoch 42/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1148 - mae: 0.2485 - mse: 0.1148\n",
            "Epoch 43/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1105 - mae: 0.2468 - mse: 0.1105\n",
            "Epoch 44/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1146 - mae: 0.2432 - mse: 0.1146\n",
            "Epoch 45/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1009 - mae: 0.2281 - mse: 0.1009\n",
            "Epoch 46/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1085 - mae: 0.2424 - mse: 0.1085\n",
            "Epoch 47/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1082 - mae: 0.2299 - mse: 0.1082\n",
            "Epoch 48/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1017 - mae: 0.2293 - mse: 0.1017\n",
            "Epoch 49/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1094 - mae: 0.2319 - mse: 0.1094\n",
            "Epoch 50/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0995 - mae: 0.2294 - mse: 0.0995\n",
            "Epoch 51/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0935 - mae: 0.2223 - mse: 0.0935\n",
            "Epoch 52/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1038 - mae: 0.2278 - mse: 0.1038\n",
            "Epoch 53/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0968 - mae: 0.2204 - mse: 0.0968\n",
            "Epoch 54/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1021 - mae: 0.2208 - mse: 0.1021\n",
            "Epoch 55/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0850 - mae: 0.2127 - mse: 0.0850\n",
            "Epoch 56/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0878 - mae: 0.2128 - mse: 0.0878\n",
            "Epoch 57/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0929 - mae: 0.2133 - mse: 0.0929\n",
            "Epoch 58/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0883 - mae: 0.2133 - mse: 0.0883\n",
            "Epoch 59/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0794 - mae: 0.2043 - mse: 0.0794\n",
            "Epoch 60/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0836 - mae: 0.2076 - mse: 0.0836\n",
            "Epoch 61/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0937 - mae: 0.2097 - mse: 0.0937\n",
            "Epoch 62/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0727 - mae: 0.1948 - mse: 0.0727\n",
            "Epoch 63/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0784 - mae: 0.1998 - mse: 0.0784\n",
            "Epoch 64/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0782 - mae: 0.1978 - mse: 0.0782\n",
            "Epoch 65/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0817 - mae: 0.2046 - mse: 0.0817\n",
            "Epoch 66/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0766 - mae: 0.1979 - mse: 0.0766\n",
            "Epoch 67/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0790 - mae: 0.1987 - mse: 0.0790\n",
            "Epoch 68/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0849 - mae: 0.2054 - mse: 0.0849\n",
            "Epoch 69/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0673 - mae: 0.1827 - mse: 0.0673\n",
            "Epoch 70/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0708 - mae: 0.1932 - mse: 0.0708\n",
            "Epoch 71/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0757 - mae: 0.1879 - mse: 0.0757\n",
            "Epoch 72/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0725 - mae: 0.1876 - mse: 0.0725\n",
            "Epoch 73/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0709 - mae: 0.1911 - mse: 0.0709\n",
            "Epoch 74/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0742 - mae: 0.1912 - mse: 0.0742\n",
            "Epoch 75/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0657 - mae: 0.1825 - mse: 0.0657\n",
            "Epoch 76/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0706 - mae: 0.1812 - mse: 0.0706\n",
            "Epoch 77/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0785 - mae: 0.1900 - mse: 0.0785\n",
            "Epoch 78/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0700 - mae: 0.1808 - mse: 0.0700\n",
            "Epoch 79/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0692 - mae: 0.1849 - mse: 0.0692\n",
            "Epoch 80/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0657 - mae: 0.1817 - mse: 0.0657\n",
            "Epoch 81/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0581 - mae: 0.1723 - mse: 0.0581\n",
            "Epoch 82/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0708 - mae: 0.1881 - mse: 0.0708\n",
            "Epoch 83/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0728 - mae: 0.1846 - mse: 0.0728\n",
            "Epoch 84/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.1766 - mse: 0.0604\n",
            "Epoch 85/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0663 - mae: 0.1788 - mse: 0.0663\n",
            "Epoch 86/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0672 - mae: 0.1786 - mse: 0.0672\n",
            "Epoch 87/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0688 - mae: 0.1832 - mse: 0.0688\n",
            "Epoch 88/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0738 - mae: 0.1824 - mse: 0.0738\n",
            "Epoch 89/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0689 - mae: 0.1859 - mse: 0.0689\n",
            "Epoch 90/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0642 - mae: 0.1713 - mse: 0.0642\n",
            "Epoch 91/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0634 - mae: 0.1757 - mse: 0.0634\n",
            "Epoch 92/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0575 - mae: 0.1701 - mse: 0.0575\n",
            "Epoch 93/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0627 - mae: 0.1780 - mse: 0.0627\n",
            "Epoch 94/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0601 - mae: 0.1706 - mse: 0.0601\n",
            "Epoch 95/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0581 - mae: 0.1609 - mse: 0.0581\n",
            "Epoch 96/128\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0562 - mae: 0.1630 - mse: 0.0562\n",
            "Epoch 97/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0589 - mae: 0.1645 - mse: 0.0589\n",
            "Epoch 98/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0641 - mae: 0.1719 - mse: 0.0641\n",
            "Epoch 99/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0559 - mae: 0.1647 - mse: 0.0559\n",
            "Epoch 100/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0641 - mae: 0.1709 - mse: 0.0641\n",
            "Epoch 101/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0601 - mae: 0.1674 - mse: 0.0601\n",
            "Epoch 102/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0579 - mae: 0.1681 - mse: 0.0579\n",
            "Epoch 103/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0661 - mae: 0.1718 - mse: 0.0661\n",
            "Epoch 104/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0514 - mae: 0.1647 - mse: 0.0514\n",
            "Epoch 105/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0561 - mae: 0.1594 - mse: 0.0561\n",
            "Epoch 106/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0637 - mae: 0.1761 - mse: 0.0637\n",
            "Epoch 107/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0492 - mae: 0.1597 - mse: 0.0492\n",
            "Epoch 108/128\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0593 - mae: 0.1666 - mse: 0.0593\n",
            "Epoch 109/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0598 - mae: 0.1653 - mse: 0.0598\n",
            "Epoch 110/128\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0556 - mae: 0.1606 - mse: 0.0556\n",
            "Epoch 111/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0567 - mae: 0.1672 - mse: 0.0567\n",
            "Epoch 112/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0535 - mae: 0.1596 - mse: 0.0535\n",
            "Epoch 113/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0542 - mae: 0.1591 - mse: 0.0542\n",
            "Epoch 114/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0525 - mae: 0.1630 - mse: 0.0525\n",
            "Epoch 115/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1575 - mse: 0.0532\n",
            "Epoch 116/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0518 - mae: 0.1545 - mse: 0.0518\n",
            "Epoch 117/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0573 - mae: 0.1632 - mse: 0.0573\n",
            "Epoch 118/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0577 - mae: 0.1658 - mse: 0.0577\n",
            "Epoch 119/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0594 - mae: 0.1668 - mse: 0.0594\n",
            "Epoch 120/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0550 - mae: 0.1592 - mse: 0.0550\n",
            "Epoch 121/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0521 - mae: 0.1575 - mse: 0.0521\n",
            "Epoch 122/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0516 - mae: 0.1580 - mse: 0.0516\n",
            "Epoch 123/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0634 - mae: 0.1624 - mse: 0.0634\n",
            "Epoch 124/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0544 - mae: 0.1613 - mse: 0.0544\n",
            "Epoch 125/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0570 - mae: 0.1576 - mse: 0.0570\n",
            "Epoch 126/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0566 - mae: 0.1653 - mse: 0.0566\n",
            "Epoch 127/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0561 - mae: 0.1622 - mse: 0.0561\n",
            "Epoch 128/128\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0557 - mae: 0.1634 - mse: 0.0557\n",
            "9/9 [==============================] - 1s 3ms/step\n",
            "Epoch 1/2000\n",
            "7/7 [==============================] - 6s 165ms/step - loss: 1.0149 - mae: 0.7961 - mse: 1.0149 - val_loss: 1.1255 - val_mae: 0.8359 - val_mse: 1.1255 - lr: 1.0000e-04\n",
            "Epoch 2/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.0103 - mae: 0.7966 - mse: 1.0103 - val_loss: 1.1226 - val_mae: 0.8350 - val_mse: 1.1226 - lr: 1.0000e-04\n",
            "Epoch 3/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.0072 - mae: 0.7947 - mse: 1.0072 - val_loss: 1.1193 - val_mae: 0.8340 - val_mse: 1.1193 - lr: 1.0000e-04\n",
            "Epoch 4/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.0065 - mae: 0.7928 - mse: 1.0065 - val_loss: 1.1157 - val_mae: 0.8329 - val_mse: 1.1157 - lr: 1.0000e-04\n",
            "Epoch 5/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9988 - mae: 0.7916 - mse: 0.9988 - val_loss: 1.1124 - val_mae: 0.8319 - val_mse: 1.1124 - lr: 1.0000e-04\n",
            "Epoch 6/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9970 - mae: 0.7879 - mse: 0.9970 - val_loss: 1.1088 - val_mae: 0.8308 - val_mse: 1.1088 - lr: 1.0000e-04\n",
            "Epoch 7/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9973 - mae: 0.7898 - mse: 0.9973 - val_loss: 1.1053 - val_mae: 0.8296 - val_mse: 1.1053 - lr: 1.0000e-04\n",
            "Epoch 8/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9909 - mae: 0.7875 - mse: 0.9909 - val_loss: 1.1013 - val_mae: 0.8283 - val_mse: 1.1013 - lr: 1.0000e-04\n",
            "Epoch 9/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9882 - mae: 0.7870 - mse: 0.9882 - val_loss: 1.0971 - val_mae: 0.8269 - val_mse: 1.0971 - lr: 1.0000e-04\n",
            "Epoch 10/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9843 - mae: 0.7850 - mse: 0.9843 - val_loss: 1.0933 - val_mae: 0.8254 - val_mse: 1.0933 - lr: 1.0000e-04\n",
            "Epoch 11/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9798 - mae: 0.7842 - mse: 0.9798 - val_loss: 1.0887 - val_mae: 0.8237 - val_mse: 1.0887 - lr: 1.0000e-04\n",
            "Epoch 12/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9746 - mae: 0.7816 - mse: 0.9746 - val_loss: 1.0843 - val_mae: 0.8219 - val_mse: 1.0843 - lr: 1.0000e-04\n",
            "Epoch 13/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9649 - mae: 0.7779 - mse: 0.9649 - val_loss: 1.0792 - val_mae: 0.8200 - val_mse: 1.0792 - lr: 1.0000e-04\n",
            "Epoch 14/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9608 - mae: 0.7771 - mse: 0.9608 - val_loss: 1.0744 - val_mae: 0.8179 - val_mse: 1.0744 - lr: 1.0000e-04\n",
            "Epoch 15/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9588 - mae: 0.7757 - mse: 0.9588 - val_loss: 1.0683 - val_mae: 0.8155 - val_mse: 1.0683 - lr: 1.0000e-04\n",
            "Epoch 16/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9481 - mae: 0.7705 - mse: 0.9481 - val_loss: 1.0614 - val_mae: 0.8130 - val_mse: 1.0614 - lr: 1.0000e-04\n",
            "Epoch 17/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9464 - mae: 0.7693 - mse: 0.9464 - val_loss: 1.0547 - val_mae: 0.8103 - val_mse: 1.0547 - lr: 1.0000e-04\n",
            "Epoch 18/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9348 - mae: 0.7644 - mse: 0.9348 - val_loss: 1.0470 - val_mae: 0.8074 - val_mse: 1.0470 - lr: 1.0000e-04\n",
            "Epoch 19/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9315 - mae: 0.7633 - mse: 0.9315 - val_loss: 1.0398 - val_mae: 0.8042 - val_mse: 1.0398 - lr: 1.0000e-04\n",
            "Epoch 20/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9225 - mae: 0.7592 - mse: 0.9225 - val_loss: 1.0319 - val_mae: 0.8009 - val_mse: 1.0319 - lr: 1.0000e-04\n",
            "Epoch 21/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9138 - mae: 0.7557 - mse: 0.9138 - val_loss: 1.0226 - val_mae: 0.7972 - val_mse: 1.0226 - lr: 1.0000e-04\n",
            "Epoch 22/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8978 - mae: 0.7506 - mse: 0.8978 - val_loss: 1.0132 - val_mae: 0.7933 - val_mse: 1.0132 - lr: 1.0000e-04\n",
            "Epoch 23/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8898 - mae: 0.7458 - mse: 0.8898 - val_loss: 1.0032 - val_mae: 0.7890 - val_mse: 1.0032 - lr: 1.0000e-04\n",
            "Epoch 24/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8715 - mae: 0.7379 - mse: 0.8715 - val_loss: 0.9920 - val_mae: 0.7844 - val_mse: 0.9920 - lr: 1.0000e-04\n",
            "Epoch 25/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8689 - mae: 0.7372 - mse: 0.8689 - val_loss: 0.9804 - val_mae: 0.7796 - val_mse: 0.9804 - lr: 1.0000e-04\n",
            "Epoch 26/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8570 - mae: 0.7320 - mse: 0.8570 - val_loss: 0.9675 - val_mae: 0.7744 - val_mse: 0.9675 - lr: 1.0000e-04\n",
            "Epoch 27/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8420 - mae: 0.7253 - mse: 0.8420 - val_loss: 0.9553 - val_mae: 0.7690 - val_mse: 0.9553 - lr: 1.0000e-04\n",
            "Epoch 28/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8227 - mae: 0.7177 - mse: 0.8227 - val_loss: 0.9416 - val_mae: 0.7631 - val_mse: 0.9416 - lr: 1.0000e-04\n",
            "Epoch 29/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8043 - mae: 0.7079 - mse: 0.8043 - val_loss: 0.9271 - val_mae: 0.7572 - val_mse: 0.9271 - lr: 1.0000e-04\n",
            "Epoch 30/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7947 - mae: 0.7057 - mse: 0.7947 - val_loss: 0.9123 - val_mae: 0.7507 - val_mse: 0.9123 - lr: 1.0000e-04\n",
            "Epoch 31/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7769 - mae: 0.6948 - mse: 0.7769 - val_loss: 0.8964 - val_mae: 0.7440 - val_mse: 0.8964 - lr: 1.0000e-04\n",
            "Epoch 32/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7582 - mae: 0.6899 - mse: 0.7582 - val_loss: 0.8788 - val_mae: 0.7370 - val_mse: 0.8788 - lr: 1.0000e-04\n",
            "Epoch 33/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7429 - mae: 0.6785 - mse: 0.7429 - val_loss: 0.8613 - val_mae: 0.7298 - val_mse: 0.8613 - lr: 1.0000e-04\n",
            "Epoch 34/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7146 - mae: 0.6671 - mse: 0.7146 - val_loss: 0.8436 - val_mae: 0.7220 - val_mse: 0.8436 - lr: 1.0000e-04\n",
            "Epoch 35/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7050 - mae: 0.6658 - mse: 0.7050 - val_loss: 0.8250 - val_mae: 0.7137 - val_mse: 0.8250 - lr: 1.0000e-04\n",
            "Epoch 36/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6840 - mae: 0.6512 - mse: 0.6840 - val_loss: 0.8056 - val_mae: 0.7051 - val_mse: 0.8056 - lr: 1.0000e-04\n",
            "Epoch 37/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6699 - mae: 0.6436 - mse: 0.6699 - val_loss: 0.7858 - val_mae: 0.6961 - val_mse: 0.7858 - lr: 1.0000e-04\n",
            "Epoch 38/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6452 - mae: 0.6328 - mse: 0.6452 - val_loss: 0.7660 - val_mae: 0.6871 - val_mse: 0.7660 - lr: 1.0000e-04\n",
            "Epoch 39/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6253 - mae: 0.6242 - mse: 0.6253 - val_loss: 0.7435 - val_mae: 0.6783 - val_mse: 0.7435 - lr: 1.0000e-04\n",
            "Epoch 40/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6060 - mae: 0.6151 - mse: 0.6060 - val_loss: 0.7244 - val_mae: 0.6694 - val_mse: 0.7244 - lr: 1.0000e-04\n",
            "Epoch 41/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5833 - mae: 0.6018 - mse: 0.5833 - val_loss: 0.7026 - val_mae: 0.6603 - val_mse: 0.7026 - lr: 1.0000e-04\n",
            "Epoch 42/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5691 - mae: 0.5985 - mse: 0.5691 - val_loss: 0.6816 - val_mae: 0.6514 - val_mse: 0.6816 - lr: 1.0000e-04\n",
            "Epoch 43/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5518 - mae: 0.5820 - mse: 0.5518 - val_loss: 0.6603 - val_mae: 0.6422 - val_mse: 0.6603 - lr: 1.0000e-04\n",
            "Epoch 44/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5234 - mae: 0.5704 - mse: 0.5234 - val_loss: 0.6401 - val_mae: 0.6334 - val_mse: 0.6401 - lr: 1.0000e-04\n",
            "Epoch 45/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5101 - mae: 0.5628 - mse: 0.5101 - val_loss: 0.6200 - val_mae: 0.6244 - val_mse: 0.6200 - lr: 1.0000e-04\n",
            "Epoch 46/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4893 - mae: 0.5521 - mse: 0.4893 - val_loss: 0.6005 - val_mae: 0.6150 - val_mse: 0.6005 - lr: 1.0000e-04\n",
            "Epoch 47/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4748 - mae: 0.5453 - mse: 0.4748 - val_loss: 0.5810 - val_mae: 0.6048 - val_mse: 0.5810 - lr: 1.0000e-04\n",
            "Epoch 48/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4443 - mae: 0.5285 - mse: 0.4443 - val_loss: 0.5619 - val_mae: 0.5946 - val_mse: 0.5619 - lr: 1.0000e-04\n",
            "Epoch 49/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4390 - mae: 0.5254 - mse: 0.4390 - val_loss: 0.5420 - val_mae: 0.5839 - val_mse: 0.5420 - lr: 1.0000e-04\n",
            "Epoch 50/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4252 - mae: 0.5176 - mse: 0.4252 - val_loss: 0.5227 - val_mae: 0.5737 - val_mse: 0.5227 - lr: 1.0000e-04\n",
            "Epoch 51/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4038 - mae: 0.5029 - mse: 0.4038 - val_loss: 0.5036 - val_mae: 0.5635 - val_mse: 0.5036 - lr: 1.0000e-04\n",
            "Epoch 52/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3819 - mae: 0.4869 - mse: 0.3819 - val_loss: 0.4851 - val_mae: 0.5535 - val_mse: 0.4851 - lr: 1.0000e-04\n",
            "Epoch 53/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3617 - mae: 0.4713 - mse: 0.3617 - val_loss: 0.4691 - val_mae: 0.5447 - val_mse: 0.4691 - lr: 1.0000e-04\n",
            "Epoch 54/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3640 - mae: 0.4739 - mse: 0.3640 - val_loss: 0.4519 - val_mae: 0.5351 - val_mse: 0.4519 - lr: 1.0000e-04\n",
            "Epoch 55/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3475 - mae: 0.4730 - mse: 0.3475 - val_loss: 0.4363 - val_mae: 0.5263 - val_mse: 0.4363 - lr: 1.0000e-04\n",
            "Epoch 56/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3336 - mae: 0.4534 - mse: 0.3336 - val_loss: 0.4195 - val_mae: 0.5163 - val_mse: 0.4195 - lr: 1.0000e-04\n",
            "Epoch 57/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3343 - mae: 0.4557 - mse: 0.3343 - val_loss: 0.4043 - val_mae: 0.5071 - val_mse: 0.4043 - lr: 1.0000e-04\n",
            "Epoch 58/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3103 - mae: 0.4453 - mse: 0.3103 - val_loss: 0.3909 - val_mae: 0.4991 - val_mse: 0.3909 - lr: 1.0000e-04\n",
            "Epoch 59/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3032 - mae: 0.4347 - mse: 0.3032 - val_loss: 0.3750 - val_mae: 0.4884 - val_mse: 0.3750 - lr: 1.0000e-04\n",
            "Epoch 60/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2959 - mae: 0.4258 - mse: 0.2959 - val_loss: 0.3615 - val_mae: 0.4794 - val_mse: 0.3615 - lr: 1.0000e-04\n",
            "Epoch 61/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2801 - mae: 0.4207 - mse: 0.2801 - val_loss: 0.3503 - val_mae: 0.4721 - val_mse: 0.3503 - lr: 1.0000e-04\n",
            "Epoch 62/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2835 - mae: 0.4221 - mse: 0.2835 - val_loss: 0.3387 - val_mae: 0.4640 - val_mse: 0.3387 - lr: 1.0000e-04\n",
            "Epoch 63/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2746 - mae: 0.4163 - mse: 0.2746 - val_loss: 0.3267 - val_mae: 0.4549 - val_mse: 0.3267 - lr: 1.0000e-04\n",
            "Epoch 64/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2538 - mae: 0.3991 - mse: 0.2538 - val_loss: 0.3178 - val_mae: 0.4488 - val_mse: 0.3178 - lr: 1.0000e-04\n",
            "Epoch 65/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2682 - mae: 0.4130 - mse: 0.2682 - val_loss: 0.3095 - val_mae: 0.4430 - val_mse: 0.3095 - lr: 1.0000e-04\n",
            "Epoch 66/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2378 - mae: 0.3886 - mse: 0.2378 - val_loss: 0.3010 - val_mae: 0.4367 - val_mse: 0.3010 - lr: 1.0000e-04\n",
            "Epoch 67/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2495 - mae: 0.3934 - mse: 0.2495 - val_loss: 0.2921 - val_mae: 0.4300 - val_mse: 0.2921 - lr: 1.0000e-04\n",
            "Epoch 68/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2287 - mae: 0.3772 - mse: 0.2287 - val_loss: 0.2852 - val_mae: 0.4248 - val_mse: 0.2852 - lr: 1.0000e-04\n",
            "Epoch 69/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2357 - mae: 0.3842 - mse: 0.2357 - val_loss: 0.2782 - val_mae: 0.4195 - val_mse: 0.2782 - lr: 1.0000e-04\n",
            "Epoch 70/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2369 - mae: 0.3821 - mse: 0.2369 - val_loss: 0.2743 - val_mae: 0.4170 - val_mse: 0.2743 - lr: 1.0000e-04\n",
            "Epoch 71/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2236 - mae: 0.3743 - mse: 0.2236 - val_loss: 0.2692 - val_mae: 0.4133 - val_mse: 0.2692 - lr: 1.0000e-04\n",
            "Epoch 72/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2442 - mae: 0.3870 - mse: 0.2442 - val_loss: 0.2653 - val_mae: 0.4107 - val_mse: 0.2653 - lr: 1.0000e-04\n",
            "Epoch 73/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2121 - mae: 0.3653 - mse: 0.2121 - val_loss: 0.2572 - val_mae: 0.4036 - val_mse: 0.2572 - lr: 1.0000e-04\n",
            "Epoch 74/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2128 - mae: 0.3620 - mse: 0.2128 - val_loss: 0.2517 - val_mae: 0.3986 - val_mse: 0.2517 - lr: 1.0000e-04\n",
            "Epoch 75/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2221 - mae: 0.3724 - mse: 0.2221 - val_loss: 0.2489 - val_mae: 0.3968 - val_mse: 0.2489 - lr: 1.0000e-04\n",
            "Epoch 76/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2054 - mae: 0.3625 - mse: 0.2054 - val_loss: 0.2461 - val_mae: 0.3947 - val_mse: 0.2461 - lr: 1.0000e-04\n",
            "Epoch 77/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2119 - mae: 0.3635 - mse: 0.2119 - val_loss: 0.2411 - val_mae: 0.3905 - val_mse: 0.2411 - lr: 1.0000e-04\n",
            "Epoch 78/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2111 - mae: 0.3533 - mse: 0.2111 - val_loss: 0.2388 - val_mae: 0.3893 - val_mse: 0.2388 - lr: 1.0000e-04\n",
            "Epoch 79/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2001 - mae: 0.3519 - mse: 0.2001 - val_loss: 0.2331 - val_mae: 0.3840 - val_mse: 0.2331 - lr: 1.0000e-04\n",
            "Epoch 80/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1975 - mae: 0.3545 - mse: 0.1975 - val_loss: 0.2302 - val_mae: 0.3820 - val_mse: 0.2302 - lr: 1.0000e-04\n",
            "Epoch 81/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1911 - mae: 0.3472 - mse: 0.1911 - val_loss: 0.2281 - val_mae: 0.3801 - val_mse: 0.2281 - lr: 1.0000e-04\n",
            "Epoch 82/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2057 - mae: 0.3528 - mse: 0.2057 - val_loss: 0.2269 - val_mae: 0.3795 - val_mse: 0.2269 - lr: 1.0000e-04\n",
            "Epoch 83/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2018 - mae: 0.3488 - mse: 0.2018 - val_loss: 0.2242 - val_mae: 0.3768 - val_mse: 0.2242 - lr: 1.0000e-04\n",
            "Epoch 84/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1925 - mae: 0.3482 - mse: 0.1925 - val_loss: 0.2243 - val_mae: 0.3778 - val_mse: 0.2243 - lr: 1.0000e-04\n",
            "Epoch 85/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1815 - mae: 0.3331 - mse: 0.1815 - val_loss: 0.2224 - val_mae: 0.3761 - val_mse: 0.2224 - lr: 1.0000e-04\n",
            "Epoch 86/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1855 - mae: 0.3415 - mse: 0.1855 - val_loss: 0.2207 - val_mae: 0.3745 - val_mse: 0.2207 - lr: 1.0000e-04\n",
            "Epoch 87/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1785 - mae: 0.3337 - mse: 0.1785 - val_loss: 0.2171 - val_mae: 0.3706 - val_mse: 0.2171 - lr: 1.0000e-04\n",
            "Epoch 88/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1843 - mae: 0.3373 - mse: 0.1843 - val_loss: 0.2170 - val_mae: 0.3712 - val_mse: 0.2170 - lr: 1.0000e-04\n",
            "Epoch 89/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1847 - mae: 0.3428 - mse: 0.1847 - val_loss: 0.2141 - val_mae: 0.3681 - val_mse: 0.2141 - lr: 1.0000e-04\n",
            "Epoch 90/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1840 - mae: 0.3383 - mse: 0.1840 - val_loss: 0.2109 - val_mae: 0.3648 - val_mse: 0.2109 - lr: 1.0000e-04\n",
            "Epoch 91/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1878 - mae: 0.3374 - mse: 0.1878 - val_loss: 0.2108 - val_mae: 0.3649 - val_mse: 0.2108 - lr: 1.0000e-04\n",
            "Epoch 92/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1709 - mae: 0.3252 - mse: 0.1709 - val_loss: 0.2111 - val_mae: 0.3654 - val_mse: 0.2111 - lr: 1.0000e-04\n",
            "Epoch 93/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1729 - mae: 0.3309 - mse: 0.1729 - val_loss: 0.2077 - val_mae: 0.3619 - val_mse: 0.2077 - lr: 1.0000e-04\n",
            "Epoch 94/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1775 - mae: 0.3311 - mse: 0.1775 - val_loss: 0.2057 - val_mae: 0.3600 - val_mse: 0.2057 - lr: 1.0000e-04\n",
            "Epoch 95/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1794 - mae: 0.3294 - mse: 0.1794 - val_loss: 0.2030 - val_mae: 0.3574 - val_mse: 0.2030 - lr: 1.0000e-04\n",
            "Epoch 96/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1716 - mae: 0.3258 - mse: 0.1716 - val_loss: 0.2017 - val_mae: 0.3562 - val_mse: 0.2017 - lr: 1.0000e-04\n",
            "Epoch 97/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1714 - mae: 0.3233 - mse: 0.1714 - val_loss: 0.2009 - val_mae: 0.3557 - val_mse: 0.2009 - lr: 1.0000e-04\n",
            "Epoch 98/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1673 - mae: 0.3139 - mse: 0.1673 - val_loss: 0.2005 - val_mae: 0.3554 - val_mse: 0.2005 - lr: 1.0000e-04\n",
            "Epoch 99/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1743 - mae: 0.3242 - mse: 0.1743 - val_loss: 0.2004 - val_mae: 0.3552 - val_mse: 0.2004 - lr: 1.0000e-04\n",
            "Epoch 100/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1607 - mae: 0.3118 - mse: 0.1607 - val_loss: 0.1992 - val_mae: 0.3537 - val_mse: 0.1992 - lr: 1.0000e-04\n",
            "Epoch 101/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1752 - mae: 0.3225 - mse: 0.1752 - val_loss: 0.1961 - val_mae: 0.3506 - val_mse: 0.1961 - lr: 1.0000e-04\n",
            "Epoch 102/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1629 - mae: 0.3122 - mse: 0.1629 - val_loss: 0.1943 - val_mae: 0.3488 - val_mse: 0.1943 - lr: 1.0000e-04\n",
            "Epoch 103/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1548 - mae: 0.3104 - mse: 0.1548 - val_loss: 0.1937 - val_mae: 0.3483 - val_mse: 0.1937 - lr: 1.0000e-04\n",
            "Epoch 104/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1461 - mae: 0.3000 - mse: 0.1461 - val_loss: 0.1937 - val_mae: 0.3481 - val_mse: 0.1937 - lr: 1.0000e-04\n",
            "Epoch 105/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1681 - mae: 0.3159 - mse: 0.1681 - val_loss: 0.1934 - val_mae: 0.3477 - val_mse: 0.1934 - lr: 1.0000e-04\n",
            "Epoch 106/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1531 - mae: 0.3080 - mse: 0.1531 - val_loss: 0.1920 - val_mae: 0.3463 - val_mse: 0.1920 - lr: 1.0000e-04\n",
            "Epoch 107/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1521 - mae: 0.3023 - mse: 0.1521 - val_loss: 0.1892 - val_mae: 0.3435 - val_mse: 0.1892 - lr: 1.0000e-04\n",
            "Epoch 108/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1545 - mae: 0.3053 - mse: 0.1545 - val_loss: 0.1873 - val_mae: 0.3416 - val_mse: 0.1873 - lr: 1.0000e-04\n",
            "Epoch 109/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1514 - mae: 0.3014 - mse: 0.1514 - val_loss: 0.1864 - val_mae: 0.3405 - val_mse: 0.1864 - lr: 1.0000e-04\n",
            "Epoch 110/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.1582 - mae: 0.3060 - mse: 0.1582 - val_loss: 0.1871 - val_mae: 0.3411 - val_mse: 0.1871 - lr: 1.0000e-04\n",
            "Epoch 111/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1528 - mae: 0.3061 - mse: 0.1528\n",
            "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1499 - mae: 0.3059 - mse: 0.1499 - val_loss: 0.1870 - val_mae: 0.3408 - val_mse: 0.1870 - lr: 1.0000e-04\n",
            "Epoch 112/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1487 - mae: 0.3023 - mse: 0.1487 - val_loss: 0.1868 - val_mae: 0.3406 - val_mse: 0.1868 - lr: 3.0000e-05\n",
            "Epoch 113/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1486 - mae: 0.2991 - mse: 0.1486 - val_loss: 0.1858 - val_mae: 0.3396 - val_mse: 0.1858 - lr: 3.0000e-05\n",
            "Epoch 114/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1479 - mae: 0.3051 - mse: 0.1479 - val_loss: 0.1846 - val_mae: 0.3384 - val_mse: 0.1846 - lr: 3.0000e-05\n",
            "Epoch 115/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1490 - mae: 0.3023 - mse: 0.1490 - val_loss: 0.1838 - val_mae: 0.3376 - val_mse: 0.1838 - lr: 3.0000e-05\n",
            "Epoch 116/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1509 - mae: 0.3013 - mse: 0.1509 - val_loss: 0.1835 - val_mae: 0.3374 - val_mse: 0.1835 - lr: 3.0000e-05\n",
            "Epoch 117/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1473 - mae: 0.2938 - mse: 0.1473 - val_loss: 0.1843 - val_mae: 0.3381 - val_mse: 0.1843 - lr: 3.0000e-05\n",
            "Epoch 118/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1792 - mae: 0.3204 - mse: 0.1792\n",
            "Epoch 118: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1444 - mae: 0.2966 - mse: 0.1444 - val_loss: 0.1850 - val_mae: 0.3387 - val_mse: 0.1850 - lr: 3.0000e-05\n",
            "Epoch 119/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1611 - mae: 0.3137 - mse: 0.1611 - val_loss: 0.1846 - val_mae: 0.3383 - val_mse: 0.1846 - lr: 9.0000e-06\n",
            "Epoch 120/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1461 - mae: 0.2981 - mse: 0.1461\n",
            "Epoch 120: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1537 - mae: 0.3089 - mse: 0.1537 - val_loss: 0.1844 - val_mae: 0.3381 - val_mse: 0.1844 - lr: 9.0000e-06\n",
            "Epoch 121/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1475 - mae: 0.3000 - mse: 0.1475 - val_loss: 0.1843 - val_mae: 0.3380 - val_mse: 0.1843 - lr: 2.7000e-06\n",
            "Epoch 121: early stopping\n",
            "Best epoch: 116\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 0.1569 - mae: 0.3140 - mse: 0.1569\n",
            "[Test loss, Test ['mae', 'mse']]: [0.15689435601234436, 0.3140493929386139, 0.15689435601234436]\n",
            "Epoch 1/139\n",
            "35/35 [==============================] - 7s 5ms/step - loss: 1.0561 - mae: 0.8116 - mse: 1.0561\n",
            "Epoch 2/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0483 - mae: 0.8106 - mse: 1.0483\n",
            "Epoch 3/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 1.0393 - mae: 0.8065 - mse: 1.0393\n",
            "Epoch 4/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0338 - mae: 0.8035 - mse: 1.0338\n",
            "Epoch 5/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 1.0228 - mae: 0.8015 - mse: 1.0228\n",
            "Epoch 6/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 1.0156 - mae: 0.7974 - mse: 1.0156\n",
            "Epoch 7/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 1.0093 - mae: 0.7949 - mse: 1.0093\n",
            "Epoch 8/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9938 - mae: 0.7881 - mse: 0.9938\n",
            "Epoch 9/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9798 - mae: 0.7836 - mse: 0.9798\n",
            "Epoch 10/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9512 - mae: 0.7715 - mse: 0.9512\n",
            "Epoch 11/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9098 - mae: 0.7541 - mse: 0.9098\n",
            "Epoch 12/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8673 - mae: 0.7371 - mse: 0.8673\n",
            "Epoch 13/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8049 - mae: 0.7105 - mse: 0.8049\n",
            "Epoch 14/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7265 - mae: 0.6758 - mse: 0.7265\n",
            "Epoch 15/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6587 - mae: 0.6396 - mse: 0.6587\n",
            "Epoch 16/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5716 - mae: 0.5958 - mse: 0.5716\n",
            "Epoch 17/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5066 - mae: 0.5609 - mse: 0.5066\n",
            "Epoch 18/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4439 - mae: 0.5255 - mse: 0.4439\n",
            "Epoch 19/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3679 - mae: 0.4759 - mse: 0.3679\n",
            "Epoch 20/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3370 - mae: 0.4546 - mse: 0.3370\n",
            "Epoch 21/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2923 - mae: 0.4190 - mse: 0.2923\n",
            "Epoch 22/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2585 - mae: 0.3990 - mse: 0.2585\n",
            "Epoch 23/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2388 - mae: 0.3786 - mse: 0.2388\n",
            "Epoch 24/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2109 - mae: 0.3550 - mse: 0.2109\n",
            "Epoch 25/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1960 - mae: 0.3410 - mse: 0.1960\n",
            "Epoch 26/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1992 - mae: 0.3433 - mse: 0.1992\n",
            "Epoch 27/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1827 - mae: 0.3292 - mse: 0.1827\n",
            "Epoch 28/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1754 - mae: 0.3227 - mse: 0.1754\n",
            "Epoch 29/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1562 - mae: 0.3015 - mse: 0.1562\n",
            "Epoch 30/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1544 - mae: 0.3037 - mse: 0.1544\n",
            "Epoch 31/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1546 - mae: 0.3004 - mse: 0.1546\n",
            "Epoch 32/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1461 - mae: 0.2910 - mse: 0.1461\n",
            "Epoch 33/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1439 - mae: 0.2878 - mse: 0.1439\n",
            "Epoch 34/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1496 - mae: 0.2901 - mse: 0.1496\n",
            "Epoch 35/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1345 - mae: 0.2765 - mse: 0.1345\n",
            "Epoch 36/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1218 - mae: 0.2707 - mse: 0.1218\n",
            "Epoch 37/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1328 - mae: 0.2720 - mse: 0.1328\n",
            "Epoch 38/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1349 - mae: 0.2721 - mse: 0.1349\n",
            "Epoch 39/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1230 - mae: 0.2636 - mse: 0.1230\n",
            "Epoch 40/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1130 - mae: 0.2524 - mse: 0.1130\n",
            "Epoch 41/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1165 - mae: 0.2537 - mse: 0.1165\n",
            "Epoch 42/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1047 - mae: 0.2463 - mse: 0.1047\n",
            "Epoch 43/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1195 - mae: 0.2559 - mse: 0.1195\n",
            "Epoch 44/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1148 - mae: 0.2544 - mse: 0.1148\n",
            "Epoch 45/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1117 - mae: 0.2507 - mse: 0.1117\n",
            "Epoch 46/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1107 - mae: 0.2450 - mse: 0.1107\n",
            "Epoch 47/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0999 - mae: 0.2389 - mse: 0.0999\n",
            "Epoch 48/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0999 - mae: 0.2361 - mse: 0.0999\n",
            "Epoch 49/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1082 - mae: 0.2432 - mse: 0.1082\n",
            "Epoch 50/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0976 - mae: 0.2328 - mse: 0.0976\n",
            "Epoch 51/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0892 - mae: 0.2247 - mse: 0.0892\n",
            "Epoch 52/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1038 - mae: 0.2307 - mse: 0.1038\n",
            "Epoch 53/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0861 - mae: 0.2175 - mse: 0.0861\n",
            "Epoch 54/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0922 - mae: 0.2235 - mse: 0.0922\n",
            "Epoch 55/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0910 - mae: 0.2285 - mse: 0.0910\n",
            "Epoch 56/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0838 - mae: 0.2138 - mse: 0.0838\n",
            "Epoch 57/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0841 - mae: 0.2170 - mse: 0.0841\n",
            "Epoch 58/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0818 - mae: 0.2150 - mse: 0.0818\n",
            "Epoch 59/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0772 - mae: 0.2039 - mse: 0.0772\n",
            "Epoch 60/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0818 - mae: 0.2103 - mse: 0.0818\n",
            "Epoch 61/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0833 - mae: 0.2123 - mse: 0.0833\n",
            "Epoch 62/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0790 - mae: 0.2088 - mse: 0.0790\n",
            "Epoch 63/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0858 - mae: 0.2139 - mse: 0.0858\n",
            "Epoch 64/139\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0769 - mae: 0.2035 - mse: 0.0769\n",
            "Epoch 65/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0870 - mae: 0.2135 - mse: 0.0870\n",
            "Epoch 66/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0707 - mae: 0.1987 - mse: 0.0707\n",
            "Epoch 67/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0708 - mae: 0.2008 - mse: 0.0708\n",
            "Epoch 68/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0754 - mae: 0.2013 - mse: 0.0754\n",
            "Epoch 69/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0684 - mae: 0.1945 - mse: 0.0684\n",
            "Epoch 70/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0750 - mae: 0.1982 - mse: 0.0750\n",
            "Epoch 71/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0665 - mae: 0.1894 - mse: 0.0665\n",
            "Epoch 72/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0732 - mae: 0.1986 - mse: 0.0732\n",
            "Epoch 73/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0763 - mae: 0.2027 - mse: 0.0763\n",
            "Epoch 74/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0670 - mae: 0.1925 - mse: 0.0670\n",
            "Epoch 75/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0702 - mae: 0.1966 - mse: 0.0702\n",
            "Epoch 76/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0765 - mae: 0.1928 - mse: 0.0765\n",
            "Epoch 77/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0716 - mae: 0.1926 - mse: 0.0716\n",
            "Epoch 78/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0670 - mae: 0.1854 - mse: 0.0670\n",
            "Epoch 79/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0633 - mae: 0.1806 - mse: 0.0633\n",
            "Epoch 80/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0728 - mae: 0.1880 - mse: 0.0728\n",
            "Epoch 81/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0625 - mae: 0.1820 - mse: 0.0625\n",
            "Epoch 82/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0727 - mae: 0.1948 - mse: 0.0727\n",
            "Epoch 83/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0602 - mae: 0.1796 - mse: 0.0602\n",
            "Epoch 84/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.1833 - mse: 0.0648\n",
            "Epoch 85/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0670 - mae: 0.1865 - mse: 0.0670\n",
            "Epoch 86/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0626 - mae: 0.1789 - mse: 0.0626\n",
            "Epoch 87/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0581 - mae: 0.1759 - mse: 0.0581\n",
            "Epoch 88/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0611 - mae: 0.1802 - mse: 0.0611\n",
            "Epoch 89/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0648 - mae: 0.1839 - mse: 0.0648\n",
            "Epoch 90/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0629 - mae: 0.1776 - mse: 0.0629\n",
            "Epoch 91/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0638 - mae: 0.1807 - mse: 0.0638\n",
            "Epoch 92/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0540 - mae: 0.1688 - mse: 0.0540\n",
            "Epoch 93/139\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0590 - mae: 0.1731 - mse: 0.0590\n",
            "Epoch 94/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0591 - mae: 0.1753 - mse: 0.0591\n",
            "Epoch 95/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0548 - mae: 0.1698 - mse: 0.0548\n",
            "Epoch 96/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0525 - mae: 0.1645 - mse: 0.0525\n",
            "Epoch 97/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0580 - mae: 0.1706 - mse: 0.0580\n",
            "Epoch 98/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0590 - mae: 0.1742 - mse: 0.0590\n",
            "Epoch 99/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0575 - mae: 0.1712 - mse: 0.0575\n",
            "Epoch 100/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0589 - mae: 0.1724 - mse: 0.0589\n",
            "Epoch 101/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0583 - mae: 0.1735 - mse: 0.0583\n",
            "Epoch 102/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0522 - mae: 0.1634 - mse: 0.0522\n",
            "Epoch 103/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1614 - mse: 0.0532\n",
            "Epoch 104/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0577 - mae: 0.1711 - mse: 0.0577\n",
            "Epoch 105/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1656 - mse: 0.0532\n",
            "Epoch 106/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0560 - mae: 0.1658 - mse: 0.0560\n",
            "Epoch 107/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0556 - mae: 0.1689 - mse: 0.0556\n",
            "Epoch 108/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0559 - mae: 0.1670 - mse: 0.0559\n",
            "Epoch 109/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0526 - mae: 0.1671 - mse: 0.0526\n",
            "Epoch 110/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0541 - mae: 0.1665 - mse: 0.0541\n",
            "Epoch 111/139\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0528 - mae: 0.1672 - mse: 0.0528\n",
            "Epoch 112/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0557 - mae: 0.1659 - mse: 0.0557\n",
            "Epoch 113/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0588 - mae: 0.1707 - mse: 0.0588\n",
            "Epoch 114/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0578 - mae: 0.1676 - mse: 0.0578\n",
            "Epoch 115/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0495 - mae: 0.1586 - mse: 0.0495\n",
            "Epoch 116/139\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0504 - mae: 0.1586 - mse: 0.0504\n",
            "Epoch 117/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0509 - mae: 0.1605 - mse: 0.0509\n",
            "Epoch 118/139\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0554 - mae: 0.1656 - mse: 0.0554\n",
            "Epoch 119/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0513 - mae: 0.1635 - mse: 0.0513\n",
            "Epoch 120/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0491 - mae: 0.1545 - mse: 0.0491\n",
            "Epoch 121/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0481 - mae: 0.1588 - mse: 0.0481\n",
            "Epoch 122/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0516 - mae: 0.1614 - mse: 0.0516\n",
            "Epoch 123/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0507 - mae: 0.1573 - mse: 0.0507\n",
            "Epoch 124/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0509 - mae: 0.1597 - mse: 0.0509\n",
            "Epoch 125/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0570 - mae: 0.1643 - mse: 0.0570\n",
            "Epoch 126/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0509 - mae: 0.1604 - mse: 0.0509\n",
            "Epoch 127/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0555 - mae: 0.1671 - mse: 0.0555\n",
            "Epoch 128/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0479 - mae: 0.1569 - mse: 0.0479\n",
            "Epoch 129/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0501 - mae: 0.1602 - mse: 0.0501\n",
            "Epoch 130/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0456 - mae: 0.1538 - mse: 0.0456\n",
            "Epoch 131/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0522 - mae: 0.1626 - mse: 0.0522\n",
            "Epoch 132/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0545 - mae: 0.1624 - mse: 0.0545\n",
            "Epoch 133/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0430 - mae: 0.1485 - mse: 0.0430\n",
            "Epoch 134/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0444 - mae: 0.1499 - mse: 0.0444\n",
            "Epoch 135/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0438 - mae: 0.1520 - mse: 0.0438\n",
            "Epoch 136/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0506 - mae: 0.1582 - mse: 0.0506\n",
            "Epoch 137/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0435 - mae: 0.1467 - mse: 0.0435\n",
            "Epoch 138/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0471 - mae: 0.1523 - mse: 0.0471\n",
            "Epoch 139/139\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0502 - mae: 0.1581 - mse: 0.0502\n",
            "9/9 [==============================] - 1s 3ms/step\n",
            "Epoch 1/2000\n",
            "7/7 [==============================] - 6s 176ms/step - loss: 0.9024 - mae: 0.6385 - mse: 0.9024 - val_loss: 0.5521 - val_mae: 0.5764 - val_mse: 0.5521 - lr: 1.0000e-04\n",
            "Epoch 2/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9042 - mae: 0.6332 - mse: 0.9042 - val_loss: 0.5476 - val_mae: 0.5711 - val_mse: 0.5476 - lr: 1.0000e-04\n",
            "Epoch 3/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9039 - mae: 0.6327 - mse: 0.9039 - val_loss: 0.5439 - val_mae: 0.5665 - val_mse: 0.5439 - lr: 1.0000e-04\n",
            "Epoch 4/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8957 - mae: 0.6252 - mse: 0.8957 - val_loss: 0.5408 - val_mae: 0.5627 - val_mse: 0.5408 - lr: 1.0000e-04\n",
            "Epoch 5/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8971 - mae: 0.6232 - mse: 0.8971 - val_loss: 0.5384 - val_mae: 0.5598 - val_mse: 0.5384 - lr: 1.0000e-04\n",
            "Epoch 6/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8929 - mae: 0.6201 - mse: 0.8929 - val_loss: 0.5360 - val_mae: 0.5569 - val_mse: 0.5360 - lr: 1.0000e-04\n",
            "Epoch 7/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8972 - mae: 0.6183 - mse: 0.8972 - val_loss: 0.5338 - val_mae: 0.5542 - val_mse: 0.5338 - lr: 1.0000e-04\n",
            "Epoch 8/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8936 - mae: 0.6143 - mse: 0.8936 - val_loss: 0.5318 - val_mae: 0.5518 - val_mse: 0.5318 - lr: 1.0000e-04\n",
            "Epoch 9/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8897 - mae: 0.6131 - mse: 0.8897 - val_loss: 0.5302 - val_mae: 0.5502 - val_mse: 0.5302 - lr: 1.0000e-04\n",
            "Epoch 10/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8876 - mae: 0.6114 - mse: 0.8876 - val_loss: 0.5288 - val_mae: 0.5489 - val_mse: 0.5288 - lr: 1.0000e-04\n",
            "Epoch 11/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8873 - mae: 0.6115 - mse: 0.8873 - val_loss: 0.5274 - val_mae: 0.5477 - val_mse: 0.5274 - lr: 1.0000e-04\n",
            "Epoch 12/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8855 - mae: 0.6089 - mse: 0.8855 - val_loss: 0.5259 - val_mae: 0.5465 - val_mse: 0.5259 - lr: 1.0000e-04\n",
            "Epoch 13/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8798 - mae: 0.6064 - mse: 0.8798 - val_loss: 0.5242 - val_mae: 0.5450 - val_mse: 0.5242 - lr: 1.0000e-04\n",
            "Epoch 14/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8785 - mae: 0.6065 - mse: 0.8785 - val_loss: 0.5223 - val_mae: 0.5431 - val_mse: 0.5223 - lr: 1.0000e-04\n",
            "Epoch 15/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8795 - mae: 0.6037 - mse: 0.8795 - val_loss: 0.5205 - val_mae: 0.5416 - val_mse: 0.5205 - lr: 1.0000e-04\n",
            "Epoch 16/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8788 - mae: 0.6038 - mse: 0.8788 - val_loss: 0.5187 - val_mae: 0.5403 - val_mse: 0.5187 - lr: 1.0000e-04\n",
            "Epoch 17/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8760 - mae: 0.6023 - mse: 0.8760 - val_loss: 0.5166 - val_mae: 0.5387 - val_mse: 0.5166 - lr: 1.0000e-04\n",
            "Epoch 18/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8733 - mae: 0.6011 - mse: 0.8733 - val_loss: 0.5145 - val_mae: 0.5373 - val_mse: 0.5145 - lr: 1.0000e-04\n",
            "Epoch 19/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8734 - mae: 0.5986 - mse: 0.8734 - val_loss: 0.5125 - val_mae: 0.5364 - val_mse: 0.5125 - lr: 1.0000e-04\n",
            "Epoch 20/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8621 - mae: 0.5996 - mse: 0.8621 - val_loss: 0.5100 - val_mae: 0.5348 - val_mse: 0.5100 - lr: 1.0000e-04\n",
            "Epoch 21/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8604 - mae: 0.5952 - mse: 0.8604 - val_loss: 0.5074 - val_mae: 0.5331 - val_mse: 0.5074 - lr: 1.0000e-04\n",
            "Epoch 22/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8579 - mae: 0.5941 - mse: 0.8579 - val_loss: 0.5046 - val_mae: 0.5310 - val_mse: 0.5046 - lr: 1.0000e-04\n",
            "Epoch 23/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.8543 - mae: 0.5933 - mse: 0.8543 - val_loss: 0.5015 - val_mae: 0.5290 - val_mse: 0.5015 - lr: 1.0000e-04\n",
            "Epoch 24/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8485 - mae: 0.5890 - mse: 0.8485 - val_loss: 0.4982 - val_mae: 0.5266 - val_mse: 0.4982 - lr: 1.0000e-04\n",
            "Epoch 25/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8371 - mae: 0.5863 - mse: 0.8371 - val_loss: 0.4946 - val_mae: 0.5240 - val_mse: 0.4946 - lr: 1.0000e-04\n",
            "Epoch 26/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8406 - mae: 0.5858 - mse: 0.8406 - val_loss: 0.4912 - val_mae: 0.5225 - val_mse: 0.4912 - lr: 1.0000e-04\n",
            "Epoch 27/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8299 - mae: 0.5825 - mse: 0.8299 - val_loss: 0.4873 - val_mae: 0.5203 - val_mse: 0.4873 - lr: 1.0000e-04\n",
            "Epoch 28/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8327 - mae: 0.5796 - mse: 0.8327 - val_loss: 0.4828 - val_mae: 0.5169 - val_mse: 0.4828 - lr: 1.0000e-04\n",
            "Epoch 29/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8204 - mae: 0.5776 - mse: 0.8204 - val_loss: 0.4778 - val_mae: 0.5128 - val_mse: 0.4778 - lr: 1.0000e-04\n",
            "Epoch 30/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8059 - mae: 0.5720 - mse: 0.8059 - val_loss: 0.4728 - val_mae: 0.5097 - val_mse: 0.4728 - lr: 1.0000e-04\n",
            "Epoch 31/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.8118 - mae: 0.5702 - mse: 0.8118 - val_loss: 0.4675 - val_mae: 0.5063 - val_mse: 0.4675 - lr: 1.0000e-04\n",
            "Epoch 32/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7980 - mae: 0.5660 - mse: 0.7980 - val_loss: 0.4620 - val_mae: 0.5025 - val_mse: 0.4620 - lr: 1.0000e-04\n",
            "Epoch 33/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7884 - mae: 0.5620 - mse: 0.7884 - val_loss: 0.4560 - val_mae: 0.4977 - val_mse: 0.4560 - lr: 1.0000e-04\n",
            "Epoch 34/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7848 - mae: 0.5584 - mse: 0.7848 - val_loss: 0.4499 - val_mae: 0.4936 - val_mse: 0.4499 - lr: 1.0000e-04\n",
            "Epoch 35/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7695 - mae: 0.5502 - mse: 0.7695 - val_loss: 0.4433 - val_mae: 0.4882 - val_mse: 0.4433 - lr: 1.0000e-04\n",
            "Epoch 36/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7604 - mae: 0.5476 - mse: 0.7604 - val_loss: 0.4366 - val_mae: 0.4828 - val_mse: 0.4366 - lr: 1.0000e-04\n",
            "Epoch 37/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7477 - mae: 0.5427 - mse: 0.7477 - val_loss: 0.4297 - val_mae: 0.4779 - val_mse: 0.4297 - lr: 1.0000e-04\n",
            "Epoch 38/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7395 - mae: 0.5388 - mse: 0.7395 - val_loss: 0.4230 - val_mae: 0.4736 - val_mse: 0.4230 - lr: 1.0000e-04\n",
            "Epoch 39/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7116 - mae: 0.5299 - mse: 0.7116 - val_loss: 0.4163 - val_mae: 0.4688 - val_mse: 0.4163 - lr: 1.0000e-04\n",
            "Epoch 40/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7136 - mae: 0.5220 - mse: 0.7136 - val_loss: 0.4090 - val_mae: 0.4636 - val_mse: 0.4090 - lr: 1.0000e-04\n",
            "Epoch 41/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7058 - mae: 0.5210 - mse: 0.7058 - val_loss: 0.4019 - val_mae: 0.4597 - val_mse: 0.4019 - lr: 1.0000e-04\n",
            "Epoch 42/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6822 - mae: 0.5118 - mse: 0.6822 - val_loss: 0.3948 - val_mae: 0.4548 - val_mse: 0.3948 - lr: 1.0000e-04\n",
            "Epoch 43/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6746 - mae: 0.5059 - mse: 0.6746 - val_loss: 0.3880 - val_mae: 0.4489 - val_mse: 0.3880 - lr: 1.0000e-04\n",
            "Epoch 44/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6545 - mae: 0.5051 - mse: 0.6545 - val_loss: 0.3814 - val_mae: 0.4446 - val_mse: 0.3814 - lr: 1.0000e-04\n",
            "Epoch 45/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6550 - mae: 0.5027 - mse: 0.6550 - val_loss: 0.3749 - val_mae: 0.4407 - val_mse: 0.3749 - lr: 1.0000e-04\n",
            "Epoch 46/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6310 - mae: 0.4911 - mse: 0.6310 - val_loss: 0.3688 - val_mae: 0.4362 - val_mse: 0.3688 - lr: 1.0000e-04\n",
            "Epoch 47/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6094 - mae: 0.4835 - mse: 0.6094 - val_loss: 0.3626 - val_mae: 0.4322 - val_mse: 0.3626 - lr: 1.0000e-04\n",
            "Epoch 48/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5978 - mae: 0.4751 - mse: 0.5978 - val_loss: 0.3567 - val_mae: 0.4291 - val_mse: 0.3567 - lr: 1.0000e-04\n",
            "Epoch 49/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5971 - mae: 0.4769 - mse: 0.5971 - val_loss: 0.3514 - val_mae: 0.4260 - val_mse: 0.3514 - lr: 1.0000e-04\n",
            "Epoch 50/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5980 - mae: 0.4732 - mse: 0.5980 - val_loss: 0.3462 - val_mae: 0.4226 - val_mse: 0.3462 - lr: 1.0000e-04\n",
            "Epoch 51/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5653 - mae: 0.4691 - mse: 0.5653 - val_loss: 0.3414 - val_mae: 0.4193 - val_mse: 0.3414 - lr: 1.0000e-04\n",
            "Epoch 52/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5401 - mae: 0.4637 - mse: 0.5401 - val_loss: 0.3368 - val_mae: 0.4164 - val_mse: 0.3368 - lr: 1.0000e-04\n",
            "Epoch 53/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5285 - mae: 0.4527 - mse: 0.5285 - val_loss: 0.3319 - val_mae: 0.4144 - val_mse: 0.3319 - lr: 1.0000e-04\n",
            "Epoch 54/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5352 - mae: 0.4539 - mse: 0.5352 - val_loss: 0.3273 - val_mae: 0.4119 - val_mse: 0.3273 - lr: 1.0000e-04\n",
            "Epoch 55/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5253 - mae: 0.4475 - mse: 0.5253 - val_loss: 0.3229 - val_mae: 0.4094 - val_mse: 0.3229 - lr: 1.0000e-04\n",
            "Epoch 56/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5230 - mae: 0.4448 - mse: 0.5230 - val_loss: 0.3191 - val_mae: 0.4062 - val_mse: 0.3191 - lr: 1.0000e-04\n",
            "Epoch 57/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5020 - mae: 0.4348 - mse: 0.5020 - val_loss: 0.3149 - val_mae: 0.4030 - val_mse: 0.3149 - lr: 1.0000e-04\n",
            "Epoch 58/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4872 - mae: 0.4350 - mse: 0.4872 - val_loss: 0.3103 - val_mae: 0.4001 - val_mse: 0.3103 - lr: 1.0000e-04\n",
            "Epoch 59/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4897 - mae: 0.4340 - mse: 0.4897 - val_loss: 0.3067 - val_mae: 0.3971 - val_mse: 0.3067 - lr: 1.0000e-04\n",
            "Epoch 60/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4771 - mae: 0.4305 - mse: 0.4771 - val_loss: 0.3021 - val_mae: 0.3941 - val_mse: 0.3021 - lr: 1.0000e-04\n",
            "Epoch 61/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4658 - mae: 0.4209 - mse: 0.4658 - val_loss: 0.2962 - val_mae: 0.3910 - val_mse: 0.2962 - lr: 1.0000e-04\n",
            "Epoch 62/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4398 - mae: 0.4137 - mse: 0.4398 - val_loss: 0.2915 - val_mae: 0.3879 - val_mse: 0.2915 - lr: 1.0000e-04\n",
            "Epoch 63/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4461 - mae: 0.4194 - mse: 0.4461 - val_loss: 0.2860 - val_mae: 0.3843 - val_mse: 0.2860 - lr: 1.0000e-04\n",
            "Epoch 64/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3990 - mae: 0.4027 - mse: 0.3990 - val_loss: 0.2812 - val_mae: 0.3810 - val_mse: 0.2812 - lr: 1.0000e-04\n",
            "Epoch 65/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4089 - mae: 0.4032 - mse: 0.4089 - val_loss: 0.2767 - val_mae: 0.3778 - val_mse: 0.2767 - lr: 1.0000e-04\n",
            "Epoch 66/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3931 - mae: 0.3912 - mse: 0.3931 - val_loss: 0.2727 - val_mae: 0.3747 - val_mse: 0.2727 - lr: 1.0000e-04\n",
            "Epoch 67/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3872 - mae: 0.3910 - mse: 0.3872 - val_loss: 0.2677 - val_mae: 0.3711 - val_mse: 0.2677 - lr: 1.0000e-04\n",
            "Epoch 68/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4039 - mae: 0.3952 - mse: 0.4039 - val_loss: 0.2619 - val_mae: 0.3676 - val_mse: 0.2619 - lr: 1.0000e-04\n",
            "Epoch 69/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3810 - mae: 0.3827 - mse: 0.3810 - val_loss: 0.2562 - val_mae: 0.3640 - val_mse: 0.2562 - lr: 1.0000e-04\n",
            "Epoch 70/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3397 - mae: 0.3715 - mse: 0.3397 - val_loss: 0.2512 - val_mae: 0.3607 - val_mse: 0.2512 - lr: 1.0000e-04\n",
            "Epoch 71/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3498 - mae: 0.3686 - mse: 0.3498 - val_loss: 0.2455 - val_mae: 0.3571 - val_mse: 0.2455 - lr: 1.0000e-04\n",
            "Epoch 72/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3547 - mae: 0.3647 - mse: 0.3547 - val_loss: 0.2406 - val_mae: 0.3539 - val_mse: 0.2406 - lr: 1.0000e-04\n",
            "Epoch 73/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3480 - mae: 0.3662 - mse: 0.3480 - val_loss: 0.2356 - val_mae: 0.3508 - val_mse: 0.2356 - lr: 1.0000e-04\n",
            "Epoch 74/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3421 - mae: 0.3571 - mse: 0.3421 - val_loss: 0.2310 - val_mae: 0.3477 - val_mse: 0.2310 - lr: 1.0000e-04\n",
            "Epoch 75/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3258 - mae: 0.3497 - mse: 0.3258 - val_loss: 0.2259 - val_mae: 0.3443 - val_mse: 0.2259 - lr: 1.0000e-04\n",
            "Epoch 76/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3039 - mae: 0.3475 - mse: 0.3039 - val_loss: 0.2208 - val_mae: 0.3409 - val_mse: 0.2208 - lr: 1.0000e-04\n",
            "Epoch 77/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2848 - mae: 0.3434 - mse: 0.2848 - val_loss: 0.2163 - val_mae: 0.3378 - val_mse: 0.2163 - lr: 1.0000e-04\n",
            "Epoch 78/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3272 - mae: 0.3470 - mse: 0.3272 - val_loss: 0.2112 - val_mae: 0.3344 - val_mse: 0.2112 - lr: 1.0000e-04\n",
            "Epoch 79/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2849 - mae: 0.3342 - mse: 0.2849 - val_loss: 0.2073 - val_mae: 0.3311 - val_mse: 0.2073 - lr: 1.0000e-04\n",
            "Epoch 80/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2784 - mae: 0.3235 - mse: 0.2784 - val_loss: 0.2030 - val_mae: 0.3280 - val_mse: 0.2030 - lr: 1.0000e-04\n",
            "Epoch 81/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2795 - mae: 0.3238 - mse: 0.2795 - val_loss: 0.1992 - val_mae: 0.3250 - val_mse: 0.1992 - lr: 1.0000e-04\n",
            "Epoch 82/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.2516 - mae: 0.3165 - mse: 0.2516 - val_loss: 0.1953 - val_mae: 0.3218 - val_mse: 0.1953 - lr: 1.0000e-04\n",
            "Epoch 83/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2834 - mae: 0.3188 - mse: 0.2834 - val_loss: 0.1912 - val_mae: 0.3186 - val_mse: 0.1912 - lr: 1.0000e-04\n",
            "Epoch 84/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2735 - mae: 0.3185 - mse: 0.2735 - val_loss: 0.1870 - val_mae: 0.3160 - val_mse: 0.1870 - lr: 1.0000e-04\n",
            "Epoch 85/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2472 - mae: 0.3094 - mse: 0.2472 - val_loss: 0.1837 - val_mae: 0.3132 - val_mse: 0.1837 - lr: 1.0000e-04\n",
            "Epoch 86/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2200 - mae: 0.3000 - mse: 0.2200 - val_loss: 0.1816 - val_mae: 0.3106 - val_mse: 0.1816 - lr: 1.0000e-04\n",
            "Epoch 87/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2102 - mae: 0.2941 - mse: 0.2102 - val_loss: 0.1789 - val_mae: 0.3081 - val_mse: 0.1789 - lr: 1.0000e-04\n",
            "Epoch 88/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2523 - mae: 0.3085 - mse: 0.2523 - val_loss: 0.1755 - val_mae: 0.3057 - val_mse: 0.1755 - lr: 1.0000e-04\n",
            "Epoch 89/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2348 - mae: 0.3051 - mse: 0.2348 - val_loss: 0.1725 - val_mae: 0.3033 - val_mse: 0.1725 - lr: 1.0000e-04\n",
            "Epoch 90/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2161 - mae: 0.2925 - mse: 0.2161 - val_loss: 0.1699 - val_mae: 0.3006 - val_mse: 0.1699 - lr: 1.0000e-04\n",
            "Epoch 91/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2111 - mae: 0.2914 - mse: 0.2111 - val_loss: 0.1680 - val_mae: 0.2982 - val_mse: 0.1680 - lr: 1.0000e-04\n",
            "Epoch 92/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1937 - mae: 0.2824 - mse: 0.1937 - val_loss: 0.1656 - val_mae: 0.2960 - val_mse: 0.1656 - lr: 1.0000e-04\n",
            "Epoch 93/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2430 - mae: 0.2897 - mse: 0.2430 - val_loss: 0.1635 - val_mae: 0.2942 - val_mse: 0.1635 - lr: 1.0000e-04\n",
            "Epoch 94/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2023 - mae: 0.2863 - mse: 0.2023 - val_loss: 0.1616 - val_mae: 0.2926 - val_mse: 0.1616 - lr: 1.0000e-04\n",
            "Epoch 95/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2175 - mae: 0.2859 - mse: 0.2175 - val_loss: 0.1597 - val_mae: 0.2912 - val_mse: 0.1597 - lr: 1.0000e-04\n",
            "Epoch 96/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2413 - mae: 0.2988 - mse: 0.2413 - val_loss: 0.1581 - val_mae: 0.2898 - val_mse: 0.1581 - lr: 1.0000e-04\n",
            "Epoch 97/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2232 - mae: 0.2912 - mse: 0.2232 - val_loss: 0.1565 - val_mae: 0.2882 - val_mse: 0.1565 - lr: 1.0000e-04\n",
            "Epoch 98/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1786 - mae: 0.2823 - mse: 0.1786 - val_loss: 0.1550 - val_mae: 0.2868 - val_mse: 0.1550 - lr: 1.0000e-04\n",
            "Epoch 99/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1987 - mae: 0.2754 - mse: 0.1987 - val_loss: 0.1533 - val_mae: 0.2857 - val_mse: 0.1533 - lr: 1.0000e-04\n",
            "Epoch 100/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1706 - mae: 0.2722 - mse: 0.1706 - val_loss: 0.1517 - val_mae: 0.2850 - val_mse: 0.1517 - lr: 1.0000e-04\n",
            "Epoch 101/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1832 - mae: 0.2768 - mse: 0.1832 - val_loss: 0.1506 - val_mae: 0.2844 - val_mse: 0.1506 - lr: 1.0000e-04\n",
            "Epoch 102/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1699 - mae: 0.2707 - mse: 0.1699 - val_loss: 0.1497 - val_mae: 0.2840 - val_mse: 0.1497 - lr: 1.0000e-04\n",
            "Epoch 103/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1486 - mae: 0.2628 - mse: 0.1486 - val_loss: 0.1484 - val_mae: 0.2833 - val_mse: 0.1484 - lr: 1.0000e-04\n",
            "Epoch 104/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1821 - mae: 0.2696 - mse: 0.1821 - val_loss: 0.1469 - val_mae: 0.2827 - val_mse: 0.1469 - lr: 1.0000e-04\n",
            "Epoch 105/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1910 - mae: 0.2811 - mse: 0.1910 - val_loss: 0.1455 - val_mae: 0.2819 - val_mse: 0.1455 - lr: 1.0000e-04\n",
            "Epoch 106/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1728 - mae: 0.2706 - mse: 0.1728 - val_loss: 0.1440 - val_mae: 0.2813 - val_mse: 0.1440 - lr: 1.0000e-04\n",
            "Epoch 107/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1748 - mae: 0.2728 - mse: 0.1748 - val_loss: 0.1431 - val_mae: 0.2809 - val_mse: 0.1431 - lr: 1.0000e-04\n",
            "Epoch 108/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1738 - mae: 0.2671 - mse: 0.1738 - val_loss: 0.1421 - val_mae: 0.2805 - val_mse: 0.1421 - lr: 1.0000e-04\n",
            "Epoch 109/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1702 - mae: 0.2715 - mse: 0.1702 - val_loss: 0.1409 - val_mae: 0.2799 - val_mse: 0.1409 - lr: 1.0000e-04\n",
            "Epoch 110/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1773 - mae: 0.2701 - mse: 0.1773 - val_loss: 0.1400 - val_mae: 0.2789 - val_mse: 0.1400 - lr: 1.0000e-04\n",
            "Epoch 111/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1513 - mae: 0.2646 - mse: 0.1513 - val_loss: 0.1386 - val_mae: 0.2782 - val_mse: 0.1386 - lr: 1.0000e-04\n",
            "Epoch 112/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1558 - mae: 0.2558 - mse: 0.1558 - val_loss: 0.1373 - val_mae: 0.2778 - val_mse: 0.1373 - lr: 1.0000e-04\n",
            "Epoch 113/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.1490 - mae: 0.2533 - mse: 0.1490 - val_loss: 0.1361 - val_mae: 0.2767 - val_mse: 0.1361 - lr: 1.0000e-04\n",
            "Epoch 114/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1496 - mae: 0.2521 - mse: 0.1496 - val_loss: 0.1351 - val_mae: 0.2752 - val_mse: 0.1351 - lr: 1.0000e-04\n",
            "Epoch 115/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1447 - mae: 0.2556 - mse: 0.1447 - val_loss: 0.1337 - val_mae: 0.2748 - val_mse: 0.1337 - lr: 1.0000e-04\n",
            "Epoch 116/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1475 - mae: 0.2543 - mse: 0.1475 - val_loss: 0.1325 - val_mae: 0.2732 - val_mse: 0.1325 - lr: 1.0000e-04\n",
            "Epoch 117/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1359 - mae: 0.2520 - mse: 0.1359 - val_loss: 0.1314 - val_mae: 0.2723 - val_mse: 0.1314 - lr: 1.0000e-04\n",
            "Epoch 118/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1764 - mae: 0.2626 - mse: 0.1764 - val_loss: 0.1304 - val_mae: 0.2716 - val_mse: 0.1304 - lr: 1.0000e-04\n",
            "Epoch 119/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1530 - mae: 0.2547 - mse: 0.1530 - val_loss: 0.1292 - val_mae: 0.2712 - val_mse: 0.1292 - lr: 1.0000e-04\n",
            "Epoch 120/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1517 - mae: 0.2461 - mse: 0.1517 - val_loss: 0.1282 - val_mae: 0.2698 - val_mse: 0.1282 - lr: 1.0000e-04\n",
            "Epoch 121/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1454 - mae: 0.2538 - mse: 0.1454 - val_loss: 0.1276 - val_mae: 0.2688 - val_mse: 0.1276 - lr: 1.0000e-04\n",
            "Epoch 122/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1401 - mae: 0.2527 - mse: 0.1401 - val_loss: 0.1260 - val_mae: 0.2684 - val_mse: 0.1260 - lr: 1.0000e-04\n",
            "Epoch 123/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1623 - mae: 0.2551 - mse: 0.1623 - val_loss: 0.1248 - val_mae: 0.2676 - val_mse: 0.1248 - lr: 1.0000e-04\n",
            "Epoch 124/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1566 - mae: 0.2475 - mse: 0.1566 - val_loss: 0.1236 - val_mae: 0.2663 - val_mse: 0.1236 - lr: 1.0000e-04\n",
            "Epoch 125/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1448 - mae: 0.2468 - mse: 0.1448 - val_loss: 0.1229 - val_mae: 0.2671 - val_mse: 0.1229 - lr: 1.0000e-04\n",
            "Epoch 126/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1442 - mae: 0.2515 - mse: 0.1442 - val_loss: 0.1218 - val_mae: 0.2648 - val_mse: 0.1218 - lr: 1.0000e-04\n",
            "Epoch 127/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1151 - mae: 0.2399 - mse: 0.1151 - val_loss: 0.1208 - val_mae: 0.2630 - val_mse: 0.1208 - lr: 1.0000e-04\n",
            "Epoch 128/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1370 - mae: 0.2398 - mse: 0.1370 - val_loss: 0.1197 - val_mae: 0.2616 - val_mse: 0.1197 - lr: 1.0000e-04\n",
            "Epoch 129/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1249 - mae: 0.2358 - mse: 0.1249 - val_loss: 0.1187 - val_mae: 0.2605 - val_mse: 0.1187 - lr: 1.0000e-04\n",
            "Epoch 130/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1261 - mae: 0.2409 - mse: 0.1261 - val_loss: 0.1177 - val_mae: 0.2595 - val_mse: 0.1177 - lr: 1.0000e-04\n",
            "Epoch 131/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1312 - mae: 0.2386 - mse: 0.1312 - val_loss: 0.1165 - val_mae: 0.2587 - val_mse: 0.1165 - lr: 1.0000e-04\n",
            "Epoch 132/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1057 - mae: 0.2311 - mse: 0.1057 - val_loss: 0.1156 - val_mae: 0.2568 - val_mse: 0.1156 - lr: 1.0000e-04\n",
            "Epoch 133/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1289 - mae: 0.2347 - mse: 0.1289 - val_loss: 0.1145 - val_mae: 0.2553 - val_mse: 0.1145 - lr: 1.0000e-04\n",
            "Epoch 134/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1150 - mae: 0.2358 - mse: 0.1150 - val_loss: 0.1134 - val_mae: 0.2539 - val_mse: 0.1134 - lr: 1.0000e-04\n",
            "Epoch 135/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1301 - mae: 0.2393 - mse: 0.1301 - val_loss: 0.1121 - val_mae: 0.2534 - val_mse: 0.1121 - lr: 1.0000e-04\n",
            "Epoch 136/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1475 - mae: 0.2440 - mse: 0.1475 - val_loss: 0.1111 - val_mae: 0.2525 - val_mse: 0.1111 - lr: 1.0000e-04\n",
            "Epoch 137/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1131 - mae: 0.2301 - mse: 0.1131 - val_loss: 0.1105 - val_mae: 0.2510 - val_mse: 0.1105 - lr: 1.0000e-04\n",
            "Epoch 138/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1169 - mae: 0.2320 - mse: 0.1169 - val_loss: 0.1094 - val_mae: 0.2501 - val_mse: 0.1094 - lr: 1.0000e-04\n",
            "Epoch 139/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1247 - mae: 0.2326 - mse: 0.1247 - val_loss: 0.1085 - val_mae: 0.2493 - val_mse: 0.1085 - lr: 1.0000e-04\n",
            "Epoch 140/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1212 - mae: 0.2280 - mse: 0.1212 - val_loss: 0.1077 - val_mae: 0.2484 - val_mse: 0.1077 - lr: 1.0000e-04\n",
            "Epoch 141/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1281 - mae: 0.2341 - mse: 0.1281 - val_loss: 0.1068 - val_mae: 0.2475 - val_mse: 0.1068 - lr: 1.0000e-04\n",
            "Epoch 142/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1673 - mae: 0.2328 - mse: 0.1673 - val_loss: 0.1059 - val_mae: 0.2466 - val_mse: 0.1059 - lr: 1.0000e-04\n",
            "Epoch 143/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1282 - mae: 0.2228 - mse: 0.1282 - val_loss: 0.1050 - val_mae: 0.2454 - val_mse: 0.1050 - lr: 1.0000e-04\n",
            "Epoch 144/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1136 - mae: 0.2255 - mse: 0.1136 - val_loss: 0.1040 - val_mae: 0.2452 - val_mse: 0.1040 - lr: 1.0000e-04\n",
            "Epoch 145/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1325 - mae: 0.2298 - mse: 0.1325 - val_loss: 0.1032 - val_mae: 0.2445 - val_mse: 0.1032 - lr: 1.0000e-04\n",
            "Epoch 146/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1377 - mae: 0.2302 - mse: 0.1377 - val_loss: 0.1026 - val_mae: 0.2424 - val_mse: 0.1026 - lr: 1.0000e-04\n",
            "Epoch 147/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1176 - mae: 0.2272 - mse: 0.1176 - val_loss: 0.1019 - val_mae: 0.2433 - val_mse: 0.1019 - lr: 1.0000e-04\n",
            "Epoch 148/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0945 - mae: 0.2221 - mse: 0.0945 - val_loss: 0.1009 - val_mae: 0.2412 - val_mse: 0.1009 - lr: 1.0000e-04\n",
            "Epoch 149/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1048 - mae: 0.2139 - mse: 0.1048 - val_loss: 0.1003 - val_mae: 0.2396 - val_mse: 0.1003 - lr: 1.0000e-04\n",
            "Epoch 150/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1167 - mae: 0.2261 - mse: 0.1167 - val_loss: 0.0996 - val_mae: 0.2386 - val_mse: 0.0996 - lr: 1.0000e-04\n",
            "Epoch 151/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1134 - mae: 0.2172 - mse: 0.1134 - val_loss: 0.0986 - val_mae: 0.2376 - val_mse: 0.0986 - lr: 1.0000e-04\n",
            "Epoch 152/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1111 - mae: 0.2184 - mse: 0.1111 - val_loss: 0.0980 - val_mae: 0.2364 - val_mse: 0.0980 - lr: 1.0000e-04\n",
            "Epoch 153/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1268 - mae: 0.2208 - mse: 0.1268 - val_loss: 0.0974 - val_mae: 0.2363 - val_mse: 0.0974 - lr: 1.0000e-04\n",
            "Epoch 154/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1186 - mae: 0.2234 - mse: 0.1186 - val_loss: 0.0968 - val_mae: 0.2355 - val_mse: 0.0968 - lr: 1.0000e-04\n",
            "Epoch 155/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0948 - mae: 0.2099 - mse: 0.0948 - val_loss: 0.0962 - val_mae: 0.2346 - val_mse: 0.0962 - lr: 1.0000e-04\n",
            "Epoch 156/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1056 - mae: 0.2167 - mse: 0.1056 - val_loss: 0.0958 - val_mae: 0.2331 - val_mse: 0.0958 - lr: 1.0000e-04\n",
            "Epoch 157/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1200 - mae: 0.2172 - mse: 0.1200 - val_loss: 0.0948 - val_mae: 0.2321 - val_mse: 0.0948 - lr: 1.0000e-04\n",
            "Epoch 158/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0941 - mae: 0.2150 - mse: 0.0941 - val_loss: 0.0943 - val_mae: 0.2310 - val_mse: 0.0943 - lr: 1.0000e-04\n",
            "Epoch 159/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0908 - mae: 0.2087 - mse: 0.0908 - val_loss: 0.0937 - val_mae: 0.2302 - val_mse: 0.0937 - lr: 1.0000e-04\n",
            "Epoch 160/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1012 - mae: 0.2140 - mse: 0.1012 - val_loss: 0.0926 - val_mae: 0.2290 - val_mse: 0.0926 - lr: 1.0000e-04\n",
            "Epoch 161/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1074 - mae: 0.2148 - mse: 0.1074 - val_loss: 0.0917 - val_mae: 0.2290 - val_mse: 0.0917 - lr: 1.0000e-04\n",
            "Epoch 162/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1224 - mae: 0.2147 - mse: 0.1224 - val_loss: 0.0910 - val_mae: 0.2269 - val_mse: 0.0910 - lr: 1.0000e-04\n",
            "Epoch 163/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1062 - mae: 0.2081 - mse: 0.1062 - val_loss: 0.0905 - val_mae: 0.2260 - val_mse: 0.0905 - lr: 1.0000e-04\n",
            "Epoch 164/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1020 - mae: 0.2120 - mse: 0.1020 - val_loss: 0.0893 - val_mae: 0.2249 - val_mse: 0.0893 - lr: 1.0000e-04\n",
            "Epoch 165/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1117 - mae: 0.2126 - mse: 0.1117 - val_loss: 0.0885 - val_mae: 0.2241 - val_mse: 0.0885 - lr: 1.0000e-04\n",
            "Epoch 166/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0922 - mae: 0.2021 - mse: 0.0922 - val_loss: 0.0880 - val_mae: 0.2228 - val_mse: 0.0880 - lr: 1.0000e-04\n",
            "Epoch 167/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0908 - mae: 0.2025 - mse: 0.0908 - val_loss: 0.0875 - val_mae: 0.2219 - val_mse: 0.0875 - lr: 1.0000e-04\n",
            "Epoch 168/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0991 - mae: 0.2077 - mse: 0.0991 - val_loss: 0.0868 - val_mae: 0.2217 - val_mse: 0.0868 - lr: 1.0000e-04\n",
            "Epoch 169/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1091 - mae: 0.2068 - mse: 0.1091 - val_loss: 0.0861 - val_mae: 0.2214 - val_mse: 0.0861 - lr: 1.0000e-04\n",
            "Epoch 170/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0883 - mae: 0.1989 - mse: 0.0883 - val_loss: 0.0854 - val_mae: 0.2200 - val_mse: 0.0854 - lr: 1.0000e-04\n",
            "Epoch 171/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0836 - mae: 0.1986 - mse: 0.0836 - val_loss: 0.0850 - val_mae: 0.2192 - val_mse: 0.0850 - lr: 1.0000e-04\n",
            "Epoch 172/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1048 - mae: 0.2024 - mse: 0.1048 - val_loss: 0.0840 - val_mae: 0.2189 - val_mse: 0.0840 - lr: 1.0000e-04\n",
            "Epoch 173/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1019 - mae: 0.2043 - mse: 0.1019 - val_loss: 0.0836 - val_mae: 0.2175 - val_mse: 0.0836 - lr: 1.0000e-04\n",
            "Epoch 174/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1066 - mae: 0.2142 - mse: 0.1066 - val_loss: 0.0845 - val_mae: 0.2180 - val_mse: 0.0845 - lr: 1.0000e-04\n",
            "Epoch 175/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0847 - mae: 0.2048 - mse: 0.0847 - val_loss: 0.0828 - val_mae: 0.2165 - val_mse: 0.0828 - lr: 1.0000e-04\n",
            "Epoch 176/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1184 - mae: 0.2044 - mse: 0.1184 - val_loss: 0.0821 - val_mae: 0.2157 - val_mse: 0.0821 - lr: 1.0000e-04\n",
            "Epoch 177/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1085 - mae: 0.2060 - mse: 0.1085 - val_loss: 0.0816 - val_mae: 0.2133 - val_mse: 0.0816 - lr: 1.0000e-04\n",
            "Epoch 178/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0865 - mae: 0.1993 - mse: 0.0865 - val_loss: 0.0802 - val_mae: 0.2120 - val_mse: 0.0802 - lr: 1.0000e-04\n",
            "Epoch 179/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.0996 - mae: 0.1998 - mse: 0.0996 - val_loss: 0.0796 - val_mae: 0.2112 - val_mse: 0.0796 - lr: 1.0000e-04\n",
            "Epoch 180/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0979 - mae: 0.1961 - mse: 0.0979 - val_loss: 0.0790 - val_mae: 0.2100 - val_mse: 0.0790 - lr: 1.0000e-04\n",
            "Epoch 181/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0884 - mae: 0.1975 - mse: 0.0884 - val_loss: 0.0782 - val_mae: 0.2096 - val_mse: 0.0782 - lr: 1.0000e-04\n",
            "Epoch 182/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0723 - mae: 0.1845 - mse: 0.0723 - val_loss: 0.0777 - val_mae: 0.2090 - val_mse: 0.0777 - lr: 1.0000e-04\n",
            "Epoch 183/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0887 - mae: 0.1949 - mse: 0.0887 - val_loss: 0.0771 - val_mae: 0.2079 - val_mse: 0.0771 - lr: 1.0000e-04\n",
            "Epoch 184/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0703 - mae: 0.1887 - mse: 0.0703 - val_loss: 0.0763 - val_mae: 0.2071 - val_mse: 0.0763 - lr: 1.0000e-04\n",
            "Epoch 185/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0967 - mae: 0.1920 - mse: 0.0967 - val_loss: 0.0753 - val_mae: 0.2053 - val_mse: 0.0753 - lr: 1.0000e-04\n",
            "Epoch 186/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.0971 - mae: 0.1944 - mse: 0.0971 - val_loss: 0.0750 - val_mae: 0.2044 - val_mse: 0.0750 - lr: 1.0000e-04\n",
            "Epoch 187/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0868 - mae: 0.1951 - mse: 0.0868 - val_loss: 0.0745 - val_mae: 0.2036 - val_mse: 0.0745 - lr: 1.0000e-04\n",
            "Epoch 188/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0798 - mae: 0.1922 - mse: 0.0798 - val_loss: 0.0741 - val_mae: 0.2047 - val_mse: 0.0741 - lr: 1.0000e-04\n",
            "Epoch 189/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0752 - mae: 0.1907 - mse: 0.0752 - val_loss: 0.0731 - val_mae: 0.2025 - val_mse: 0.0731 - lr: 1.0000e-04\n",
            "Epoch 190/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0708 - mae: 0.1840 - mse: 0.0708 - val_loss: 0.0731 - val_mae: 0.2013 - val_mse: 0.0731 - lr: 1.0000e-04\n",
            "Epoch 191/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1053 - mae: 0.1926 - mse: 0.1053 - val_loss: 0.0722 - val_mae: 0.2001 - val_mse: 0.0722 - lr: 1.0000e-04\n",
            "Epoch 192/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1095 - mae: 0.1902 - mse: 0.1095 - val_loss: 0.0716 - val_mae: 0.1997 - val_mse: 0.0716 - lr: 1.0000e-04\n",
            "Epoch 193/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0804 - mae: 0.1902 - mse: 0.0804 - val_loss: 0.0712 - val_mae: 0.1988 - val_mse: 0.0712 - lr: 1.0000e-04\n",
            "Epoch 194/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0787 - mae: 0.1866 - mse: 0.0787 - val_loss: 0.0712 - val_mae: 0.1988 - val_mse: 0.0712 - lr: 1.0000e-04\n",
            "Epoch 195/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0714 - mae: 0.1834 - mse: 0.0714 - val_loss: 0.0701 - val_mae: 0.1978 - val_mse: 0.0701 - lr: 1.0000e-04\n",
            "Epoch 196/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0707 - mae: 0.1828 - mse: 0.0707 - val_loss: 0.0697 - val_mae: 0.1969 - val_mse: 0.0697 - lr: 1.0000e-04\n",
            "Epoch 197/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0724 - mae: 0.1822 - mse: 0.0724 - val_loss: 0.0695 - val_mae: 0.1961 - val_mse: 0.0695 - lr: 1.0000e-04\n",
            "Epoch 198/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0841 - mae: 0.1927 - mse: 0.0841 - val_loss: 0.0689 - val_mae: 0.1958 - val_mse: 0.0689 - lr: 1.0000e-04\n",
            "Epoch 199/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0712 - mae: 0.1787 - mse: 0.0712 - val_loss: 0.0683 - val_mae: 0.1953 - val_mse: 0.0683 - lr: 1.0000e-04\n",
            "Epoch 200/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0754 - mae: 0.1882 - mse: 0.0754 - val_loss: 0.0677 - val_mae: 0.1942 - val_mse: 0.0677 - lr: 1.0000e-04\n",
            "Epoch 201/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0954 - mae: 0.1912 - mse: 0.0954 - val_loss: 0.0671 - val_mae: 0.1933 - val_mse: 0.0671 - lr: 1.0000e-04\n",
            "Epoch 202/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0790 - mae: 0.1820 - mse: 0.0790 - val_loss: 0.0667 - val_mae: 0.1922 - val_mse: 0.0667 - lr: 1.0000e-04\n",
            "Epoch 203/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0716 - mae: 0.1850 - mse: 0.0716 - val_loss: 0.0662 - val_mae: 0.1921 - val_mse: 0.0662 - lr: 1.0000e-04\n",
            "Epoch 204/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0568 - mae: 0.1747 - mse: 0.0568 - val_loss: 0.0658 - val_mae: 0.1916 - val_mse: 0.0658 - lr: 1.0000e-04\n",
            "Epoch 205/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.0795 - mae: 0.1760 - mse: 0.0795 - val_loss: 0.0652 - val_mae: 0.1897 - val_mse: 0.0652 - lr: 1.0000e-04\n",
            "Epoch 206/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0810 - mae: 0.1799 - mse: 0.0810 - val_loss: 0.0646 - val_mae: 0.1888 - val_mse: 0.0646 - lr: 1.0000e-04\n",
            "Epoch 207/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0815 - mae: 0.1871 - mse: 0.0815 - val_loss: 0.0641 - val_mae: 0.1886 - val_mse: 0.0641 - lr: 1.0000e-04\n",
            "Epoch 208/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0685 - mae: 0.1747 - mse: 0.0685 - val_loss: 0.0638 - val_mae: 0.1882 - val_mse: 0.0638 - lr: 1.0000e-04\n",
            "Epoch 209/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0978 - mae: 0.1804 - mse: 0.0978 - val_loss: 0.0633 - val_mae: 0.1867 - val_mse: 0.0633 - lr: 1.0000e-04\n",
            "Epoch 210/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0580 - mae: 0.1689 - mse: 0.0580 - val_loss: 0.0630 - val_mae: 0.1860 - val_mse: 0.0630 - lr: 1.0000e-04\n",
            "Epoch 211/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0746 - mae: 0.1780 - mse: 0.0746 - val_loss: 0.0627 - val_mae: 0.1857 - val_mse: 0.0627 - lr: 1.0000e-04\n",
            "Epoch 212/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0684 - mae: 0.1740 - mse: 0.0684 - val_loss: 0.0625 - val_mae: 0.1852 - val_mse: 0.0625 - lr: 1.0000e-04\n",
            "Epoch 213/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0784 - mae: 0.1800 - mse: 0.0784 - val_loss: 0.0619 - val_mae: 0.1848 - val_mse: 0.0619 - lr: 1.0000e-04\n",
            "Epoch 214/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0923 - mae: 0.1776 - mse: 0.0923 - val_loss: 0.0619 - val_mae: 0.1854 - val_mse: 0.0619 - lr: 1.0000e-04\n",
            "Epoch 215/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0620 - mae: 0.1659 - mse: 0.0620 - val_loss: 0.0612 - val_mae: 0.1839 - val_mse: 0.0612 - lr: 1.0000e-04\n",
            "Epoch 216/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0719 - mae: 0.1775 - mse: 0.0719 - val_loss: 0.0619 - val_mae: 0.1841 - val_mse: 0.0619 - lr: 1.0000e-04\n",
            "Epoch 217/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0699 - mae: 0.1655 - mse: 0.0699 - val_loss: 0.0609 - val_mae: 0.1829 - val_mse: 0.0609 - lr: 1.0000e-04\n",
            "Epoch 218/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0909 - mae: 0.1834 - mse: 0.0909 - val_loss: 0.0606 - val_mae: 0.1831 - val_mse: 0.0606 - lr: 1.0000e-04\n",
            "Epoch 219/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0704 - mae: 0.1759 - mse: 0.0704 - val_loss: 0.0599 - val_mae: 0.1816 - val_mse: 0.0599 - lr: 1.0000e-04\n",
            "Epoch 220/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0580 - mae: 0.1684 - mse: 0.0580 - val_loss: 0.0595 - val_mae: 0.1809 - val_mse: 0.0595 - lr: 1.0000e-04\n",
            "Epoch 221/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0600 - mae: 0.1623 - mse: 0.0600 - val_loss: 0.0590 - val_mae: 0.1806 - val_mse: 0.0590 - lr: 1.0000e-04\n",
            "Epoch 222/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0781 - mae: 0.1696 - mse: 0.0781 - val_loss: 0.0585 - val_mae: 0.1797 - val_mse: 0.0585 - lr: 1.0000e-04\n",
            "Epoch 223/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0747 - mae: 0.1705 - mse: 0.0747 - val_loss: 0.0582 - val_mae: 0.1789 - val_mse: 0.0582 - lr: 1.0000e-04\n",
            "Epoch 224/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0858 - mae: 0.1785 - mse: 0.0858 - val_loss: 0.0583 - val_mae: 0.1802 - val_mse: 0.0583 - lr: 1.0000e-04\n",
            "Epoch 225/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0960 - mae: 0.1768 - mse: 0.0960 - val_loss: 0.0575 - val_mae: 0.1782 - val_mse: 0.0575 - lr: 1.0000e-04\n",
            "Epoch 226/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0715 - mae: 0.1702 - mse: 0.0715 - val_loss: 0.0569 - val_mae: 0.1776 - val_mse: 0.0569 - lr: 1.0000e-04\n",
            "Epoch 227/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0794 - mae: 0.1760 - mse: 0.0794 - val_loss: 0.0580 - val_mae: 0.1802 - val_mse: 0.0580 - lr: 1.0000e-04\n",
            "Epoch 228/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0676 - mae: 0.1727 - mse: 0.0676 - val_loss: 0.0561 - val_mae: 0.1760 - val_mse: 0.0561 - lr: 1.0000e-04\n",
            "Epoch 229/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0932 - mae: 0.1726 - mse: 0.0932 - val_loss: 0.0565 - val_mae: 0.1767 - val_mse: 0.0565 - lr: 1.0000e-04\n",
            "Epoch 230/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0845 - mae: 0.1714 - mse: 0.0845 - val_loss: 0.0559 - val_mae: 0.1754 - val_mse: 0.0559 - lr: 1.0000e-04\n",
            "Epoch 231/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0690 - mae: 0.1708 - mse: 0.0690 - val_loss: 0.0561 - val_mae: 0.1761 - val_mse: 0.0561 - lr: 1.0000e-04\n",
            "Epoch 232/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0721 - mae: 0.1741 - mse: 0.0721 - val_loss: 0.0556 - val_mae: 0.1749 - val_mse: 0.0556 - lr: 1.0000e-04\n",
            "Epoch 233/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0640 - mae: 0.1660 - mse: 0.0640 - val_loss: 0.0550 - val_mae: 0.1739 - val_mse: 0.0550 - lr: 1.0000e-04\n",
            "Epoch 234/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0562 - mae: 0.1631 - mse: 0.0562 - val_loss: 0.0550 - val_mae: 0.1738 - val_mse: 0.0550 - lr: 1.0000e-04\n",
            "Epoch 235/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0787 - mae: 0.1715 - mse: 0.0787 - val_loss: 0.0547 - val_mae: 0.1733 - val_mse: 0.0547 - lr: 1.0000e-04\n",
            "Epoch 236/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0611 - mae: 0.1644 - mse: 0.0611 - val_loss: 0.0544 - val_mae: 0.1728 - val_mse: 0.0544 - lr: 1.0000e-04\n",
            "Epoch 237/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0758 - mae: 0.1694 - mse: 0.0758 - val_loss: 0.0540 - val_mae: 0.1723 - val_mse: 0.0540 - lr: 1.0000e-04\n",
            "Epoch 238/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0499 - mae: 0.1594 - mse: 0.0499 - val_loss: 0.0538 - val_mae: 0.1720 - val_mse: 0.0538 - lr: 1.0000e-04\n",
            "Epoch 239/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0711 - mae: 0.1665 - mse: 0.0711 - val_loss: 0.0533 - val_mae: 0.1710 - val_mse: 0.0533 - lr: 1.0000e-04\n",
            "Epoch 240/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0658 - mae: 0.1639 - mse: 0.0658 - val_loss: 0.0528 - val_mae: 0.1701 - val_mse: 0.0528 - lr: 1.0000e-04\n",
            "Epoch 241/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0543 - mae: 0.1577 - mse: 0.0543 - val_loss: 0.0523 - val_mae: 0.1691 - val_mse: 0.0523 - lr: 1.0000e-04\n",
            "Epoch 242/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0609 - mae: 0.1551 - mse: 0.0609 - val_loss: 0.0514 - val_mae: 0.1677 - val_mse: 0.0514 - lr: 1.0000e-04\n",
            "Epoch 243/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0609 - mae: 0.1600 - mse: 0.0609 - val_loss: 0.0512 - val_mae: 0.1677 - val_mse: 0.0512 - lr: 1.0000e-04\n",
            "Epoch 244/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0663 - mae: 0.1642 - mse: 0.0663 - val_loss: 0.0510 - val_mae: 0.1672 - val_mse: 0.0510 - lr: 1.0000e-04\n",
            "Epoch 245/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0679 - mae: 0.1599 - mse: 0.0679 - val_loss: 0.0509 - val_mae: 0.1673 - val_mse: 0.0509 - lr: 1.0000e-04\n",
            "Epoch 246/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0614 - mae: 0.1574 - mse: 0.0614 - val_loss: 0.0509 - val_mae: 0.1670 - val_mse: 0.0509 - lr: 1.0000e-04\n",
            "Epoch 247/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0649 - mae: 0.1601 - mse: 0.0649 - val_loss: 0.0508 - val_mae: 0.1665 - val_mse: 0.0508 - lr: 1.0000e-04\n",
            "Epoch 248/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0558 - mae: 0.1636 - mse: 0.0558 - val_loss: 0.0503 - val_mae: 0.1656 - val_mse: 0.0503 - lr: 1.0000e-04\n",
            "Epoch 249/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0787 - mae: 0.1613 - mse: 0.0787 - val_loss: 0.0505 - val_mae: 0.1656 - val_mse: 0.0505 - lr: 1.0000e-04\n",
            "Epoch 250/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0613 - mae: 0.1585 - mse: 0.0613 - val_loss: 0.0497 - val_mae: 0.1643 - val_mse: 0.0497 - lr: 1.0000e-04\n",
            "Epoch 251/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0561 - mae: 0.1577 - mse: 0.0561 - val_loss: 0.0494 - val_mae: 0.1635 - val_mse: 0.0494 - lr: 1.0000e-04\n",
            "Epoch 252/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0602 - mae: 0.1591 - mse: 0.0602 - val_loss: 0.0493 - val_mae: 0.1633 - val_mse: 0.0493 - lr: 1.0000e-04\n",
            "Epoch 253/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0673 - mae: 0.1585 - mse: 0.0673 - val_loss: 0.0484 - val_mae: 0.1622 - val_mse: 0.0484 - lr: 1.0000e-04\n",
            "Epoch 254/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0591 - mae: 0.1585 - mse: 0.0591 - val_loss: 0.0480 - val_mae: 0.1617 - val_mse: 0.0480 - lr: 1.0000e-04\n",
            "Epoch 255/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0710 - mae: 0.1586 - mse: 0.0710 - val_loss: 0.0479 - val_mae: 0.1613 - val_mse: 0.0479 - lr: 1.0000e-04\n",
            "Epoch 256/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0635 - mae: 0.1517 - mse: 0.0635 - val_loss: 0.0480 - val_mae: 0.1613 - val_mse: 0.0480 - lr: 1.0000e-04\n",
            "Epoch 257/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0778 - mae: 0.1836 - mse: 0.0778\n",
            "Epoch 257: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0685 - mae: 0.1616 - mse: 0.0685 - val_loss: 0.0486 - val_mae: 0.1620 - val_mse: 0.0486 - lr: 1.0000e-04\n",
            "Epoch 258/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0523 - mae: 0.1526 - mse: 0.0523 - val_loss: 0.0484 - val_mae: 0.1618 - val_mse: 0.0484 - lr: 3.0000e-05\n",
            "Epoch 259/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0518 - mae: 0.1491 - mse: 0.0518\n",
            "Epoch 259: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0511 - mae: 0.1480 - mse: 0.0511 - val_loss: 0.0484 - val_mae: 0.1618 - val_mse: 0.0484 - lr: 3.0000e-05\n",
            "Epoch 260/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0506 - mae: 0.1536 - mse: 0.0506 - val_loss: 0.0484 - val_mae: 0.1617 - val_mse: 0.0484 - lr: 9.0000e-06\n",
            "Epoch 260: early stopping\n",
            "Best epoch: 255\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1333 - mae: 0.1782 - mse: 0.1333\n",
            "[Test loss, Test ['mae', 'mse']]: [0.13328981399536133, 0.17821572721004486, 0.13328981399536133]\n",
            "Epoch 1/306\n",
            "35/35 [==============================] - 5s 6ms/step - loss: 0.8474 - mae: 0.5993 - mse: 0.8474\n",
            "Epoch 2/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.8304 - mae: 0.5899 - mse: 0.8304\n",
            "Epoch 3/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.8328 - mae: 0.5907 - mse: 0.8328\n",
            "Epoch 4/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8250 - mae: 0.5905 - mse: 0.8250\n",
            "Epoch 5/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8215 - mae: 0.5887 - mse: 0.8215\n",
            "Epoch 6/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8091 - mae: 0.5849 - mse: 0.8091\n",
            "Epoch 7/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8011 - mae: 0.5781 - mse: 0.8011\n",
            "Epoch 8/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7896 - mae: 0.5747 - mse: 0.7896\n",
            "Epoch 9/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7856 - mae: 0.5715 - mse: 0.7856\n",
            "Epoch 10/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7633 - mae: 0.5614 - mse: 0.7633\n",
            "Epoch 11/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7409 - mae: 0.5516 - mse: 0.7409\n",
            "Epoch 12/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7136 - mae: 0.5402 - mse: 0.7136\n",
            "Epoch 13/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6830 - mae: 0.5274 - mse: 0.6830\n",
            "Epoch 14/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6557 - mae: 0.5094 - mse: 0.6557\n",
            "Epoch 15/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6162 - mae: 0.4949 - mse: 0.6162\n",
            "Epoch 16/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5824 - mae: 0.4754 - mse: 0.5824\n",
            "Epoch 17/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5367 - mae: 0.4626 - mse: 0.5367\n",
            "Epoch 18/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4882 - mae: 0.4497 - mse: 0.4882\n",
            "Epoch 19/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4983 - mae: 0.4435 - mse: 0.4983\n",
            "Epoch 20/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4425 - mae: 0.4309 - mse: 0.4425\n",
            "Epoch 21/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4253 - mae: 0.4214 - mse: 0.4253\n",
            "Epoch 22/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4046 - mae: 0.4101 - mse: 0.4046\n",
            "Epoch 23/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3960 - mae: 0.4085 - mse: 0.3960\n",
            "Epoch 24/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4092 - mae: 0.4052 - mse: 0.4092\n",
            "Epoch 25/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3469 - mae: 0.3835 - mse: 0.3469\n",
            "Epoch 26/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3284 - mae: 0.3803 - mse: 0.3284\n",
            "Epoch 27/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3257 - mae: 0.3719 - mse: 0.3257\n",
            "Epoch 28/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3264 - mae: 0.3644 - mse: 0.3264\n",
            "Epoch 29/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2860 - mae: 0.3508 - mse: 0.2860\n",
            "Epoch 30/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2684 - mae: 0.3450 - mse: 0.2684\n",
            "Epoch 31/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2622 - mae: 0.3416 - mse: 0.2622\n",
            "Epoch 32/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2333 - mae: 0.3267 - mse: 0.2333\n",
            "Epoch 33/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2213 - mae: 0.3178 - mse: 0.2213\n",
            "Epoch 34/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2212 - mae: 0.3129 - mse: 0.2212\n",
            "Epoch 35/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2354 - mae: 0.3118 - mse: 0.2354\n",
            "Epoch 36/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2036 - mae: 0.3044 - mse: 0.2036\n",
            "Epoch 37/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1846 - mae: 0.2914 - mse: 0.1846\n",
            "Epoch 38/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1711 - mae: 0.2832 - mse: 0.1711\n",
            "Epoch 39/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1765 - mae: 0.2819 - mse: 0.1765\n",
            "Epoch 40/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1555 - mae: 0.2717 - mse: 0.1555\n",
            "Epoch 41/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2089 - mae: 0.2881 - mse: 0.2089\n",
            "Epoch 42/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1646 - mae: 0.2740 - mse: 0.1646\n",
            "Epoch 43/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1586 - mae: 0.2697 - mse: 0.1586\n",
            "Epoch 44/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1639 - mae: 0.2633 - mse: 0.1639\n",
            "Epoch 45/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1247 - mae: 0.2529 - mse: 0.1247\n",
            "Epoch 46/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1766 - mae: 0.2621 - mse: 0.1766\n",
            "Epoch 47/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1440 - mae: 0.2590 - mse: 0.1440\n",
            "Epoch 48/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1381 - mae: 0.2489 - mse: 0.1381\n",
            "Epoch 49/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1485 - mae: 0.2491 - mse: 0.1485\n",
            "Epoch 50/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1323 - mae: 0.2463 - mse: 0.1323\n",
            "Epoch 51/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1592 - mae: 0.2464 - mse: 0.1592\n",
            "Epoch 52/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1289 - mae: 0.2416 - mse: 0.1289\n",
            "Epoch 53/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1372 - mae: 0.2415 - mse: 0.1372\n",
            "Epoch 54/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1192 - mae: 0.2355 - mse: 0.1192\n",
            "Epoch 55/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1235 - mae: 0.2280 - mse: 0.1235\n",
            "Epoch 56/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1164 - mae: 0.2240 - mse: 0.1164\n",
            "Epoch 57/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1122 - mae: 0.2201 - mse: 0.1122\n",
            "Epoch 58/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1151 - mae: 0.2217 - mse: 0.1151\n",
            "Epoch 59/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1093 - mae: 0.2099 - mse: 0.1093\n",
            "Epoch 60/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1195 - mae: 0.2206 - mse: 0.1195\n",
            "Epoch 61/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0962 - mae: 0.2126 - mse: 0.0962\n",
            "Epoch 62/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0881 - mae: 0.2064 - mse: 0.0881\n",
            "Epoch 63/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0773 - mae: 0.1980 - mse: 0.0773\n",
            "Epoch 64/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0980 - mae: 0.2039 - mse: 0.0980\n",
            "Epoch 65/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0839 - mae: 0.2017 - mse: 0.0839\n",
            "Epoch 66/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0956 - mae: 0.2008 - mse: 0.0956\n",
            "Epoch 67/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0919 - mae: 0.1945 - mse: 0.0919\n",
            "Epoch 68/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0849 - mae: 0.1966 - mse: 0.0849\n",
            "Epoch 69/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1018 - mae: 0.2001 - mse: 0.1018\n",
            "Epoch 70/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0868 - mae: 0.1937 - mse: 0.0868\n",
            "Epoch 71/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0810 - mae: 0.1891 - mse: 0.0810\n",
            "Epoch 72/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.1939 - mse: 0.0926\n",
            "Epoch 73/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0792 - mae: 0.1874 - mse: 0.0792\n",
            "Epoch 74/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0705 - mae: 0.1794 - mse: 0.0705\n",
            "Epoch 75/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0840 - mae: 0.1882 - mse: 0.0840\n",
            "Epoch 76/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0795 - mae: 0.1857 - mse: 0.0795\n",
            "Epoch 77/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0865 - mae: 0.1818 - mse: 0.0865\n",
            "Epoch 78/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0717 - mae: 0.1762 - mse: 0.0717\n",
            "Epoch 79/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0757 - mae: 0.1786 - mse: 0.0757\n",
            "Epoch 80/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0750 - mae: 0.1793 - mse: 0.0750\n",
            "Epoch 81/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0570 - mae: 0.1673 - mse: 0.0570\n",
            "Epoch 82/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0776 - mae: 0.1773 - mse: 0.0776\n",
            "Epoch 83/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0660 - mae: 0.1757 - mse: 0.0660\n",
            "Epoch 84/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0742 - mae: 0.1701 - mse: 0.0742\n",
            "Epoch 85/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0665 - mae: 0.1695 - mse: 0.0665\n",
            "Epoch 86/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0606 - mae: 0.1643 - mse: 0.0606\n",
            "Epoch 87/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0559 - mae: 0.1676 - mse: 0.0559\n",
            "Epoch 88/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0664 - mae: 0.1680 - mse: 0.0664\n",
            "Epoch 89/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.1633 - mse: 0.0604\n",
            "Epoch 90/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0666 - mae: 0.1604 - mse: 0.0666\n",
            "Epoch 91/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0583 - mae: 0.1630 - mse: 0.0583\n",
            "Epoch 92/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0573 - mae: 0.1594 - mse: 0.0573\n",
            "Epoch 93/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0800 - mae: 0.1659 - mse: 0.0800\n",
            "Epoch 94/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0517 - mae: 0.1564 - mse: 0.0517\n",
            "Epoch 95/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0547 - mae: 0.1542 - mse: 0.0547\n",
            "Epoch 96/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0506 - mae: 0.1537 - mse: 0.0506\n",
            "Epoch 97/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0655 - mae: 0.1537 - mse: 0.0655\n",
            "Epoch 98/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0560 - mae: 0.1534 - mse: 0.0560\n",
            "Epoch 99/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0581 - mae: 0.1513 - mse: 0.0581\n",
            "Epoch 100/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0585 - mae: 0.1540 - mse: 0.0585\n",
            "Epoch 101/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0495 - mae: 0.1501 - mse: 0.0495\n",
            "Epoch 102/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0737 - mae: 0.1532 - mse: 0.0737\n",
            "Epoch 103/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0469 - mae: 0.1431 - mse: 0.0469\n",
            "Epoch 104/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0515 - mae: 0.1523 - mse: 0.0515\n",
            "Epoch 105/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0755 - mae: 0.1557 - mse: 0.0755\n",
            "Epoch 106/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0440 - mae: 0.1414 - mse: 0.0440\n",
            "Epoch 107/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0499 - mae: 0.1459 - mse: 0.0499\n",
            "Epoch 108/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0508 - mae: 0.1471 - mse: 0.0508\n",
            "Epoch 109/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0562 - mae: 0.1446 - mse: 0.0562\n",
            "Epoch 110/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0425 - mae: 0.1439 - mse: 0.0425\n",
            "Epoch 111/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0533 - mae: 0.1504 - mse: 0.0533\n",
            "Epoch 112/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0505 - mae: 0.1464 - mse: 0.0505\n",
            "Epoch 113/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0451 - mae: 0.1448 - mse: 0.0451\n",
            "Epoch 114/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0534 - mae: 0.1414 - mse: 0.0534\n",
            "Epoch 115/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0478 - mae: 0.1447 - mse: 0.0478\n",
            "Epoch 116/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0568 - mae: 0.1404 - mse: 0.0568\n",
            "Epoch 117/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0614 - mae: 0.1474 - mse: 0.0614\n",
            "Epoch 118/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0539 - mae: 0.1398 - mse: 0.0539\n",
            "Epoch 119/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0468 - mae: 0.1430 - mse: 0.0468\n",
            "Epoch 120/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0467 - mae: 0.1368 - mse: 0.0467\n",
            "Epoch 121/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0475 - mae: 0.1368 - mse: 0.0475\n",
            "Epoch 122/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0356 - mae: 0.1351 - mse: 0.0356\n",
            "Epoch 123/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0537 - mae: 0.1354 - mse: 0.0537\n",
            "Epoch 124/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0480 - mae: 0.1358 - mse: 0.0480\n",
            "Epoch 125/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0493 - mae: 0.1396 - mse: 0.0493\n",
            "Epoch 126/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0382 - mae: 0.1339 - mse: 0.0382\n",
            "Epoch 127/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0491 - mae: 0.1406 - mse: 0.0491\n",
            "Epoch 128/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0469 - mae: 0.1375 - mse: 0.0469\n",
            "Epoch 129/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0365 - mae: 0.1327 - mse: 0.0365\n",
            "Epoch 130/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0404 - mae: 0.1340 - mse: 0.0404\n",
            "Epoch 131/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0387 - mae: 0.1355 - mse: 0.0387\n",
            "Epoch 132/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0534 - mae: 0.1362 - mse: 0.0534\n",
            "Epoch 133/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0333 - mae: 0.1251 - mse: 0.0333\n",
            "Epoch 134/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0625 - mae: 0.1381 - mse: 0.0625\n",
            "Epoch 135/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0497 - mae: 0.1346 - mse: 0.0497\n",
            "Epoch 136/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0354 - mae: 0.1251 - mse: 0.0354\n",
            "Epoch 137/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0534 - mae: 0.1377 - mse: 0.0534\n",
            "Epoch 138/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0424 - mae: 0.1295 - mse: 0.0424\n",
            "Epoch 139/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0408 - mae: 0.1326 - mse: 0.0408\n",
            "Epoch 140/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0524 - mae: 0.1380 - mse: 0.0524\n",
            "Epoch 141/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0396 - mae: 0.1283 - mse: 0.0396\n",
            "Epoch 142/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0434 - mae: 0.1315 - mse: 0.0434\n",
            "Epoch 143/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0444 - mae: 0.1267 - mse: 0.0444\n",
            "Epoch 144/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0363 - mae: 0.1238 - mse: 0.0363\n",
            "Epoch 145/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0364 - mae: 0.1241 - mse: 0.0364\n",
            "Epoch 146/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0349 - mae: 0.1224 - mse: 0.0349\n",
            "Epoch 147/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.1278 - mse: 0.0368\n",
            "Epoch 148/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.1184 - mse: 0.0304\n",
            "Epoch 149/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0342 - mae: 0.1258 - mse: 0.0342\n",
            "Epoch 150/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0502 - mae: 0.1261 - mse: 0.0502\n",
            "Epoch 151/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0521 - mae: 0.1273 - mse: 0.0521\n",
            "Epoch 152/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0364 - mae: 0.1220 - mse: 0.0364\n",
            "Epoch 153/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0337 - mae: 0.1188 - mse: 0.0337\n",
            "Epoch 154/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0372 - mae: 0.1257 - mse: 0.0372\n",
            "Epoch 155/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0387 - mae: 0.1258 - mse: 0.0387\n",
            "Epoch 156/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0418 - mae: 0.1227 - mse: 0.0418\n",
            "Epoch 157/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.1242 - mse: 0.0419\n",
            "Epoch 158/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0466 - mae: 0.1269 - mse: 0.0466\n",
            "Epoch 159/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0425 - mae: 0.1262 - mse: 0.0425\n",
            "Epoch 160/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0382 - mae: 0.1218 - mse: 0.0382\n",
            "Epoch 161/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0473 - mae: 0.1259 - mse: 0.0473\n",
            "Epoch 162/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0400 - mae: 0.1252 - mse: 0.0400\n",
            "Epoch 163/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0312 - mae: 0.1218 - mse: 0.0312\n",
            "Epoch 164/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0428 - mae: 0.1226 - mse: 0.0428\n",
            "Epoch 165/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0401 - mae: 0.1225 - mse: 0.0401\n",
            "Epoch 166/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0345 - mae: 0.1188 - mse: 0.0345\n",
            "Epoch 167/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0394 - mae: 0.1269 - mse: 0.0394\n",
            "Epoch 168/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.1198 - mse: 0.0380\n",
            "Epoch 169/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0391 - mae: 0.1263 - mse: 0.0391\n",
            "Epoch 170/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0431 - mae: 0.1202 - mse: 0.0431\n",
            "Epoch 171/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0468 - mae: 0.1249 - mse: 0.0468\n",
            "Epoch 172/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0378 - mae: 0.1180 - mse: 0.0378\n",
            "Epoch 173/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.1184 - mse: 0.0324\n",
            "Epoch 174/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0323 - mae: 0.1179 - mse: 0.0323\n",
            "Epoch 175/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0320 - mae: 0.1178 - mse: 0.0320\n",
            "Epoch 176/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0336 - mae: 0.1185 - mse: 0.0336\n",
            "Epoch 177/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.1123 - mse: 0.0286\n",
            "Epoch 178/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0296 - mae: 0.1178 - mse: 0.0296\n",
            "Epoch 179/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0409 - mae: 0.1184 - mse: 0.0409\n",
            "Epoch 180/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0463 - mae: 0.1255 - mse: 0.0463\n",
            "Epoch 181/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0394 - mae: 0.1210 - mse: 0.0394\n",
            "Epoch 182/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0374 - mae: 0.1201 - mse: 0.0374\n",
            "Epoch 183/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0383 - mae: 0.1205 - mse: 0.0383\n",
            "Epoch 184/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0477 - mae: 0.1230 - mse: 0.0477\n",
            "Epoch 185/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.1216 - mse: 0.0368\n",
            "Epoch 186/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0354 - mae: 0.1207 - mse: 0.0354\n",
            "Epoch 187/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.1190 - mse: 0.0380\n",
            "Epoch 188/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0354 - mae: 0.1203 - mse: 0.0354\n",
            "Epoch 189/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0300 - mae: 0.1147 - mse: 0.0300\n",
            "Epoch 190/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0318 - mae: 0.1141 - mse: 0.0318\n",
            "Epoch 191/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0470 - mae: 0.1242 - mse: 0.0470\n",
            "Epoch 192/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.1159 - mse: 0.0297\n",
            "Epoch 193/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0291 - mae: 0.1109 - mse: 0.0291\n",
            "Epoch 194/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0449 - mae: 0.1225 - mse: 0.0449\n",
            "Epoch 195/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.1162 - mse: 0.0368\n",
            "Epoch 196/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0407 - mae: 0.1241 - mse: 0.0407\n",
            "Epoch 197/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1145 - mse: 0.0281\n",
            "Epoch 198/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0344 - mae: 0.1133 - mse: 0.0344\n",
            "Epoch 199/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0413 - mae: 0.1205 - mse: 0.0413\n",
            "Epoch 200/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.1098 - mse: 0.0306\n",
            "Epoch 201/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0440 - mae: 0.1164 - mse: 0.0440\n",
            "Epoch 202/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0310 - mae: 0.1145 - mse: 0.0310\n",
            "Epoch 203/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0328 - mae: 0.1138 - mse: 0.0328\n",
            "Epoch 204/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0313 - mae: 0.1163 - mse: 0.0313\n",
            "Epoch 205/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0406 - mae: 0.1204 - mse: 0.0406\n",
            "Epoch 206/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0364 - mae: 0.1177 - mse: 0.0364\n",
            "Epoch 207/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0304 - mae: 0.1144 - mse: 0.0304\n",
            "Epoch 208/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0415 - mae: 0.1148 - mse: 0.0415\n",
            "Epoch 209/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0388 - mae: 0.1201 - mse: 0.0388\n",
            "Epoch 210/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0428 - mae: 0.1204 - mse: 0.0428\n",
            "Epoch 211/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0373 - mae: 0.1166 - mse: 0.0373\n",
            "Epoch 212/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0357 - mae: 0.1133 - mse: 0.0357\n",
            "Epoch 213/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0393 - mae: 0.1177 - mse: 0.0393\n",
            "Epoch 214/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.1104 - mse: 0.0283\n",
            "Epoch 215/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0325 - mae: 0.1145 - mse: 0.0325\n",
            "Epoch 216/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.1133 - mse: 0.0361\n",
            "Epoch 217/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.1166 - mse: 0.0329\n",
            "Epoch 218/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0343 - mae: 0.1111 - mse: 0.0343\n",
            "Epoch 219/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0288 - mae: 0.1089 - mse: 0.0288\n",
            "Epoch 220/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0313 - mae: 0.1139 - mse: 0.0313\n",
            "Epoch 221/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0401 - mae: 0.1147 - mse: 0.0401\n",
            "Epoch 222/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0345 - mae: 0.1174 - mse: 0.0345\n",
            "Epoch 223/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0511 - mae: 0.1177 - mse: 0.0511\n",
            "Epoch 224/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.1128 - mse: 0.0306\n",
            "Epoch 225/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0355 - mae: 0.1124 - mse: 0.0355\n",
            "Epoch 226/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0351 - mae: 0.1151 - mse: 0.0351\n",
            "Epoch 227/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1070 - mse: 0.0274\n",
            "Epoch 228/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0266 - mae: 0.1109 - mse: 0.0266\n",
            "Epoch 229/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0339 - mae: 0.1134 - mse: 0.0339\n",
            "Epoch 230/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.1120 - mse: 0.0264\n",
            "Epoch 231/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0409 - mae: 0.1201 - mse: 0.0409\n",
            "Epoch 232/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0336 - mae: 0.1136 - mse: 0.0336\n",
            "Epoch 233/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0312 - mae: 0.1171 - mse: 0.0312\n",
            "Epoch 234/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0465 - mae: 0.1220 - mse: 0.0465\n",
            "Epoch 235/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0300 - mae: 0.1067 - mse: 0.0300\n",
            "Epoch 236/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0438 - mae: 0.1150 - mse: 0.0438\n",
            "Epoch 237/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0348 - mae: 0.1121 - mse: 0.0348\n",
            "Epoch 238/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.1097 - mse: 0.0294\n",
            "Epoch 239/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0457 - mae: 0.1130 - mse: 0.0457\n",
            "Epoch 240/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1064 - mse: 0.0274\n",
            "Epoch 241/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0378 - mae: 0.1150 - mse: 0.0378\n",
            "Epoch 242/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0256 - mae: 0.1062 - mse: 0.0256\n",
            "Epoch 243/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.1137 - mse: 0.0347\n",
            "Epoch 244/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0341 - mae: 0.1088 - mse: 0.0341\n",
            "Epoch 245/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0284 - mae: 0.1073 - mse: 0.0284\n",
            "Epoch 246/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0278 - mae: 0.1090 - mse: 0.0278\n",
            "Epoch 247/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1095 - mse: 0.0334\n",
            "Epoch 248/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0286 - mae: 0.1076 - mse: 0.0286\n",
            "Epoch 249/306\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0310 - mae: 0.1102 - mse: 0.0310\n",
            "Epoch 250/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0307 - mae: 0.1108 - mse: 0.0307\n",
            "Epoch 251/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0370 - mae: 0.1119 - mse: 0.0370\n",
            "Epoch 252/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.1087 - mse: 0.0324\n",
            "Epoch 253/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0319 - mae: 0.1121 - mse: 0.0319\n",
            "Epoch 254/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0479 - mae: 0.1152 - mse: 0.0479\n",
            "Epoch 255/306\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0324 - mae: 0.1074 - mse: 0.0324\n",
            "Epoch 256/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0282 - mae: 0.1072 - mse: 0.0282\n",
            "Epoch 257/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0348 - mae: 0.1093 - mse: 0.0348\n",
            "Epoch 258/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.1117 - mse: 0.0361\n",
            "Epoch 259/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0370 - mae: 0.1146 - mse: 0.0370\n",
            "Epoch 260/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1056 - mse: 0.0273\n",
            "Epoch 261/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0378 - mae: 0.1134 - mse: 0.0378\n",
            "Epoch 262/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0296 - mae: 0.1089 - mse: 0.0296\n",
            "Epoch 263/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0309 - mae: 0.1064 - mse: 0.0309\n",
            "Epoch 264/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1075 - mse: 0.0274\n",
            "Epoch 265/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0411 - mae: 0.1052 - mse: 0.0411\n",
            "Epoch 266/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0238 - mae: 0.1032 - mse: 0.0238\n",
            "Epoch 267/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1069 - mse: 0.0273\n",
            "Epoch 268/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0276 - mae: 0.1035 - mse: 0.0276\n",
            "Epoch 269/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0252 - mae: 0.1040 - mse: 0.0252\n",
            "Epoch 270/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0325 - mae: 0.1100 - mse: 0.0325\n",
            "Epoch 271/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0392 - mae: 0.1133 - mse: 0.0392\n",
            "Epoch 272/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.1113 - mse: 0.0303\n",
            "Epoch 273/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0267 - mae: 0.1035 - mse: 0.0267\n",
            "Epoch 274/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0259 - mae: 0.1061 - mse: 0.0259\n",
            "Epoch 275/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0422 - mae: 0.1129 - mse: 0.0422\n",
            "Epoch 276/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1050 - mse: 0.0281\n",
            "Epoch 277/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0338 - mae: 0.1086 - mse: 0.0338\n",
            "Epoch 278/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0286 - mae: 0.1058 - mse: 0.0286\n",
            "Epoch 279/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0322 - mae: 0.1081 - mse: 0.0322\n",
            "Epoch 280/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.1074 - mse: 0.0311\n",
            "Epoch 281/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0331 - mae: 0.1052 - mse: 0.0331\n",
            "Epoch 282/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0283 - mae: 0.1075 - mse: 0.0283\n",
            "Epoch 283/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0250 - mae: 0.1058 - mse: 0.0250\n",
            "Epoch 284/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.1110 - mse: 0.0330\n",
            "Epoch 285/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0343 - mae: 0.1115 - mse: 0.0343\n",
            "Epoch 286/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0405 - mae: 0.1089 - mse: 0.0405\n",
            "Epoch 287/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0255 - mae: 0.1033 - mse: 0.0255\n",
            "Epoch 288/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1060 - mse: 0.0274\n",
            "Epoch 289/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0235 - mae: 0.1032 - mse: 0.0235\n",
            "Epoch 290/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0324 - mae: 0.1053 - mse: 0.0324\n",
            "Epoch 291/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.1064 - mse: 0.0297\n",
            "Epoch 292/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0464 - mae: 0.1135 - mse: 0.0464\n",
            "Epoch 293/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.1060 - mse: 0.0274\n",
            "Epoch 294/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0275 - mae: 0.1004 - mse: 0.0275\n",
            "Epoch 295/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0322 - mae: 0.1071 - mse: 0.0322\n",
            "Epoch 296/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0254 - mae: 0.1011 - mse: 0.0254\n",
            "Epoch 297/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0459 - mae: 0.1156 - mse: 0.0459\n",
            "Epoch 298/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0292 - mae: 0.1066 - mse: 0.0292\n",
            "Epoch 299/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0301 - mae: 0.1098 - mse: 0.0301\n",
            "Epoch 300/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.1075 - mse: 0.0294\n",
            "Epoch 301/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0260 - mae: 0.1023 - mse: 0.0260\n",
            "Epoch 302/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0403 - mae: 0.1081 - mse: 0.0403\n",
            "Epoch 303/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0295 - mae: 0.1043 - mse: 0.0295\n",
            "Epoch 304/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.1067 - mse: 0.0244\n",
            "Epoch 305/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0343 - mae: 0.1125 - mse: 0.0343\n",
            "Epoch 306/306\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.1093 - mse: 0.0306\n",
            "9/9 [==============================] - 1s 3ms/step\n",
            "Epoch 1/2000\n",
            "7/7 [==============================] - 8s 202ms/step - loss: 0.9661 - mae: 0.7728 - mse: 0.9661 - val_loss: 1.1119 - val_mae: 0.7759 - val_mse: 1.1119 - lr: 1.0000e-04\n",
            "Epoch 2/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9699 - mae: 0.7742 - mse: 0.9699 - val_loss: 1.1120 - val_mae: 0.7760 - val_mse: 1.1120 - lr: 1.0000e-04\n",
            "Epoch 3/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9645 - mae: 0.7741 - mse: 0.9645 - val_loss: 1.1117 - val_mae: 0.7760 - val_mse: 1.1117 - lr: 1.0000e-04\n",
            "Epoch 4/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9655 - mae: 0.7731 - mse: 0.9655 - val_loss: 1.1117 - val_mae: 0.7760 - val_mse: 1.1117 - lr: 1.0000e-04\n",
            "Epoch 5/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9727 - mae: 0.7767 - mse: 0.9727 - val_loss: 1.1113 - val_mae: 0.7759 - val_mse: 1.1113 - lr: 1.0000e-04\n",
            "Epoch 6/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9644 - mae: 0.7721 - mse: 0.9644 - val_loss: 1.1109 - val_mae: 0.7757 - val_mse: 1.1109 - lr: 1.0000e-04\n",
            "Epoch 7/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9631 - mae: 0.7720 - mse: 0.9631 - val_loss: 1.1104 - val_mae: 0.7755 - val_mse: 1.1104 - lr: 1.0000e-04\n",
            "Epoch 8/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9635 - mae: 0.7729 - mse: 0.9635 - val_loss: 1.1097 - val_mae: 0.7753 - val_mse: 1.1097 - lr: 1.0000e-04\n",
            "Epoch 9/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9622 - mae: 0.7716 - mse: 0.9622 - val_loss: 1.1094 - val_mae: 0.7752 - val_mse: 1.1094 - lr: 1.0000e-04\n",
            "Epoch 10/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9667 - mae: 0.7744 - mse: 0.9667 - val_loss: 1.1088 - val_mae: 0.7750 - val_mse: 1.1088 - lr: 1.0000e-04\n",
            "Epoch 11/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9643 - mae: 0.7735 - mse: 0.9643 - val_loss: 1.1080 - val_mae: 0.7746 - val_mse: 1.1080 - lr: 1.0000e-04\n",
            "Epoch 12/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9631 - mae: 0.7718 - mse: 0.9631 - val_loss: 1.1069 - val_mae: 0.7742 - val_mse: 1.1069 - lr: 1.0000e-04\n",
            "Epoch 13/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9608 - mae: 0.7710 - mse: 0.9608 - val_loss: 1.1059 - val_mae: 0.7738 - val_mse: 1.1059 - lr: 1.0000e-04\n",
            "Epoch 14/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9536 - mae: 0.7682 - mse: 0.9536 - val_loss: 1.1047 - val_mae: 0.7733 - val_mse: 1.1047 - lr: 1.0000e-04\n",
            "Epoch 15/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9593 - mae: 0.7696 - mse: 0.9593 - val_loss: 1.1035 - val_mae: 0.7728 - val_mse: 1.1035 - lr: 1.0000e-04\n",
            "Epoch 16/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.9552 - mae: 0.7685 - mse: 0.9552 - val_loss: 1.1023 - val_mae: 0.7723 - val_mse: 1.1023 - lr: 1.0000e-04\n",
            "Epoch 17/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9547 - mae: 0.7696 - mse: 0.9547 - val_loss: 1.1011 - val_mae: 0.7718 - val_mse: 1.1011 - lr: 1.0000e-04\n",
            "Epoch 18/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9560 - mae: 0.7709 - mse: 0.9560 - val_loss: 1.0994 - val_mae: 0.7711 - val_mse: 1.0994 - lr: 1.0000e-04\n",
            "Epoch 19/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9541 - mae: 0.7685 - mse: 0.9541 - val_loss: 1.0983 - val_mae: 0.7706 - val_mse: 1.0983 - lr: 1.0000e-04\n",
            "Epoch 20/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.9522 - mae: 0.7687 - mse: 0.9522 - val_loss: 1.0966 - val_mae: 0.7699 - val_mse: 1.0966 - lr: 1.0000e-04\n",
            "Epoch 21/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9533 - mae: 0.7680 - mse: 0.9533 - val_loss: 1.0946 - val_mae: 0.7691 - val_mse: 1.0946 - lr: 1.0000e-04\n",
            "Epoch 22/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9496 - mae: 0.7686 - mse: 0.9496 - val_loss: 1.0923 - val_mae: 0.7681 - val_mse: 1.0923 - lr: 1.0000e-04\n",
            "Epoch 23/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.9428 - mae: 0.7659 - mse: 0.9428 - val_loss: 1.0902 - val_mae: 0.7672 - val_mse: 1.0902 - lr: 1.0000e-04\n",
            "Epoch 24/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9389 - mae: 0.7631 - mse: 0.9389 - val_loss: 1.0877 - val_mae: 0.7661 - val_mse: 1.0877 - lr: 1.0000e-04\n",
            "Epoch 25/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9457 - mae: 0.7656 - mse: 0.9457 - val_loss: 1.0848 - val_mae: 0.7648 - val_mse: 1.0848 - lr: 1.0000e-04\n",
            "Epoch 26/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9393 - mae: 0.7633 - mse: 0.9393 - val_loss: 1.0818 - val_mae: 0.7636 - val_mse: 1.0818 - lr: 1.0000e-04\n",
            "Epoch 27/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9388 - mae: 0.7631 - mse: 0.9388 - val_loss: 1.0789 - val_mae: 0.7623 - val_mse: 1.0789 - lr: 1.0000e-04\n",
            "Epoch 28/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9333 - mae: 0.7611 - mse: 0.9333 - val_loss: 1.0754 - val_mae: 0.7608 - val_mse: 1.0754 - lr: 1.0000e-04\n",
            "Epoch 29/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9306 - mae: 0.7596 - mse: 0.9306 - val_loss: 1.0719 - val_mae: 0.7593 - val_mse: 1.0719 - lr: 1.0000e-04\n",
            "Epoch 30/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9297 - mae: 0.7599 - mse: 0.9297 - val_loss: 1.0680 - val_mae: 0.7576 - val_mse: 1.0680 - lr: 1.0000e-04\n",
            "Epoch 31/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9232 - mae: 0.7575 - mse: 0.9232 - val_loss: 1.0634 - val_mae: 0.7557 - val_mse: 1.0634 - lr: 1.0000e-04\n",
            "Epoch 32/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9228 - mae: 0.7554 - mse: 0.9228 - val_loss: 1.0590 - val_mae: 0.7538 - val_mse: 1.0590 - lr: 1.0000e-04\n",
            "Epoch 33/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9172 - mae: 0.7538 - mse: 0.9172 - val_loss: 1.0542 - val_mae: 0.7518 - val_mse: 1.0542 - lr: 1.0000e-04\n",
            "Epoch 34/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9132 - mae: 0.7529 - mse: 0.9132 - val_loss: 1.0493 - val_mae: 0.7496 - val_mse: 1.0493 - lr: 1.0000e-04\n",
            "Epoch 35/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9132 - mae: 0.7516 - mse: 0.9132 - val_loss: 1.0444 - val_mae: 0.7476 - val_mse: 1.0444 - lr: 1.0000e-04\n",
            "Epoch 36/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9077 - mae: 0.7485 - mse: 0.9077 - val_loss: 1.0386 - val_mae: 0.7451 - val_mse: 1.0386 - lr: 1.0000e-04\n",
            "Epoch 37/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8981 - mae: 0.7460 - mse: 0.8981 - val_loss: 1.0326 - val_mae: 0.7424 - val_mse: 1.0326 - lr: 1.0000e-04\n",
            "Epoch 38/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8952 - mae: 0.7440 - mse: 0.8952 - val_loss: 1.0268 - val_mae: 0.7400 - val_mse: 1.0268 - lr: 1.0000e-04\n",
            "Epoch 39/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8849 - mae: 0.7405 - mse: 0.8849 - val_loss: 1.0199 - val_mae: 0.7371 - val_mse: 1.0199 - lr: 1.0000e-04\n",
            "Epoch 40/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8815 - mae: 0.7398 - mse: 0.8815 - val_loss: 1.0125 - val_mae: 0.7339 - val_mse: 1.0125 - lr: 1.0000e-04\n",
            "Epoch 41/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8715 - mae: 0.7349 - mse: 0.8715 - val_loss: 1.0042 - val_mae: 0.7302 - val_mse: 1.0042 - lr: 1.0000e-04\n",
            "Epoch 42/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8602 - mae: 0.7315 - mse: 0.8602 - val_loss: 0.9952 - val_mae: 0.7262 - val_mse: 0.9952 - lr: 1.0000e-04\n",
            "Epoch 43/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8537 - mae: 0.7283 - mse: 0.8537 - val_loss: 0.9857 - val_mae: 0.7219 - val_mse: 0.9857 - lr: 1.0000e-04\n",
            "Epoch 44/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8469 - mae: 0.7256 - mse: 0.8469 - val_loss: 0.9758 - val_mae: 0.7174 - val_mse: 0.9758 - lr: 1.0000e-04\n",
            "Epoch 45/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8435 - mae: 0.7206 - mse: 0.8435 - val_loss: 0.9654 - val_mae: 0.7128 - val_mse: 0.9654 - lr: 1.0000e-04\n",
            "Epoch 46/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.8355 - mae: 0.7169 - mse: 0.8355 - val_loss: 0.9540 - val_mae: 0.7075 - val_mse: 0.9540 - lr: 1.0000e-04\n",
            "Epoch 47/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8105 - mae: 0.7069 - mse: 0.8105 - val_loss: 0.9422 - val_mae: 0.7022 - val_mse: 0.9422 - lr: 1.0000e-04\n",
            "Epoch 48/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8077 - mae: 0.7066 - mse: 0.8077 - val_loss: 0.9301 - val_mae: 0.6969 - val_mse: 0.9301 - lr: 1.0000e-04\n",
            "Epoch 49/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7903 - mae: 0.6987 - mse: 0.7903 - val_loss: 0.9164 - val_mae: 0.6908 - val_mse: 0.9164 - lr: 1.0000e-04\n",
            "Epoch 50/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7773 - mae: 0.6915 - mse: 0.7773 - val_loss: 0.9029 - val_mae: 0.6851 - val_mse: 0.9029 - lr: 1.0000e-04\n",
            "Epoch 51/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7691 - mae: 0.6894 - mse: 0.7691 - val_loss: 0.8874 - val_mae: 0.6778 - val_mse: 0.8874 - lr: 1.0000e-04\n",
            "Epoch 52/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7495 - mae: 0.6800 - mse: 0.7495 - val_loss: 0.8724 - val_mae: 0.6713 - val_mse: 0.8724 - lr: 1.0000e-04\n",
            "Epoch 53/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7370 - mae: 0.6723 - mse: 0.7370 - val_loss: 0.8573 - val_mae: 0.6650 - val_mse: 0.8573 - lr: 1.0000e-04\n",
            "Epoch 54/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7152 - mae: 0.6643 - mse: 0.7152 - val_loss: 0.8391 - val_mae: 0.6564 - val_mse: 0.8391 - lr: 1.0000e-04\n",
            "Epoch 55/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7048 - mae: 0.6562 - mse: 0.7048 - val_loss: 0.8202 - val_mae: 0.6472 - val_mse: 0.8202 - lr: 1.0000e-04\n",
            "Epoch 56/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6918 - mae: 0.6487 - mse: 0.6918 - val_loss: 0.8042 - val_mae: 0.6405 - val_mse: 0.8042 - lr: 1.0000e-04\n",
            "Epoch 57/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6649 - mae: 0.6374 - mse: 0.6649 - val_loss: 0.7839 - val_mae: 0.6302 - val_mse: 0.7839 - lr: 1.0000e-04\n",
            "Epoch 58/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6458 - mae: 0.6278 - mse: 0.6458 - val_loss: 0.7646 - val_mae: 0.6209 - val_mse: 0.7646 - lr: 1.0000e-04\n",
            "Epoch 59/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6402 - mae: 0.6208 - mse: 0.6402 - val_loss: 0.7453 - val_mae: 0.6123 - val_mse: 0.7453 - lr: 1.0000e-04\n",
            "Epoch 60/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6076 - mae: 0.6144 - mse: 0.6076 - val_loss: 0.7247 - val_mae: 0.6025 - val_mse: 0.7247 - lr: 1.0000e-04\n",
            "Epoch 61/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5998 - mae: 0.6004 - mse: 0.5998 - val_loss: 0.7051 - val_mae: 0.5933 - val_mse: 0.7051 - lr: 1.0000e-04\n",
            "Epoch 62/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5739 - mae: 0.5913 - mse: 0.5739 - val_loss: 0.6854 - val_mae: 0.5840 - val_mse: 0.6854 - lr: 1.0000e-04\n",
            "Epoch 63/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5631 - mae: 0.5841 - mse: 0.5631 - val_loss: 0.6658 - val_mae: 0.5741 - val_mse: 0.6658 - lr: 1.0000e-04\n",
            "Epoch 64/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5364 - mae: 0.5686 - mse: 0.5364 - val_loss: 0.6481 - val_mae: 0.5657 - val_mse: 0.6481 - lr: 1.0000e-04\n",
            "Epoch 65/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5091 - mae: 0.5589 - mse: 0.5091 - val_loss: 0.6298 - val_mae: 0.5567 - val_mse: 0.6298 - lr: 1.0000e-04\n",
            "Epoch 66/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5048 - mae: 0.5508 - mse: 0.5048 - val_loss: 0.6113 - val_mae: 0.5471 - val_mse: 0.6113 - lr: 1.0000e-04\n",
            "Epoch 67/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4809 - mae: 0.5399 - mse: 0.4809 - val_loss: 0.5922 - val_mae: 0.5367 - val_mse: 0.5922 - lr: 1.0000e-04\n",
            "Epoch 68/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4805 - mae: 0.5336 - mse: 0.4805 - val_loss: 0.5746 - val_mae: 0.5272 - val_mse: 0.5746 - lr: 1.0000e-04\n",
            "Epoch 69/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.4446 - mae: 0.5198 - mse: 0.4446 - val_loss: 0.5572 - val_mae: 0.5175 - val_mse: 0.5572 - lr: 1.0000e-04\n",
            "Epoch 70/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4529 - mae: 0.5261 - mse: 0.4529 - val_loss: 0.5398 - val_mae: 0.5077 - val_mse: 0.5398 - lr: 1.0000e-04\n",
            "Epoch 71/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4302 - mae: 0.5101 - mse: 0.4302 - val_loss: 0.5236 - val_mae: 0.4985 - val_mse: 0.5236 - lr: 1.0000e-04\n",
            "Epoch 72/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4079 - mae: 0.4967 - mse: 0.4079 - val_loss: 0.5104 - val_mae: 0.4920 - val_mse: 0.5104 - lr: 1.0000e-04\n",
            "Epoch 73/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.4127 - mae: 0.4921 - mse: 0.4127 - val_loss: 0.4974 - val_mae: 0.4852 - val_mse: 0.4974 - lr: 1.0000e-04\n",
            "Epoch 74/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.3878 - mae: 0.4872 - mse: 0.3878 - val_loss: 0.4805 - val_mae: 0.4736 - val_mse: 0.4805 - lr: 1.0000e-04\n",
            "Epoch 75/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3751 - mae: 0.4737 - mse: 0.3751 - val_loss: 0.4670 - val_mae: 0.4658 - val_mse: 0.4670 - lr: 1.0000e-04\n",
            "Epoch 76/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3677 - mae: 0.4732 - mse: 0.3677 - val_loss: 0.4581 - val_mae: 0.4625 - val_mse: 0.4581 - lr: 1.0000e-04\n",
            "Epoch 77/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3567 - mae: 0.4573 - mse: 0.3567 - val_loss: 0.4486 - val_mae: 0.4581 - val_mse: 0.4486 - lr: 1.0000e-04\n",
            "Epoch 78/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3763 - mae: 0.4516 - mse: 0.3763 - val_loss: 0.4324 - val_mae: 0.4470 - val_mse: 0.4324 - lr: 1.0000e-04\n",
            "Epoch 79/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.3257 - mae: 0.4358 - mse: 0.3257 - val_loss: 0.4175 - val_mae: 0.4360 - val_mse: 0.4175 - lr: 1.0000e-04\n",
            "Epoch 80/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3283 - mae: 0.4413 - mse: 0.3283 - val_loss: 0.4082 - val_mae: 0.4309 - val_mse: 0.4082 - lr: 1.0000e-04\n",
            "Epoch 81/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3147 - mae: 0.4245 - mse: 0.3147 - val_loss: 0.4041 - val_mae: 0.4311 - val_mse: 0.4041 - lr: 1.0000e-04\n",
            "Epoch 82/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3200 - mae: 0.4259 - mse: 0.3200 - val_loss: 0.3941 - val_mae: 0.4242 - val_mse: 0.3941 - lr: 1.0000e-04\n",
            "Epoch 83/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3026 - mae: 0.4221 - mse: 0.3026 - val_loss: 0.3821 - val_mae: 0.4149 - val_mse: 0.3821 - lr: 1.0000e-04\n",
            "Epoch 84/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2882 - mae: 0.4041 - mse: 0.2882 - val_loss: 0.3706 - val_mae: 0.4062 - val_mse: 0.3706 - lr: 1.0000e-04\n",
            "Epoch 85/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2863 - mae: 0.4056 - mse: 0.2863 - val_loss: 0.3603 - val_mae: 0.3990 - val_mse: 0.3603 - lr: 1.0000e-04\n",
            "Epoch 86/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2832 - mae: 0.3915 - mse: 0.2832 - val_loss: 0.3529 - val_mae: 0.3951 - val_mse: 0.3529 - lr: 1.0000e-04\n",
            "Epoch 87/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.2647 - mae: 0.3912 - mse: 0.2647 - val_loss: 0.3504 - val_mae: 0.3961 - val_mse: 0.3504 - lr: 1.0000e-04\n",
            "Epoch 88/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2596 - mae: 0.3852 - mse: 0.2596 - val_loss: 0.3412 - val_mae: 0.3901 - val_mse: 0.3412 - lr: 1.0000e-04\n",
            "Epoch 89/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2372 - mae: 0.3714 - mse: 0.2372 - val_loss: 0.3313 - val_mae: 0.3833 - val_mse: 0.3313 - lr: 1.0000e-04\n",
            "Epoch 90/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2320 - mae: 0.3648 - mse: 0.2320 - val_loss: 0.3199 - val_mae: 0.3747 - val_mse: 0.3199 - lr: 1.0000e-04\n",
            "Epoch 91/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2428 - mae: 0.3634 - mse: 0.2428 - val_loss: 0.3113 - val_mae: 0.3684 - val_mse: 0.3113 - lr: 1.0000e-04\n",
            "Epoch 92/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2385 - mae: 0.3672 - mse: 0.2385 - val_loss: 0.3111 - val_mae: 0.3709 - val_mse: 0.3111 - lr: 1.0000e-04\n",
            "Epoch 93/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2494 - mae: 0.3531 - mse: 0.2494 - val_loss: 0.3014 - val_mae: 0.3629 - val_mse: 0.3014 - lr: 1.0000e-04\n",
            "Epoch 94/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2320 - mae: 0.3524 - mse: 0.2320 - val_loss: 0.2923 - val_mae: 0.3546 - val_mse: 0.2923 - lr: 1.0000e-04\n",
            "Epoch 95/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2128 - mae: 0.3366 - mse: 0.2128 - val_loss: 0.2878 - val_mae: 0.3523 - val_mse: 0.2878 - lr: 1.0000e-04\n",
            "Epoch 96/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2337 - mae: 0.3529 - mse: 0.2337 - val_loss: 0.2832 - val_mae: 0.3497 - val_mse: 0.2832 - lr: 1.0000e-04\n",
            "Epoch 97/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1981 - mae: 0.3348 - mse: 0.1981 - val_loss: 0.2742 - val_mae: 0.3416 - val_mse: 0.2742 - lr: 1.0000e-04\n",
            "Epoch 98/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2016 - mae: 0.3341 - mse: 0.2016 - val_loss: 0.2714 - val_mae: 0.3407 - val_mse: 0.2714 - lr: 1.0000e-04\n",
            "Epoch 99/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1974 - mae: 0.3320 - mse: 0.1974 - val_loss: 0.2646 - val_mae: 0.3351 - val_mse: 0.2646 - lr: 1.0000e-04\n",
            "Epoch 100/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1934 - mae: 0.3208 - mse: 0.1934 - val_loss: 0.2576 - val_mae: 0.3282 - val_mse: 0.2576 - lr: 1.0000e-04\n",
            "Epoch 101/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.2050 - mae: 0.3231 - mse: 0.2050 - val_loss: 0.2546 - val_mae: 0.3268 - val_mse: 0.2546 - lr: 1.0000e-04\n",
            "Epoch 102/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1805 - mae: 0.3190 - mse: 0.1805 - val_loss: 0.2531 - val_mae: 0.3265 - val_mse: 0.2531 - lr: 1.0000e-04\n",
            "Epoch 103/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1658 - mae: 0.3065 - mse: 0.1658 - val_loss: 0.2477 - val_mae: 0.3214 - val_mse: 0.2477 - lr: 1.0000e-04\n",
            "Epoch 104/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1815 - mae: 0.3133 - mse: 0.1815 - val_loss: 0.2431 - val_mae: 0.3171 - val_mse: 0.2431 - lr: 1.0000e-04\n",
            "Epoch 105/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1731 - mae: 0.3096 - mse: 0.1731 - val_loss: 0.2392 - val_mae: 0.3144 - val_mse: 0.2392 - lr: 1.0000e-04\n",
            "Epoch 106/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1787 - mae: 0.3054 - mse: 0.1787 - val_loss: 0.2369 - val_mae: 0.3132 - val_mse: 0.2369 - lr: 1.0000e-04\n",
            "Epoch 107/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1608 - mae: 0.3012 - mse: 0.1608 - val_loss: 0.2310 - val_mae: 0.3072 - val_mse: 0.2310 - lr: 1.0000e-04\n",
            "Epoch 108/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1721 - mae: 0.3024 - mse: 0.1721 - val_loss: 0.2276 - val_mae: 0.3040 - val_mse: 0.2276 - lr: 1.0000e-04\n",
            "Epoch 109/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.1534 - mae: 0.2985 - mse: 0.1534 - val_loss: 0.2241 - val_mae: 0.3010 - val_mse: 0.2241 - lr: 1.0000e-04\n",
            "Epoch 110/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1570 - mae: 0.2923 - mse: 0.1570 - val_loss: 0.2210 - val_mae: 0.2980 - val_mse: 0.2210 - lr: 1.0000e-04\n",
            "Epoch 111/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1601 - mae: 0.2866 - mse: 0.1601 - val_loss: 0.2183 - val_mae: 0.2954 - val_mse: 0.2183 - lr: 1.0000e-04\n",
            "Epoch 112/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1435 - mae: 0.2860 - mse: 0.1435 - val_loss: 0.2162 - val_mae: 0.2934 - val_mse: 0.2162 - lr: 1.0000e-04\n",
            "Epoch 113/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1455 - mae: 0.2858 - mse: 0.1455 - val_loss: 0.2137 - val_mae: 0.2910 - val_mse: 0.2137 - lr: 1.0000e-04\n",
            "Epoch 114/2000\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.1415 - mae: 0.2806 - mse: 0.1415 - val_loss: 0.2106 - val_mae: 0.2880 - val_mse: 0.2106 - lr: 1.0000e-04\n",
            "Epoch 115/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1448 - mae: 0.2836 - mse: 0.1448 - val_loss: 0.2078 - val_mae: 0.2851 - val_mse: 0.2078 - lr: 1.0000e-04\n",
            "Epoch 116/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1571 - mae: 0.2885 - mse: 0.1571 - val_loss: 0.2055 - val_mae: 0.2823 - val_mse: 0.2055 - lr: 1.0000e-04\n",
            "Epoch 117/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1455 - mae: 0.2846 - mse: 0.1455 - val_loss: 0.2034 - val_mae: 0.2804 - val_mse: 0.2034 - lr: 1.0000e-04\n",
            "Epoch 118/2000\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.1539 - mae: 0.2762 - mse: 0.1539 - val_loss: 0.2019 - val_mae: 0.2789 - val_mse: 0.2019 - lr: 1.0000e-04\n",
            "Epoch 119/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1439 - mae: 0.2686 - mse: 0.1439 - val_loss: 0.1997 - val_mae: 0.2767 - val_mse: 0.1997 - lr: 1.0000e-04\n",
            "Epoch 120/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1583 - mae: 0.2746 - mse: 0.1583 - val_loss: 0.1976 - val_mae: 0.2747 - val_mse: 0.1976 - lr: 1.0000e-04\n",
            "Epoch 121/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1424 - mae: 0.2740 - mse: 0.1424 - val_loss: 0.1960 - val_mae: 0.2727 - val_mse: 0.1960 - lr: 1.0000e-04\n",
            "Epoch 122/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1386 - mae: 0.2697 - mse: 0.1386 - val_loss: 0.1948 - val_mae: 0.2714 - val_mse: 0.1948 - lr: 1.0000e-04\n",
            "Epoch 123/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1270 - mae: 0.2582 - mse: 0.1270 - val_loss: 0.1940 - val_mae: 0.2702 - val_mse: 0.1940 - lr: 1.0000e-04\n",
            "Epoch 124/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1407 - mae: 0.2606 - mse: 0.1407 - val_loss: 0.1923 - val_mae: 0.2688 - val_mse: 0.1923 - lr: 1.0000e-04\n",
            "Epoch 125/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1432 - mae: 0.2679 - mse: 0.1432 - val_loss: 0.1909 - val_mae: 0.2675 - val_mse: 0.1909 - lr: 1.0000e-04\n",
            "Epoch 126/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1313 - mae: 0.2579 - mse: 0.1313 - val_loss: 0.1889 - val_mae: 0.2654 - val_mse: 0.1889 - lr: 1.0000e-04\n",
            "Epoch 127/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1373 - mae: 0.2591 - mse: 0.1373 - val_loss: 0.1887 - val_mae: 0.2644 - val_mse: 0.1887 - lr: 1.0000e-04\n",
            "Epoch 128/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1459 - mae: 0.2582 - mse: 0.1459 - val_loss: 0.1860 - val_mae: 0.2617 - val_mse: 0.1860 - lr: 1.0000e-04\n",
            "Epoch 129/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1382 - mae: 0.2559 - mse: 0.1382 - val_loss: 0.1827 - val_mae: 0.2589 - val_mse: 0.1827 - lr: 1.0000e-04\n",
            "Epoch 130/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1286 - mae: 0.2585 - mse: 0.1286 - val_loss: 0.1805 - val_mae: 0.2570 - val_mse: 0.1805 - lr: 1.0000e-04\n",
            "Epoch 131/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1184 - mae: 0.2511 - mse: 0.1184 - val_loss: 0.1830 - val_mae: 0.2584 - val_mse: 0.1830 - lr: 1.0000e-04\n",
            "Epoch 132/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1060 - mae: 0.2470 - mse: 0.1060\n",
            "Epoch 132: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1261 - mae: 0.2524 - mse: 0.1261 - val_loss: 0.1824 - val_mae: 0.2575 - val_mse: 0.1824 - lr: 1.0000e-04\n",
            "Epoch 133/2000\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.1261 - mae: 0.2442 - mse: 0.1261 - val_loss: 0.1811 - val_mae: 0.2561 - val_mse: 0.1811 - lr: 3.0000e-05\n",
            "Epoch 134/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1191 - mae: 0.2437 - mse: 0.1191 - val_loss: 0.1795 - val_mae: 0.2545 - val_mse: 0.1795 - lr: 3.0000e-05\n",
            "Epoch 135/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1141 - mae: 0.2447 - mse: 0.1141 - val_loss: 0.1787 - val_mae: 0.2538 - val_mse: 0.1787 - lr: 3.0000e-05\n",
            "Epoch 136/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1240 - mae: 0.2467 - mse: 0.1240 - val_loss: 0.1782 - val_mae: 0.2533 - val_mse: 0.1782 - lr: 3.0000e-05\n",
            "Epoch 137/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1156 - mae: 0.2428 - mse: 0.1156 - val_loss: 0.1781 - val_mae: 0.2532 - val_mse: 0.1781 - lr: 3.0000e-05\n",
            "Epoch 138/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1065 - mae: 0.2457 - mse: 0.1065 - val_loss: 0.1777 - val_mae: 0.2527 - val_mse: 0.1777 - lr: 3.0000e-05\n",
            "Epoch 139/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1304 - mae: 0.2446 - mse: 0.1304 - val_loss: 0.1774 - val_mae: 0.2524 - val_mse: 0.1774 - lr: 3.0000e-05\n",
            "Epoch 140/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1222 - mae: 0.2491 - mse: 0.1222 - val_loss: 0.1766 - val_mae: 0.2518 - val_mse: 0.1766 - lr: 3.0000e-05\n",
            "Epoch 141/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1298 - mae: 0.2483 - mse: 0.1298 - val_loss: 0.1764 - val_mae: 0.2515 - val_mse: 0.1764 - lr: 3.0000e-05\n",
            "Epoch 142/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1323 - mae: 0.2532 - mse: 0.1323 - val_loss: 0.1758 - val_mae: 0.2509 - val_mse: 0.1758 - lr: 3.0000e-05\n",
            "Epoch 143/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1233 - mae: 0.2445 - mse: 0.1233 - val_loss: 0.1758 - val_mae: 0.2507 - val_mse: 0.1758 - lr: 3.0000e-05\n",
            "Epoch 144/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1181 - mae: 0.2605 - mse: 0.1181\n",
            "Epoch 144: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1142 - mae: 0.2354 - mse: 0.1142 - val_loss: 0.1759 - val_mae: 0.2508 - val_mse: 0.1759 - lr: 3.0000e-05\n",
            "Epoch 145/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1237 - mae: 0.2426 - mse: 0.1237 - val_loss: 0.1760 - val_mae: 0.2508 - val_mse: 0.1760 - lr: 9.0000e-06\n",
            "Epoch 146/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0975 - mae: 0.2309 - mse: 0.0975\n",
            "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1300 - mae: 0.2369 - mse: 0.1300 - val_loss: 0.1761 - val_mae: 0.2509 - val_mse: 0.1761 - lr: 9.0000e-06\n",
            "Epoch 147/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1105 - mae: 0.2446 - mse: 0.1105 - val_loss: 0.1762 - val_mae: 0.2510 - val_mse: 0.1762 - lr: 2.7000e-06\n",
            "Epoch 148/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0781 - mae: 0.2057 - mse: 0.0781\n",
            "Epoch 148: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1151 - mae: 0.2432 - mse: 0.1151 - val_loss: 0.1761 - val_mae: 0.2509 - val_mse: 0.1761 - lr: 2.7000e-06\n",
            "Epoch 148: early stopping\n",
            "Best epoch: 143\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1406 - mae: 0.2820 - mse: 0.1406\n",
            "[Test loss, Test ['mae', 'mse']]: [0.1405802220106125, 0.28199368715286255, 0.1405802220106125]\n",
            "Epoch 1/171\n",
            "35/35 [==============================] - 5s 6ms/step - loss: 1.0010 - mae: 0.7754 - mse: 1.0010\n",
            "Epoch 2/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9956 - mae: 0.7727 - mse: 0.9956\n",
            "Epoch 3/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9902 - mae: 0.7706 - mse: 0.9902\n",
            "Epoch 4/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9841 - mae: 0.7687 - mse: 0.9841\n",
            "Epoch 5/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9744 - mae: 0.7654 - mse: 0.9744\n",
            "Epoch 6/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9628 - mae: 0.7602 - mse: 0.9628\n",
            "Epoch 7/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9453 - mae: 0.7542 - mse: 0.9453\n",
            "Epoch 8/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.9281 - mae: 0.7473 - mse: 0.9281\n",
            "Epoch 9/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8955 - mae: 0.7352 - mse: 0.8955\n",
            "Epoch 10/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8545 - mae: 0.7173 - mse: 0.8545\n",
            "Epoch 11/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7989 - mae: 0.6935 - mse: 0.7989\n",
            "Epoch 12/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7438 - mae: 0.6669 - mse: 0.7438\n",
            "Epoch 13/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6771 - mae: 0.6386 - mse: 0.6771\n",
            "Epoch 14/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6053 - mae: 0.6002 - mse: 0.6053\n",
            "Epoch 15/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5390 - mae: 0.5589 - mse: 0.5390\n",
            "Epoch 16/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4827 - mae: 0.5235 - mse: 0.4827\n",
            "Epoch 17/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4155 - mae: 0.4848 - mse: 0.4155\n",
            "Epoch 18/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3772 - mae: 0.4632 - mse: 0.3772\n",
            "Epoch 19/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3461 - mae: 0.4384 - mse: 0.3461\n",
            "Epoch 20/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3002 - mae: 0.4148 - mse: 0.3002\n",
            "Epoch 21/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2827 - mae: 0.3909 - mse: 0.2827\n",
            "Epoch 22/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2743 - mae: 0.3780 - mse: 0.2743\n",
            "Epoch 23/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2506 - mae: 0.3569 - mse: 0.2506\n",
            "Epoch 24/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2733 - mae: 0.3630 - mse: 0.2733\n",
            "Epoch 25/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2253 - mae: 0.3405 - mse: 0.2253\n",
            "Epoch 26/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1970 - mae: 0.3239 - mse: 0.1970\n",
            "Epoch 27/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2122 - mae: 0.3180 - mse: 0.2122\n",
            "Epoch 28/171\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.2035 - mae: 0.3170 - mse: 0.2035\n",
            "Epoch 29/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1937 - mae: 0.3123 - mse: 0.1937\n",
            "Epoch 30/171\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.1905 - mae: 0.3040 - mse: 0.1905\n",
            "Epoch 31/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1728 - mae: 0.2851 - mse: 0.1728\n",
            "Epoch 32/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1604 - mae: 0.2827 - mse: 0.1604\n",
            "Epoch 33/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1741 - mae: 0.2848 - mse: 0.1741\n",
            "Epoch 34/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1499 - mae: 0.2723 - mse: 0.1499\n",
            "Epoch 35/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1594 - mae: 0.2749 - mse: 0.1594\n",
            "Epoch 36/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1601 - mae: 0.2763 - mse: 0.1601\n",
            "Epoch 37/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1451 - mae: 0.2687 - mse: 0.1451\n",
            "Epoch 38/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1500 - mae: 0.2607 - mse: 0.1500\n",
            "Epoch 39/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1590 - mae: 0.2603 - mse: 0.1590\n",
            "Epoch 40/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1379 - mae: 0.2462 - mse: 0.1379\n",
            "Epoch 41/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1438 - mae: 0.2604 - mse: 0.1438\n",
            "Epoch 42/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1272 - mae: 0.2400 - mse: 0.1272\n",
            "Epoch 43/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1502 - mae: 0.2542 - mse: 0.1502\n",
            "Epoch 44/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1391 - mae: 0.2494 - mse: 0.1391\n",
            "Epoch 45/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1317 - mae: 0.2374 - mse: 0.1317\n",
            "Epoch 46/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1370 - mae: 0.2437 - mse: 0.1370\n",
            "Epoch 47/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1348 - mae: 0.2482 - mse: 0.1348\n",
            "Epoch 48/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1294 - mae: 0.2382 - mse: 0.1294\n",
            "Epoch 49/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1353 - mae: 0.2366 - mse: 0.1353\n",
            "Epoch 50/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1186 - mae: 0.2303 - mse: 0.1186\n",
            "Epoch 51/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1054 - mae: 0.2225 - mse: 0.1054\n",
            "Epoch 52/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1081 - mae: 0.2274 - mse: 0.1081\n",
            "Epoch 53/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1165 - mae: 0.2294 - mse: 0.1165\n",
            "Epoch 54/171\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.1171 - mae: 0.2321 - mse: 0.1171\n",
            "Epoch 55/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1187 - mae: 0.2230 - mse: 0.1187\n",
            "Epoch 56/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1149 - mae: 0.2213 - mse: 0.1149\n",
            "Epoch 57/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0942 - mae: 0.2173 - mse: 0.0942\n",
            "Epoch 58/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1042 - mae: 0.2200 - mse: 0.1042\n",
            "Epoch 59/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0913 - mae: 0.2073 - mse: 0.0913\n",
            "Epoch 60/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1142 - mae: 0.2174 - mse: 0.1142\n",
            "Epoch 61/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1307 - mae: 0.2134 - mse: 0.1307\n",
            "Epoch 62/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0912 - mae: 0.2045 - mse: 0.0912\n",
            "Epoch 63/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1074 - mae: 0.2058 - mse: 0.1074\n",
            "Epoch 64/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0893 - mae: 0.2027 - mse: 0.0893\n",
            "Epoch 65/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1042 - mae: 0.2071 - mse: 0.1042\n",
            "Epoch 66/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1012 - mae: 0.2082 - mse: 0.1012\n",
            "Epoch 67/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0910 - mae: 0.2002 - mse: 0.0910\n",
            "Epoch 68/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1003 - mae: 0.2113 - mse: 0.1003\n",
            "Epoch 69/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0922 - mae: 0.2002 - mse: 0.0922\n",
            "Epoch 70/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0901 - mae: 0.2076 - mse: 0.0901\n",
            "Epoch 71/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0952 - mae: 0.1908 - mse: 0.0952\n",
            "Epoch 72/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0787 - mae: 0.1958 - mse: 0.0787\n",
            "Epoch 73/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0836 - mae: 0.2018 - mse: 0.0836\n",
            "Epoch 74/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0896 - mae: 0.1960 - mse: 0.0896\n",
            "Epoch 75/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0815 - mae: 0.1949 - mse: 0.0815\n",
            "Epoch 76/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0763 - mae: 0.1926 - mse: 0.0763\n",
            "Epoch 77/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0868 - mae: 0.1929 - mse: 0.0868\n",
            "Epoch 78/171\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0805 - mae: 0.1883 - mse: 0.0805\n",
            "Epoch 79/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0819 - mae: 0.1876 - mse: 0.0819\n",
            "Epoch 80/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0776 - mae: 0.1916 - mse: 0.0776\n",
            "Epoch 81/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0677 - mae: 0.1893 - mse: 0.0677\n",
            "Epoch 82/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0795 - mae: 0.1911 - mse: 0.0795\n",
            "Epoch 83/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0974 - mae: 0.1847 - mse: 0.0974\n",
            "Epoch 84/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0686 - mae: 0.1823 - mse: 0.0686\n",
            "Epoch 85/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0847 - mae: 0.1874 - mse: 0.0847\n",
            "Epoch 86/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0878 - mae: 0.1871 - mse: 0.0878\n",
            "Epoch 87/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0618 - mae: 0.1814 - mse: 0.0618\n",
            "Epoch 88/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0892 - mae: 0.1918 - mse: 0.0892\n",
            "Epoch 89/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0822 - mae: 0.1877 - mse: 0.0822\n",
            "Epoch 90/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0863 - mae: 0.1887 - mse: 0.0863\n",
            "Epoch 91/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0834 - mae: 0.1912 - mse: 0.0834\n",
            "Epoch 92/171\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0753 - mae: 0.1821 - mse: 0.0753\n",
            "Epoch 93/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0690 - mae: 0.1730 - mse: 0.0690\n",
            "Epoch 94/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0563 - mae: 0.1712 - mse: 0.0563\n",
            "Epoch 95/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0732 - mae: 0.1820 - mse: 0.0732\n",
            "Epoch 96/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0685 - mae: 0.1811 - mse: 0.0685\n",
            "Epoch 97/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0970 - mae: 0.1759 - mse: 0.0970\n",
            "Epoch 98/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0899 - mae: 0.1804 - mse: 0.0899\n",
            "Epoch 99/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0681 - mae: 0.1764 - mse: 0.0681\n",
            "Epoch 100/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0605 - mae: 0.1752 - mse: 0.0605\n",
            "Epoch 101/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0756 - mae: 0.1786 - mse: 0.0756\n",
            "Epoch 102/171\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0733 - mae: 0.1757 - mse: 0.0733\n",
            "Epoch 103/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0646 - mae: 0.1667 - mse: 0.0646\n",
            "Epoch 104/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0609 - mae: 0.1708 - mse: 0.0609\n",
            "Epoch 105/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0771 - mae: 0.1755 - mse: 0.0771\n",
            "Epoch 106/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0693 - mae: 0.1747 - mse: 0.0693\n",
            "Epoch 107/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0602 - mae: 0.1687 - mse: 0.0602\n",
            "Epoch 108/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0703 - mae: 0.1700 - mse: 0.0703\n",
            "Epoch 109/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0595 - mae: 0.1694 - mse: 0.0595\n",
            "Epoch 110/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0677 - mae: 0.1746 - mse: 0.0677\n",
            "Epoch 111/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0579 - mae: 0.1639 - mse: 0.0579\n",
            "Epoch 112/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0693 - mae: 0.1770 - mse: 0.0693\n",
            "Epoch 113/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0586 - mae: 0.1678 - mse: 0.0586\n",
            "Epoch 114/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0622 - mae: 0.1690 - mse: 0.0622\n",
            "Epoch 115/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0640 - mae: 0.1716 - mse: 0.0640\n",
            "Epoch 116/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0562 - mae: 0.1719 - mse: 0.0562\n",
            "Epoch 117/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0630 - mae: 0.1750 - mse: 0.0630\n",
            "Epoch 118/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0683 - mae: 0.1684 - mse: 0.0683\n",
            "Epoch 119/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0569 - mae: 0.1621 - mse: 0.0569\n",
            "Epoch 120/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0550 - mae: 0.1640 - mse: 0.0550\n",
            "Epoch 121/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0676 - mae: 0.1715 - mse: 0.0676\n",
            "Epoch 122/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0620 - mae: 0.1647 - mse: 0.0620\n",
            "Epoch 123/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0635 - mae: 0.1669 - mse: 0.0635\n",
            "Epoch 124/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1632 - mse: 0.0532\n",
            "Epoch 125/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.1675 - mse: 0.0649\n",
            "Epoch 126/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0622 - mae: 0.1695 - mse: 0.0622\n",
            "Epoch 127/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0545 - mae: 0.1566 - mse: 0.0545\n",
            "Epoch 128/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0544 - mae: 0.1652 - mse: 0.0544\n",
            "Epoch 129/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0523 - mae: 0.1606 - mse: 0.0523\n",
            "Epoch 130/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0530 - mae: 0.1628 - mse: 0.0530\n",
            "Epoch 131/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0617 - mae: 0.1652 - mse: 0.0617\n",
            "Epoch 132/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0589 - mae: 0.1662 - mse: 0.0589\n",
            "Epoch 133/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0590 - mae: 0.1588 - mse: 0.0590\n",
            "Epoch 134/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0571 - mae: 0.1640 - mse: 0.0571\n",
            "Epoch 135/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0551 - mae: 0.1558 - mse: 0.0551\n",
            "Epoch 136/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0598 - mae: 0.1663 - mse: 0.0598\n",
            "Epoch 137/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0694 - mae: 0.1579 - mse: 0.0694\n",
            "Epoch 138/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0507 - mae: 0.1569 - mse: 0.0507\n",
            "Epoch 139/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0629 - mae: 0.1658 - mse: 0.0629\n",
            "Epoch 140/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0590 - mae: 0.1626 - mse: 0.0590\n",
            "Epoch 141/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1553 - mse: 0.0532\n",
            "Epoch 142/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0484 - mae: 0.1603 - mse: 0.0484\n",
            "Epoch 143/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0537 - mae: 0.1582 - mse: 0.0537\n",
            "Epoch 144/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0481 - mae: 0.1603 - mse: 0.0481\n",
            "Epoch 145/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0568 - mae: 0.1592 - mse: 0.0568\n",
            "Epoch 146/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0460 - mae: 0.1518 - mse: 0.0460\n",
            "Epoch 147/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0522 - mae: 0.1544 - mse: 0.0522\n",
            "Epoch 148/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0511 - mae: 0.1537 - mse: 0.0511\n",
            "Epoch 149/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0439 - mae: 0.1495 - mse: 0.0439\n",
            "Epoch 150/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0422 - mae: 0.1475 - mse: 0.0422\n",
            "Epoch 151/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1567 - mse: 0.0532\n",
            "Epoch 152/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0512 - mae: 0.1554 - mse: 0.0512\n",
            "Epoch 153/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0540 - mae: 0.1503 - mse: 0.0540\n",
            "Epoch 154/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0602 - mae: 0.1566 - mse: 0.0602\n",
            "Epoch 155/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0618 - mae: 0.1595 - mse: 0.0618\n",
            "Epoch 156/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0504 - mae: 0.1572 - mse: 0.0504\n",
            "Epoch 157/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0575 - mae: 0.1581 - mse: 0.0575\n",
            "Epoch 158/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0565 - mae: 0.1550 - mse: 0.0565\n",
            "Epoch 159/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0489 - mae: 0.1449 - mse: 0.0489\n",
            "Epoch 160/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0486 - mae: 0.1495 - mse: 0.0486\n",
            "Epoch 161/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0541 - mae: 0.1589 - mse: 0.0541\n",
            "Epoch 162/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0497 - mae: 0.1554 - mse: 0.0497\n",
            "Epoch 163/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0480 - mae: 0.1507 - mse: 0.0480\n",
            "Epoch 164/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0439 - mae: 0.1513 - mse: 0.0439\n",
            "Epoch 165/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0414 - mae: 0.1497 - mse: 0.0414\n",
            "Epoch 166/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0475 - mae: 0.1474 - mse: 0.0475\n",
            "Epoch 167/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0550 - mae: 0.1513 - mse: 0.0550\n",
            "Epoch 168/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0504 - mae: 0.1545 - mse: 0.0504\n",
            "Epoch 169/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0505 - mae: 0.1530 - mse: 0.0505\n",
            "Epoch 170/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0494 - mae: 0.1500 - mse: 0.0494\n",
            "Epoch 171/171\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0427 - mae: 0.1437 - mse: 0.0427\n",
            "9/9 [==============================] - 1s 3ms/step\n",
            "Epoch 1/2000\n",
            "7/7 [==============================] - 6s 178ms/step - loss: 0.9326 - mae: 0.6787 - mse: 0.9326 - val_loss: 0.7541 - val_mae: 0.6154 - val_mse: 0.7541 - lr: 1.0000e-04\n",
            "Epoch 2/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9203 - mae: 0.6750 - mse: 0.9203 - val_loss: 0.7530 - val_mae: 0.6150 - val_mse: 0.7530 - lr: 1.0000e-04\n",
            "Epoch 3/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.9184 - mae: 0.6745 - mse: 0.9184 - val_loss: 0.7512 - val_mae: 0.6135 - val_mse: 0.7512 - lr: 1.0000e-04\n",
            "Epoch 4/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9224 - mae: 0.6754 - mse: 0.9224 - val_loss: 0.7508 - val_mae: 0.6143 - val_mse: 0.7508 - lr: 1.0000e-04\n",
            "Epoch 5/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9127 - mae: 0.6708 - mse: 0.9127 - val_loss: 0.7489 - val_mae: 0.6126 - val_mse: 0.7489 - lr: 1.0000e-04\n",
            "Epoch 6/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9210 - mae: 0.6721 - mse: 0.9210 - val_loss: 0.7475 - val_mae: 0.6117 - val_mse: 0.7475 - lr: 1.0000e-04\n",
            "Epoch 7/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9124 - mae: 0.6694 - mse: 0.9124 - val_loss: 0.7458 - val_mae: 0.6104 - val_mse: 0.7458 - lr: 1.0000e-04\n",
            "Epoch 8/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9114 - mae: 0.6681 - mse: 0.9114 - val_loss: 0.7444 - val_mae: 0.6095 - val_mse: 0.7444 - lr: 1.0000e-04\n",
            "Epoch 9/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9104 - mae: 0.6677 - mse: 0.9104 - val_loss: 0.7429 - val_mae: 0.6084 - val_mse: 0.7429 - lr: 1.0000e-04\n",
            "Epoch 10/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9077 - mae: 0.6658 - mse: 0.9077 - val_loss: 0.7417 - val_mae: 0.6079 - val_mse: 0.7417 - lr: 1.0000e-04\n",
            "Epoch 11/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.9064 - mae: 0.6652 - mse: 0.9064 - val_loss: 0.7408 - val_mae: 0.6079 - val_mse: 0.7408 - lr: 1.0000e-04\n",
            "Epoch 12/2000\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.9003 - mae: 0.6642 - mse: 0.9003 - val_loss: 0.7394 - val_mae: 0.6072 - val_mse: 0.7394 - lr: 1.0000e-04\n",
            "Epoch 13/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8991 - mae: 0.6632 - mse: 0.8991 - val_loss: 0.7371 - val_mae: 0.6051 - val_mse: 0.7371 - lr: 1.0000e-04\n",
            "Epoch 14/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8967 - mae: 0.6583 - mse: 0.8967 - val_loss: 0.7353 - val_mae: 0.6039 - val_mse: 0.7353 - lr: 1.0000e-04\n",
            "Epoch 15/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8978 - mae: 0.6585 - mse: 0.8978 - val_loss: 0.7337 - val_mae: 0.6030 - val_mse: 0.7337 - lr: 1.0000e-04\n",
            "Epoch 16/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8939 - mae: 0.6580 - mse: 0.8939 - val_loss: 0.7315 - val_mae: 0.6014 - val_mse: 0.7315 - lr: 1.0000e-04\n",
            "Epoch 17/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8960 - mae: 0.6576 - mse: 0.8960 - val_loss: 0.7292 - val_mae: 0.5997 - val_mse: 0.7292 - lr: 1.0000e-04\n",
            "Epoch 18/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8895 - mae: 0.6561 - mse: 0.8895 - val_loss: 0.7269 - val_mae: 0.5982 - val_mse: 0.7269 - lr: 1.0000e-04\n",
            "Epoch 19/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8868 - mae: 0.6540 - mse: 0.8868 - val_loss: 0.7243 - val_mae: 0.5963 - val_mse: 0.7243 - lr: 1.0000e-04\n",
            "Epoch 20/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8817 - mae: 0.6490 - mse: 0.8817 - val_loss: 0.7216 - val_mae: 0.5944 - val_mse: 0.7216 - lr: 1.0000e-04\n",
            "Epoch 21/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8798 - mae: 0.6465 - mse: 0.8798 - val_loss: 0.7189 - val_mae: 0.5927 - val_mse: 0.7189 - lr: 1.0000e-04\n",
            "Epoch 22/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8802 - mae: 0.6485 - mse: 0.8802 - val_loss: 0.7159 - val_mae: 0.5906 - val_mse: 0.7159 - lr: 1.0000e-04\n",
            "Epoch 23/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8749 - mae: 0.6449 - mse: 0.8749 - val_loss: 0.7130 - val_mae: 0.5891 - val_mse: 0.7130 - lr: 1.0000e-04\n",
            "Epoch 24/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8669 - mae: 0.6415 - mse: 0.8669 - val_loss: 0.7099 - val_mae: 0.5878 - val_mse: 0.7099 - lr: 1.0000e-04\n",
            "Epoch 25/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8604 - mae: 0.6370 - mse: 0.8604 - val_loss: 0.7059 - val_mae: 0.5851 - val_mse: 0.7059 - lr: 1.0000e-04\n",
            "Epoch 26/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8589 - mae: 0.6373 - mse: 0.8589 - val_loss: 0.7023 - val_mae: 0.5835 - val_mse: 0.7023 - lr: 1.0000e-04\n",
            "Epoch 27/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8467 - mae: 0.6312 - mse: 0.8467 - val_loss: 0.6982 - val_mae: 0.5813 - val_mse: 0.6982 - lr: 1.0000e-04\n",
            "Epoch 28/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8432 - mae: 0.6283 - mse: 0.8432 - val_loss: 0.6933 - val_mae: 0.5781 - val_mse: 0.6933 - lr: 1.0000e-04\n",
            "Epoch 29/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8400 - mae: 0.6247 - mse: 0.8400 - val_loss: 0.6885 - val_mae: 0.5756 - val_mse: 0.6885 - lr: 1.0000e-04\n",
            "Epoch 30/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8319 - mae: 0.6218 - mse: 0.8319 - val_loss: 0.6823 - val_mae: 0.5710 - val_mse: 0.6823 - lr: 1.0000e-04\n",
            "Epoch 31/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8304 - mae: 0.6177 - mse: 0.8304 - val_loss: 0.6759 - val_mae: 0.5664 - val_mse: 0.6759 - lr: 1.0000e-04\n",
            "Epoch 32/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8163 - mae: 0.6110 - mse: 0.8163 - val_loss: 0.6699 - val_mae: 0.5629 - val_mse: 0.6699 - lr: 1.0000e-04\n",
            "Epoch 33/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.8053 - mae: 0.6061 - mse: 0.8053 - val_loss: 0.6628 - val_mae: 0.5580 - val_mse: 0.6628 - lr: 1.0000e-04\n",
            "Epoch 34/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7993 - mae: 0.6019 - mse: 0.7993 - val_loss: 0.6556 - val_mae: 0.5534 - val_mse: 0.6556 - lr: 1.0000e-04\n",
            "Epoch 35/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7918 - mae: 0.5963 - mse: 0.7918 - val_loss: 0.6486 - val_mae: 0.5498 - val_mse: 0.6486 - lr: 1.0000e-04\n",
            "Epoch 36/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7797 - mae: 0.5903 - mse: 0.7797 - val_loss: 0.6409 - val_mae: 0.5451 - val_mse: 0.6409 - lr: 1.0000e-04\n",
            "Epoch 37/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7621 - mae: 0.5820 - mse: 0.7621 - val_loss: 0.6323 - val_mae: 0.5392 - val_mse: 0.6323 - lr: 1.0000e-04\n",
            "Epoch 38/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7547 - mae: 0.5798 - mse: 0.7547 - val_loss: 0.6239 - val_mae: 0.5343 - val_mse: 0.6239 - lr: 1.0000e-04\n",
            "Epoch 39/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7444 - mae: 0.5713 - mse: 0.7444 - val_loss: 0.6135 - val_mae: 0.5252 - val_mse: 0.6135 - lr: 1.0000e-04\n",
            "Epoch 40/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7254 - mae: 0.5594 - mse: 0.7254 - val_loss: 0.6036 - val_mae: 0.5178 - val_mse: 0.6036 - lr: 1.0000e-04\n",
            "Epoch 41/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.7221 - mae: 0.5582 - mse: 0.7221 - val_loss: 0.5952 - val_mae: 0.5143 - val_mse: 0.5952 - lr: 1.0000e-04\n",
            "Epoch 42/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7100 - mae: 0.5486 - mse: 0.7100 - val_loss: 0.5865 - val_mae: 0.5101 - val_mse: 0.5865 - lr: 1.0000e-04\n",
            "Epoch 43/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6937 - mae: 0.5438 - mse: 0.6937 - val_loss: 0.5763 - val_mae: 0.5023 - val_mse: 0.5763 - lr: 1.0000e-04\n",
            "Epoch 44/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6725 - mae: 0.5373 - mse: 0.6725 - val_loss: 0.5664 - val_mae: 0.4958 - val_mse: 0.5664 - lr: 1.0000e-04\n",
            "Epoch 45/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6593 - mae: 0.5309 - mse: 0.6593 - val_loss: 0.5570 - val_mae: 0.4902 - val_mse: 0.5570 - lr: 1.0000e-04\n",
            "Epoch 46/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6372 - mae: 0.5182 - mse: 0.6372 - val_loss: 0.5472 - val_mae: 0.4837 - val_mse: 0.5472 - lr: 1.0000e-04\n",
            "Epoch 47/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6241 - mae: 0.5106 - mse: 0.6241 - val_loss: 0.5377 - val_mae: 0.4776 - val_mse: 0.5377 - lr: 1.0000e-04\n",
            "Epoch 48/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6151 - mae: 0.5075 - mse: 0.6151 - val_loss: 0.5277 - val_mae: 0.4703 - val_mse: 0.5277 - lr: 1.0000e-04\n",
            "Epoch 49/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5912 - mae: 0.4976 - mse: 0.5912 - val_loss: 0.5179 - val_mae: 0.4634 - val_mse: 0.5179 - lr: 1.0000e-04\n",
            "Epoch 50/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5486 - mae: 0.4820 - mse: 0.5486 - val_loss: 0.5089 - val_mae: 0.4590 - val_mse: 0.5089 - lr: 1.0000e-04\n",
            "Epoch 51/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5747 - mae: 0.4881 - mse: 0.5747 - val_loss: 0.5006 - val_mae: 0.4546 - val_mse: 0.5006 - lr: 1.0000e-04\n",
            "Epoch 52/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5523 - mae: 0.4803 - mse: 0.5523 - val_loss: 0.4924 - val_mae: 0.4505 - val_mse: 0.4924 - lr: 1.0000e-04\n",
            "Epoch 53/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5304 - mae: 0.4652 - mse: 0.5304 - val_loss: 0.4828 - val_mae: 0.4432 - val_mse: 0.4828 - lr: 1.0000e-04\n",
            "Epoch 54/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5219 - mae: 0.4633 - mse: 0.5219 - val_loss: 0.4740 - val_mae: 0.4381 - val_mse: 0.4740 - lr: 1.0000e-04\n",
            "Epoch 55/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5082 - mae: 0.4618 - mse: 0.5082 - val_loss: 0.4659 - val_mae: 0.4345 - val_mse: 0.4659 - lr: 1.0000e-04\n",
            "Epoch 56/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5049 - mae: 0.4533 - mse: 0.5049 - val_loss: 0.4581 - val_mae: 0.4311 - val_mse: 0.4581 - lr: 1.0000e-04\n",
            "Epoch 57/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4974 - mae: 0.4543 - mse: 0.4974 - val_loss: 0.4510 - val_mae: 0.4283 - val_mse: 0.4510 - lr: 1.0000e-04\n",
            "Epoch 58/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4810 - mae: 0.4479 - mse: 0.4810 - val_loss: 0.4443 - val_mae: 0.4266 - val_mse: 0.4443 - lr: 1.0000e-04\n",
            "Epoch 59/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4450 - mae: 0.4336 - mse: 0.4450 - val_loss: 0.4363 - val_mae: 0.4223 - val_mse: 0.4363 - lr: 1.0000e-04\n",
            "Epoch 60/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4711 - mae: 0.4426 - mse: 0.4711 - val_loss: 0.4293 - val_mae: 0.4199 - val_mse: 0.4293 - lr: 1.0000e-04\n",
            "Epoch 61/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4262 - mae: 0.4246 - mse: 0.4262 - val_loss: 0.4224 - val_mae: 0.4170 - val_mse: 0.4224 - lr: 1.0000e-04\n",
            "Epoch 62/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4458 - mae: 0.4295 - mse: 0.4458 - val_loss: 0.4158 - val_mae: 0.4144 - val_mse: 0.4158 - lr: 1.0000e-04\n",
            "Epoch 63/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4020 - mae: 0.4161 - mse: 0.4020 - val_loss: 0.4090 - val_mae: 0.4117 - val_mse: 0.4090 - lr: 1.0000e-04\n",
            "Epoch 64/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3767 - mae: 0.4049 - mse: 0.3767 - val_loss: 0.4028 - val_mae: 0.4098 - val_mse: 0.4028 - lr: 1.0000e-04\n",
            "Epoch 65/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3963 - mae: 0.4102 - mse: 0.3963 - val_loss: 0.3963 - val_mae: 0.4068 - val_mse: 0.3963 - lr: 1.0000e-04\n",
            "Epoch 66/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3853 - mae: 0.4039 - mse: 0.3853 - val_loss: 0.3893 - val_mae: 0.4027 - val_mse: 0.3893 - lr: 1.0000e-04\n",
            "Epoch 67/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3793 - mae: 0.4004 - mse: 0.3793 - val_loss: 0.3826 - val_mae: 0.3994 - val_mse: 0.3826 - lr: 1.0000e-04\n",
            "Epoch 68/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3708 - mae: 0.4027 - mse: 0.3708 - val_loss: 0.3764 - val_mae: 0.3970 - val_mse: 0.3764 - lr: 1.0000e-04\n",
            "Epoch 69/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3496 - mae: 0.3909 - mse: 0.3496 - val_loss: 0.3692 - val_mae: 0.3934 - val_mse: 0.3692 - lr: 1.0000e-04\n",
            "Epoch 70/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3523 - mae: 0.3858 - mse: 0.3523 - val_loss: 0.3628 - val_mae: 0.3908 - val_mse: 0.3628 - lr: 1.0000e-04\n",
            "Epoch 71/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3341 - mae: 0.3804 - mse: 0.3341 - val_loss: 0.3559 - val_mae: 0.3872 - val_mse: 0.3559 - lr: 1.0000e-04\n",
            "Epoch 72/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3406 - mae: 0.3880 - mse: 0.3406 - val_loss: 0.3491 - val_mae: 0.3836 - val_mse: 0.3491 - lr: 1.0000e-04\n",
            "Epoch 73/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3088 - mae: 0.3747 - mse: 0.3088 - val_loss: 0.3431 - val_mae: 0.3809 - val_mse: 0.3431 - lr: 1.0000e-04\n",
            "Epoch 74/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3219 - mae: 0.3671 - mse: 0.3219 - val_loss: 0.3362 - val_mae: 0.3775 - val_mse: 0.3362 - lr: 1.0000e-04\n",
            "Epoch 75/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3076 - mae: 0.3634 - mse: 0.3076 - val_loss: 0.3299 - val_mae: 0.3745 - val_mse: 0.3299 - lr: 1.0000e-04\n",
            "Epoch 76/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2813 - mae: 0.3518 - mse: 0.2813 - val_loss: 0.3238 - val_mae: 0.3717 - val_mse: 0.3238 - lr: 1.0000e-04\n",
            "Epoch 77/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2903 - mae: 0.3561 - mse: 0.2903 - val_loss: 0.3159 - val_mae: 0.3665 - val_mse: 0.3159 - lr: 1.0000e-04\n",
            "Epoch 78/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2976 - mae: 0.3553 - mse: 0.2976 - val_loss: 0.3090 - val_mae: 0.3623 - val_mse: 0.3090 - lr: 1.0000e-04\n",
            "Epoch 79/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2693 - mae: 0.3449 - mse: 0.2693 - val_loss: 0.3040 - val_mae: 0.3608 - val_mse: 0.3040 - lr: 1.0000e-04\n",
            "Epoch 80/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2484 - mae: 0.3367 - mse: 0.2484 - val_loss: 0.2980 - val_mae: 0.3581 - val_mse: 0.2980 - lr: 1.0000e-04\n",
            "Epoch 81/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2559 - mae: 0.3284 - mse: 0.2559 - val_loss: 0.2896 - val_mae: 0.3524 - val_mse: 0.2896 - lr: 1.0000e-04\n",
            "Epoch 82/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2279 - mae: 0.3211 - mse: 0.2279 - val_loss: 0.2824 - val_mae: 0.3479 - val_mse: 0.2824 - lr: 1.0000e-04\n",
            "Epoch 83/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2256 - mae: 0.3193 - mse: 0.2256 - val_loss: 0.2799 - val_mae: 0.3487 - val_mse: 0.2799 - lr: 1.0000e-04\n",
            "Epoch 84/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2360 - mae: 0.3166 - mse: 0.2360 - val_loss: 0.2711 - val_mae: 0.3425 - val_mse: 0.2711 - lr: 1.0000e-04\n",
            "Epoch 85/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.2291 - mae: 0.3108 - mse: 0.2291 - val_loss: 0.2617 - val_mae: 0.3354 - val_mse: 0.2617 - lr: 1.0000e-04\n",
            "Epoch 86/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1996 - mae: 0.2992 - mse: 0.1996 - val_loss: 0.2549 - val_mae: 0.3310 - val_mse: 0.2549 - lr: 1.0000e-04\n",
            "Epoch 87/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2164 - mae: 0.3027 - mse: 0.2164 - val_loss: 0.2513 - val_mae: 0.3295 - val_mse: 0.2513 - lr: 1.0000e-04\n",
            "Epoch 88/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2045 - mae: 0.2978 - mse: 0.2045 - val_loss: 0.2446 - val_mae: 0.3248 - val_mse: 0.2446 - lr: 1.0000e-04\n",
            "Epoch 89/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1929 - mae: 0.2915 - mse: 0.1929 - val_loss: 0.2367 - val_mae: 0.3190 - val_mse: 0.2367 - lr: 1.0000e-04\n",
            "Epoch 90/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1933 - mae: 0.2904 - mse: 0.1933 - val_loss: 0.2293 - val_mae: 0.3134 - val_mse: 0.2293 - lr: 1.0000e-04\n",
            "Epoch 91/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1789 - mae: 0.2799 - mse: 0.1789 - val_loss: 0.2278 - val_mae: 0.3143 - val_mse: 0.2278 - lr: 1.0000e-04\n",
            "Epoch 92/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1794 - mae: 0.2791 - mse: 0.1794 - val_loss: 0.2183 - val_mae: 0.3068 - val_mse: 0.2183 - lr: 1.0000e-04\n",
            "Epoch 93/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1616 - mae: 0.2640 - mse: 0.1616 - val_loss: 0.2126 - val_mae: 0.3026 - val_mse: 0.2126 - lr: 1.0000e-04\n",
            "Epoch 94/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1680 - mae: 0.2710 - mse: 0.1680 - val_loss: 0.2096 - val_mae: 0.3014 - val_mse: 0.2096 - lr: 1.0000e-04\n",
            "Epoch 95/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1686 - mae: 0.2699 - mse: 0.1686 - val_loss: 0.2055 - val_mae: 0.2993 - val_mse: 0.2055 - lr: 1.0000e-04\n",
            "Epoch 96/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1659 - mae: 0.2545 - mse: 0.1659 - val_loss: 0.1973 - val_mae: 0.2910 - val_mse: 0.1973 - lr: 1.0000e-04\n",
            "Epoch 97/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1642 - mae: 0.2626 - mse: 0.1642 - val_loss: 0.1922 - val_mae: 0.2872 - val_mse: 0.1922 - lr: 1.0000e-04\n",
            "Epoch 98/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1560 - mae: 0.2531 - mse: 0.1560 - val_loss: 0.1926 - val_mae: 0.2906 - val_mse: 0.1926 - lr: 1.0000e-04\n",
            "Epoch 99/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1551 - mae: 0.2529 - mse: 0.1551 - val_loss: 0.1852 - val_mae: 0.2838 - val_mse: 0.1852 - lr: 1.0000e-04\n",
            "Epoch 100/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1456 - mae: 0.2454 - mse: 0.1456 - val_loss: 0.1766 - val_mae: 0.2744 - val_mse: 0.1766 - lr: 1.0000e-04\n",
            "Epoch 101/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1387 - mae: 0.2437 - mse: 0.1387 - val_loss: 0.1746 - val_mae: 0.2743 - val_mse: 0.1746 - lr: 1.0000e-04\n",
            "Epoch 102/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1330 - mae: 0.2343 - mse: 0.1330 - val_loss: 0.1744 - val_mae: 0.2759 - val_mse: 0.1744 - lr: 1.0000e-04\n",
            "Epoch 103/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1147 - mae: 0.2294 - mse: 0.1147 - val_loss: 0.1694 - val_mae: 0.2706 - val_mse: 0.1694 - lr: 1.0000e-04\n",
            "Epoch 104/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1194 - mae: 0.2295 - mse: 0.1194 - val_loss: 0.1647 - val_mae: 0.2660 - val_mse: 0.1647 - lr: 1.0000e-04\n",
            "Epoch 105/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1463 - mae: 0.2413 - mse: 0.1463 - val_loss: 0.1617 - val_mae: 0.2639 - val_mse: 0.1617 - lr: 1.0000e-04\n",
            "Epoch 106/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1241 - mae: 0.2245 - mse: 0.1241 - val_loss: 0.1567 - val_mae: 0.2586 - val_mse: 0.1567 - lr: 1.0000e-04\n",
            "Epoch 107/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1310 - mae: 0.2312 - mse: 0.1310 - val_loss: 0.1557 - val_mae: 0.2587 - val_mse: 0.1557 - lr: 1.0000e-04\n",
            "Epoch 108/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1338 - mae: 0.2239 - mse: 0.1338 - val_loss: 0.1519 - val_mae: 0.2551 - val_mse: 0.1519 - lr: 1.0000e-04\n",
            "Epoch 109/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1161 - mae: 0.2240 - mse: 0.1161 - val_loss: 0.1486 - val_mae: 0.2517 - val_mse: 0.1486 - lr: 1.0000e-04\n",
            "Epoch 110/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0969 - mae: 0.2118 - mse: 0.0969 - val_loss: 0.1491 - val_mae: 0.2541 - val_mse: 0.1491 - lr: 1.0000e-04\n",
            "Epoch 111/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1232 - mae: 0.2287 - mse: 0.1232 - val_loss: 0.1473 - val_mae: 0.2529 - val_mse: 0.1473 - lr: 1.0000e-04\n",
            "Epoch 112/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1098 - mae: 0.2175 - mse: 0.1098 - val_loss: 0.1413 - val_mae: 0.2443 - val_mse: 0.1413 - lr: 1.0000e-04\n",
            "Epoch 113/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1304 - mae: 0.2255 - mse: 0.1304 - val_loss: 0.1397 - val_mae: 0.2434 - val_mse: 0.1397 - lr: 1.0000e-04\n",
            "Epoch 114/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1033 - mae: 0.2121 - mse: 0.1033 - val_loss: 0.1410 - val_mae: 0.2471 - val_mse: 0.1410 - lr: 1.0000e-04\n",
            "Epoch 115/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0992 - mae: 0.2107 - mse: 0.0992 - val_loss: 0.1371 - val_mae: 0.2425 - val_mse: 0.1371 - lr: 1.0000e-04\n",
            "Epoch 116/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0971 - mae: 0.2085 - mse: 0.0971 - val_loss: 0.1353 - val_mae: 0.2411 - val_mse: 0.1353 - lr: 1.0000e-04\n",
            "Epoch 117/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1303 - mae: 0.2145 - mse: 0.1303 - val_loss: 0.1347 - val_mae: 0.2418 - val_mse: 0.1347 - lr: 1.0000e-04\n",
            "Epoch 118/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1197 - mae: 0.2061 - mse: 0.1197 - val_loss: 0.1320 - val_mae: 0.2385 - val_mse: 0.1320 - lr: 1.0000e-04\n",
            "Epoch 119/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1150 - mae: 0.2151 - mse: 0.1150 - val_loss: 0.1271 - val_mae: 0.2317 - val_mse: 0.1271 - lr: 1.0000e-04\n",
            "Epoch 120/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1040 - mae: 0.2130 - mse: 0.1040 - val_loss: 0.1250 - val_mae: 0.2294 - val_mse: 0.1250 - lr: 1.0000e-04\n",
            "Epoch 121/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1126 - mae: 0.2032 - mse: 0.1126 - val_loss: 0.1249 - val_mae: 0.2300 - val_mse: 0.1249 - lr: 1.0000e-04\n",
            "Epoch 122/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0886 - mae: 0.2061 - mse: 0.0886 - val_loss: 0.1252 - val_mae: 0.2316 - val_mse: 0.1252 - lr: 1.0000e-04\n",
            "Epoch 123/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1007 - mae: 0.2017 - mse: 0.1007 - val_loss: 0.1208 - val_mae: 0.2250 - val_mse: 0.1208 - lr: 1.0000e-04\n",
            "Epoch 124/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0927 - mae: 0.1952 - mse: 0.0927 - val_loss: 0.1203 - val_mae: 0.2247 - val_mse: 0.1203 - lr: 1.0000e-04\n",
            "Epoch 125/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0831 - mae: 0.1891 - mse: 0.0831 - val_loss: 0.1216 - val_mae: 0.2275 - val_mse: 0.1216 - lr: 1.0000e-04\n",
            "Epoch 126/2000\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1154 - mae: 0.1958 - mse: 0.1154 - val_loss: 0.1183 - val_mae: 0.2225 - val_mse: 0.1183 - lr: 1.0000e-04\n",
            "Epoch 127/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0863 - mae: 0.1945 - mse: 0.0863 - val_loss: 0.1154 - val_mae: 0.2179 - val_mse: 0.1154 - lr: 1.0000e-04\n",
            "Epoch 128/2000\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1062 - mae: 0.1948 - mse: 0.1062 - val_loss: 0.1131 - val_mae: 0.2148 - val_mse: 0.1131 - lr: 1.0000e-04\n",
            "Epoch 129/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0907 - mae: 0.1954 - mse: 0.0907 - val_loss: 0.1156 - val_mae: 0.2196 - val_mse: 0.1156 - lr: 1.0000e-04\n",
            "Epoch 130/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0891 - mae: 0.1989 - mse: 0.0891\n",
            "Epoch 130: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0987 - mae: 0.1966 - mse: 0.0987 - val_loss: 0.1152 - val_mae: 0.2199 - val_mse: 0.1152 - lr: 1.0000e-04\n",
            "Epoch 131/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0758 - mae: 0.1850 - mse: 0.0758 - val_loss: 0.1133 - val_mae: 0.2166 - val_mse: 0.1133 - lr: 3.0000e-05\n",
            "Epoch 132/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0824 - mae: 0.1901 - mse: 0.0824 - val_loss: 0.1116 - val_mae: 0.2136 - val_mse: 0.1116 - lr: 3.0000e-05\n",
            "Epoch 133/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0860 - mae: 0.1928 - mse: 0.0860 - val_loss: 0.1116 - val_mae: 0.2138 - val_mse: 0.1116 - lr: 3.0000e-05\n",
            "Epoch 134/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0582 - mae: 0.1700 - mse: 0.0582\n",
            "Epoch 134: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0835 - mae: 0.1847 - mse: 0.0835 - val_loss: 0.1119 - val_mae: 0.2148 - val_mse: 0.1119 - lr: 3.0000e-05\n",
            "Epoch 135/2000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0893 - mae: 0.1903 - mse: 0.0893 - val_loss: 0.1120 - val_mae: 0.2152 - val_mse: 0.1120 - lr: 9.0000e-06\n",
            "Epoch 136/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.1418 - mae: 0.2144 - mse: 0.1418\n",
            "Epoch 136: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1143 - mae: 0.1942 - mse: 0.1143 - val_loss: 0.1121 - val_mae: 0.2155 - val_mse: 0.1121 - lr: 9.0000e-06\n",
            "Epoch 137/2000\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0762 - mae: 0.1873 - mse: 0.0762 - val_loss: 0.1121 - val_mae: 0.2155 - val_mse: 0.1121 - lr: 2.7000e-06\n",
            "Epoch 138/2000\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.0560 - mae: 0.1729 - mse: 0.0560\n",
            "Epoch 138: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0790 - mae: 0.1918 - mse: 0.0790 - val_loss: 0.1120 - val_mae: 0.2153 - val_mse: 0.1120 - lr: 2.7000e-06\n",
            "Epoch 138: early stopping\n",
            "Best epoch: 133\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 0.1273 - mae: 0.2194 - mse: 0.1273\n",
            "[Test loss, Test ['mae', 'mse']]: [0.12730982899665833, 0.21938127279281616, 0.12730982899665833]\n",
            "Epoch 1/159\n",
            "35/35 [==============================] - 7s 6ms/step - loss: 0.8970 - mae: 0.6779 - mse: 0.8970\n",
            "Epoch 2/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8829 - mae: 0.6674 - mse: 0.8829\n",
            "Epoch 3/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8901 - mae: 0.6621 - mse: 0.8901\n",
            "Epoch 4/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8862 - mae: 0.6594 - mse: 0.8862\n",
            "Epoch 5/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8806 - mae: 0.6559 - mse: 0.8806\n",
            "Epoch 6/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8749 - mae: 0.6510 - mse: 0.8749\n",
            "Epoch 7/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8790 - mae: 0.6519 - mse: 0.8790\n",
            "Epoch 8/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8760 - mae: 0.6484 - mse: 0.8760\n",
            "Epoch 9/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8712 - mae: 0.6425 - mse: 0.8712\n",
            "Epoch 10/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8678 - mae: 0.6428 - mse: 0.8678\n",
            "Epoch 11/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8632 - mae: 0.6413 - mse: 0.8632\n",
            "Epoch 12/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8527 - mae: 0.6351 - mse: 0.8527\n",
            "Epoch 13/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8468 - mae: 0.6301 - mse: 0.8468\n",
            "Epoch 14/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8328 - mae: 0.6237 - mse: 0.8328\n",
            "Epoch 15/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.8172 - mae: 0.6168 - mse: 0.8172\n",
            "Epoch 16/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7887 - mae: 0.6022 - mse: 0.7887\n",
            "Epoch 17/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7641 - mae: 0.5858 - mse: 0.7641\n",
            "Epoch 18/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.7250 - mae: 0.5654 - mse: 0.7250\n",
            "Epoch 19/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6846 - mae: 0.5413 - mse: 0.6846\n",
            "Epoch 20/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.6378 - mae: 0.5145 - mse: 0.6378\n",
            "Epoch 21/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5911 - mae: 0.4933 - mse: 0.5911\n",
            "Epoch 22/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5637 - mae: 0.4846 - mse: 0.5637\n",
            "Epoch 23/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.5232 - mae: 0.4625 - mse: 0.5232\n",
            "Epoch 24/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4999 - mae: 0.4577 - mse: 0.4999\n",
            "Epoch 25/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4639 - mae: 0.4438 - mse: 0.4639\n",
            "Epoch 26/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.4542 - mae: 0.4313 - mse: 0.4542\n",
            "Epoch 27/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3982 - mae: 0.4142 - mse: 0.3982\n",
            "Epoch 28/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3655 - mae: 0.3964 - mse: 0.3655\n",
            "Epoch 29/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3413 - mae: 0.3830 - mse: 0.3413\n",
            "Epoch 30/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.3111 - mae: 0.3736 - mse: 0.3111\n",
            "Epoch 31/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2882 - mae: 0.3589 - mse: 0.2882\n",
            "Epoch 32/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2741 - mae: 0.3496 - mse: 0.2741\n",
            "Epoch 33/159\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.2431 - mae: 0.3344 - mse: 0.2431\n",
            "Epoch 34/159\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.2278 - mae: 0.3270 - mse: 0.2278\n",
            "Epoch 35/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.2370 - mae: 0.3196 - mse: 0.2370\n",
            "Epoch 36/159\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.2124 - mae: 0.3081 - mse: 0.2124\n",
            "Epoch 37/159\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.1936 - mae: 0.2948 - mse: 0.1936\n",
            "Epoch 38/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1800 - mae: 0.2849 - mse: 0.1800\n",
            "Epoch 39/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1841 - mae: 0.2897 - mse: 0.1841\n",
            "Epoch 40/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1684 - mae: 0.2795 - mse: 0.1684\n",
            "Epoch 41/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1595 - mae: 0.2715 - mse: 0.1595\n",
            "Epoch 42/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1604 - mae: 0.2632 - mse: 0.1604\n",
            "Epoch 43/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1438 - mae: 0.2600 - mse: 0.1438\n",
            "Epoch 44/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1707 - mae: 0.2649 - mse: 0.1707\n",
            "Epoch 45/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1427 - mae: 0.2521 - mse: 0.1427\n",
            "Epoch 46/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1331 - mae: 0.2412 - mse: 0.1331\n",
            "Epoch 47/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1312 - mae: 0.2440 - mse: 0.1312\n",
            "Epoch 48/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1208 - mae: 0.2387 - mse: 0.1208\n",
            "Epoch 49/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1095 - mae: 0.2272 - mse: 0.1095\n",
            "Epoch 50/159\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.1230 - mae: 0.2330 - mse: 0.1230\n",
            "Epoch 51/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1278 - mae: 0.2369 - mse: 0.1278\n",
            "Epoch 52/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1045 - mae: 0.2218 - mse: 0.1045\n",
            "Epoch 53/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1155 - mae: 0.2221 - mse: 0.1155\n",
            "Epoch 54/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1176 - mae: 0.2271 - mse: 0.1176\n",
            "Epoch 55/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1157 - mae: 0.2182 - mse: 0.1157\n",
            "Epoch 56/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0951 - mae: 0.2108 - mse: 0.0951\n",
            "Epoch 57/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0919 - mae: 0.2047 - mse: 0.0919\n",
            "Epoch 58/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1007 - mae: 0.2057 - mse: 0.1007\n",
            "Epoch 59/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0915 - mae: 0.2044 - mse: 0.0915\n",
            "Epoch 60/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1003 - mae: 0.2026 - mse: 0.1003\n",
            "Epoch 61/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0989 - mae: 0.2019 - mse: 0.0989\n",
            "Epoch 62/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0752 - mae: 0.1884 - mse: 0.0752\n",
            "Epoch 63/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0815 - mae: 0.1952 - mse: 0.0815\n",
            "Epoch 64/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0857 - mae: 0.1932 - mse: 0.0857\n",
            "Epoch 65/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0831 - mae: 0.1917 - mse: 0.0831\n",
            "Epoch 66/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0804 - mae: 0.1940 - mse: 0.0804\n",
            "Epoch 67/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0859 - mae: 0.1937 - mse: 0.0859\n",
            "Epoch 68/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0783 - mae: 0.1858 - mse: 0.0783\n",
            "Epoch 69/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0848 - mae: 0.1884 - mse: 0.0848\n",
            "Epoch 70/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0676 - mae: 0.1795 - mse: 0.0676\n",
            "Epoch 71/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0746 - mae: 0.1768 - mse: 0.0746\n",
            "Epoch 72/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0842 - mae: 0.1860 - mse: 0.0842\n",
            "Epoch 73/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0839 - mae: 0.1831 - mse: 0.0839\n",
            "Epoch 74/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0670 - mae: 0.1771 - mse: 0.0670\n",
            "Epoch 75/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0770 - mae: 0.1755 - mse: 0.0770\n",
            "Epoch 76/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0739 - mae: 0.1764 - mse: 0.0739\n",
            "Epoch 77/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0750 - mae: 0.1778 - mse: 0.0750\n",
            "Epoch 78/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0623 - mae: 0.1731 - mse: 0.0623\n",
            "Epoch 79/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0607 - mae: 0.1730 - mse: 0.0607\n",
            "Epoch 80/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0668 - mae: 0.1726 - mse: 0.0668\n",
            "Epoch 81/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0714 - mae: 0.1710 - mse: 0.0714\n",
            "Epoch 82/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0704 - mae: 0.1782 - mse: 0.0704\n",
            "Epoch 83/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0694 - mae: 0.1722 - mse: 0.0694\n",
            "Epoch 84/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0638 - mae: 0.1681 - mse: 0.0638\n",
            "Epoch 85/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0698 - mae: 0.1681 - mse: 0.0698\n",
            "Epoch 86/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0562 - mae: 0.1620 - mse: 0.0562\n",
            "Epoch 87/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0618 - mae: 0.1694 - mse: 0.0618\n",
            "Epoch 88/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0764 - mae: 0.1689 - mse: 0.0764\n",
            "Epoch 89/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0687 - mae: 0.1667 - mse: 0.0687\n",
            "Epoch 90/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0609 - mae: 0.1644 - mse: 0.0609\n",
            "Epoch 91/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0746 - mae: 0.1670 - mse: 0.0746\n",
            "Epoch 92/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0586 - mae: 0.1604 - mse: 0.0586\n",
            "Epoch 93/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0641 - mae: 0.1632 - mse: 0.0641\n",
            "Epoch 94/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0536 - mae: 0.1585 - mse: 0.0536\n",
            "Epoch 95/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0679 - mae: 0.1642 - mse: 0.0679\n",
            "Epoch 96/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0623 - mae: 0.1605 - mse: 0.0623\n",
            "Epoch 97/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0496 - mae: 0.1568 - mse: 0.0496\n",
            "Epoch 98/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0638 - mae: 0.1650 - mse: 0.0638\n",
            "Epoch 99/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0690 - mae: 0.1646 - mse: 0.0690\n",
            "Epoch 100/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0530 - mae: 0.1540 - mse: 0.0530\n",
            "Epoch 101/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0564 - mae: 0.1549 - mse: 0.0564\n",
            "Epoch 102/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0520 - mae: 0.1526 - mse: 0.0520\n",
            "Epoch 103/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0493 - mae: 0.1535 - mse: 0.0493\n",
            "Epoch 104/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0508 - mae: 0.1549 - mse: 0.0508\n",
            "Epoch 105/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0588 - mae: 0.1550 - mse: 0.0588\n",
            "Epoch 106/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0533 - mae: 0.1517 - mse: 0.0533\n",
            "Epoch 107/159\n",
            "35/35 [==============================] - 0s 7ms/step - loss: 0.0500 - mae: 0.1570 - mse: 0.0500\n",
            "Epoch 108/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0484 - mae: 0.1564 - mse: 0.0484\n",
            "Epoch 109/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0519 - mae: 0.1568 - mse: 0.0519\n",
            "Epoch 110/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0484 - mae: 0.1520 - mse: 0.0484\n",
            "Epoch 111/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0571 - mae: 0.1518 - mse: 0.0571\n",
            "Epoch 112/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0625 - mae: 0.1600 - mse: 0.0625\n",
            "Epoch 113/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0531 - mae: 0.1551 - mse: 0.0531\n",
            "Epoch 114/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0552 - mae: 0.1501 - mse: 0.0552\n",
            "Epoch 115/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0492 - mae: 0.1494 - mse: 0.0492\n",
            "Epoch 116/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0509 - mae: 0.1530 - mse: 0.0509\n",
            "Epoch 117/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0487 - mae: 0.1443 - mse: 0.0487\n",
            "Epoch 118/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0545 - mae: 0.1541 - mse: 0.0545\n",
            "Epoch 119/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0512 - mae: 0.1487 - mse: 0.0512\n",
            "Epoch 120/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0556 - mae: 0.1468 - mse: 0.0556\n",
            "Epoch 121/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0571 - mae: 0.1490 - mse: 0.0571\n",
            "Epoch 122/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0421 - mae: 0.1438 - mse: 0.0421\n",
            "Epoch 123/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0532 - mae: 0.1510 - mse: 0.0532\n",
            "Epoch 124/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0475 - mae: 0.1459 - mse: 0.0475\n",
            "Epoch 125/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0580 - mae: 0.1508 - mse: 0.0580\n",
            "Epoch 126/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0453 - mae: 0.1426 - mse: 0.0453\n",
            "Epoch 127/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0595 - mae: 0.1553 - mse: 0.0595\n",
            "Epoch 128/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0449 - mae: 0.1472 - mse: 0.0449\n",
            "Epoch 129/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0494 - mae: 0.1454 - mse: 0.0494\n",
            "Epoch 130/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0508 - mae: 0.1412 - mse: 0.0508\n",
            "Epoch 131/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0437 - mae: 0.1439 - mse: 0.0437\n",
            "Epoch 132/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0439 - mae: 0.1436 - mse: 0.0439\n",
            "Epoch 133/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0551 - mae: 0.1472 - mse: 0.0551\n",
            "Epoch 134/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0491 - mae: 0.1446 - mse: 0.0491\n",
            "Epoch 135/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0600 - mae: 0.1524 - mse: 0.0600\n",
            "Epoch 136/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0427 - mae: 0.1408 - mse: 0.0427\n",
            "Epoch 137/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0409 - mae: 0.1401 - mse: 0.0409\n",
            "Epoch 138/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0505 - mae: 0.1445 - mse: 0.0505\n",
            "Epoch 139/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0465 - mae: 0.1410 - mse: 0.0465\n",
            "Epoch 140/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0580 - mae: 0.1456 - mse: 0.0580\n",
            "Epoch 141/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0412 - mae: 0.1370 - mse: 0.0412\n",
            "Epoch 142/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0550 - mae: 0.1419 - mse: 0.0550\n",
            "Epoch 143/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0468 - mae: 0.1371 - mse: 0.0468\n",
            "Epoch 144/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0412 - mae: 0.1386 - mse: 0.0412\n",
            "Epoch 145/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0473 - mae: 0.1393 - mse: 0.0473\n",
            "Epoch 146/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0471 - mae: 0.1431 - mse: 0.0471\n",
            "Epoch 147/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0442 - mae: 0.1379 - mse: 0.0442\n",
            "Epoch 148/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0462 - mae: 0.1387 - mse: 0.0462\n",
            "Epoch 149/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0458 - mae: 0.1410 - mse: 0.0458\n",
            "Epoch 150/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0416 - mae: 0.1342 - mse: 0.0416\n",
            "Epoch 151/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0464 - mae: 0.1441 - mse: 0.0464\n",
            "Epoch 152/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0400 - mae: 0.1364 - mse: 0.0400\n",
            "Epoch 153/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0465 - mae: 0.1382 - mse: 0.0465\n",
            "Epoch 154/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0466 - mae: 0.1395 - mse: 0.0466\n",
            "Epoch 155/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.1379 - mse: 0.0419\n",
            "Epoch 156/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0427 - mae: 0.1335 - mse: 0.0427\n",
            "Epoch 157/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0606 - mae: 0.1399 - mse: 0.0606\n",
            "Epoch 158/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0491 - mae: 0.1344 - mse: 0.0491\n",
            "Epoch 159/159\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0471 - mae: 0.1366 - mse: 0.0471\n",
            "9/9 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "DEXahcmysBtP",
        "outputId": "d29abbc2-3255-4bf3-bc14-6a3302f46670"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lets_plot.plot.core.PlotSpec at 0x7d6ae7e49d20>"
            ],
            "text/html": [
              "<html lang=\"en\">\n",
              "   <head>\n",
              "       <script type=\"text/javascript\" data-lets-plot-script=\"library\" src=\"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.0/js-package/distr/lets-plot.min.js\"></script>\n",
              "   </head>\n",
              "   <body>\n",
              "          <div id=\"3LDMov\"></div>\n",
              "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
              "       var plotSpec={\n",
              "\"data\":{\n",
              "\"epoch\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0],\n",
              "\"loss\":[\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\",\"val_loss\"],\n",
              "\"value\":[0.8954892754554749,0.8927105069160461,0.9020295739173889,0.8794109225273132,0.8789118528366089,0.8737947344779968,0.8811411261558533,0.8863463997840881,0.8785973191261292,0.8587303161621094,0.8588404655456543,0.8660411834716797,0.8551729917526245,0.8387224674224854,0.8467682003974915,0.8539869785308838,0.8438261151313782,0.8434677124023438,0.852216362953186,0.8469242453575134,0.8383850455284119,0.8301711678504944,0.8363490700721741,0.8288554549217224,0.8171762228012085,0.8229203820228577,0.8361276984214783,0.8527635335922241,0.8236287236213684,0.8179097175598145,0.8232667446136475,0.8217074275016785,0.8135356903076172,0.8042867183685303,0.8153062462806702,0.8099381923675537,0.8156049251556396,0.8149818181991577,0.8038926720619202,0.8040645122528076,0.8110884428024292,0.781345784664154,0.8008430004119873,0.8046262264251709,0.790857195854187,0.7786024808883667,0.8024075031280518,0.7964644432067871,0.7778717875480652,0.7785291075706482,0.7676259875297546,0.7661818265914917,0.7652318477630615,0.7913251519203186,0.7637151479721069,0.7773438692092896,0.7542417049407959,0.7619096040725708,0.7727996706962585,0.7507910132408142,0.7505529522895813,0.75312739610672,0.7531607747077942,0.7490913271903992,0.7378501892089844,0.7487753629684448,0.743043065071106,0.7594084143638611,0.7397559285163879,0.7425696849822998,0.7202825546264648,0.7179375290870667,0.7342902421951294,0.7254003286361694,0.7379081845283508,0.7268508672714233,0.7179062962532043,0.7296229004859924,0.7082456350326538,0.7343794107437134,0.7305536270141602,0.7132221460342407,0.7255990505218506,0.7038130164146423,0.7158874273300171,0.7032274007797241,0.6916885375976563,0.7112998962402344,0.7159829139709473,0.7013587951660156,0.6989700794219971,0.6914793252944946,0.6975542902946472,0.6869003772735596,0.6893382668495178,0.681415319442749,0.7005779147148132,0.6763153076171875,0.6872650384902954,0.6790981888771057,0.6783221364021301,0.660767674446106,0.6747419238090515,0.6831309199333191,0.6558207869529724,0.6791082620620728,0.6750795245170593,0.6591584086418152,0.6572605967521667,0.6453592777252197,0.6726879477500916,0.6656453013420105,0.6769172549247742,0.6460174918174744,0.6523256897926331,0.6447136998176575,0.6405353546142578,0.6199788451194763,0.6605718731880188,0.6355593204498291,0.6271092295646667,0.6236646771430969,0.6300987601280212,0.6275413036346436,0.6377884149551392,0.6273499131202698,0.6255010962486267,0.6292237639427185,0.6169177889823914,0.6158484220504761,0.6137273907661438,0.6199878454208374,0.6062721610069275,0.5968214869499207,0.5874097943305969,0.6029062271118164,0.6119330525398254,0.5984171032905579,0.6074404120445251,0.6093646883964539,0.5871692895889282,0.5864477753639221,0.5768091082572937,0.5803781151771545,0.5973718762397766,0.5725295543670654,0.5775047540664673,0.5833016037940979,0.5770964026451111,0.5835402607917786,0.5691094994544983,0.5976449251174927,0.5797097682952881,0.5566821694374084,0.5655632019042969,0.5700914859771729,0.5496854186058044,0.5572605729103088,0.5521376729011536,0.5593454241752625,0.5606005191802979,0.5638461709022522,0.5434255599975586,0.5569545030593872,0.5479660034179688,0.5367658138275146,0.5387465357780457,0.5439602136611938,0.5390039682388306,0.5396843552589417,0.5229329466819763,0.5247973799705505,0.5260405540466309,0.5304886698722839,0.5285617709159851,0.5312721729278564,0.5383735299110413,0.5401420593261719,0.5269777774810791,0.526256263256073,0.5089641213417053,0.5228514075279236,0.5149164199829102,0.526638925075531,0.5121564865112305,0.49906277656555176,0.5118101239204407,0.5233114957809448,0.4962591826915741,0.509976863861084,0.5126463770866394,0.4948905110359192,0.49631816148757935,0.49781161546707153,0.5031301379203796,0.506982147693634,0.49135515093803406,0.4792601764202118,0.4967561364173889,0.49523618817329407,0.4852617681026459,0.5059220790863037,0.4713385999202728,0.4731138348579407,0.4580090343952179,0.48199528455734253,0.47848913073539734,0.49185889959335327,0.4607198238372803,0.4653068482875824,0.4684319794178009,0.45200151205062866,0.47118648886680603,0.45675331354141235,0.47707366943359375,0.4604139029979706,0.47015029191970825,0.4614774286746979,0.46718356013298035,0.46702322363853455,0.4449383616447449,0.45383456349372864,0.4554169476032257,0.4693748354911804,0.458296537399292,0.4604911804199219,0.44114506244659424,0.44273698329925537,0.4441550672054291,0.42859378457069397,0.4437388479709625,0.4363265633583069,0.43394672870635986,0.44940856099128723,0.4386417269706726,0.4550131559371948,0.43439868092536926,0.43446311354637146,0.45088785886764526,0.42761850357055664,0.4200328290462494,0.44323134422302246,0.4369952082633972,0.427571564912796,0.4120366871356964,0.4474714994430542,0.42755842208862305,0.4457925260066986,0.41964012384414673,0.4355663061141968,0.4302298426628113,0.4136159420013428,0.4090212881565094,0.4087195098400116,0.4121427536010742,0.4112585186958313,0.42118480801582336,0.4040171504020691,0.42407673597335815,0.4105674624443054,0.40932580828666687,0.40770769119262695,0.40891051292419434,0.4132189452648163,0.4018806517124176,0.3902709186077118,0.3945416212081909,0.40573304891586304,0.3988921642303467,0.41684690117836,0.4095199406147003,0.399549663066864,0.40376123785972595,0.40034082531929016,0.40980783104896545,0.3952005207538605,0.4096803367137909,0.3960598409175873,0.39272764325141907,0.4000370502471924,0.39693430066108704,0.38943707942962646,0.40868687629699707,0.3843017518520355,0.38561585545539856,0.37574949860572815,0.38702651858329773,0.3683793544769287,0.37477749586105347,0.3919456899166107,0.3876361846923828,0.3805792033672333,0.3867800235748291,0.38676485419273376,0.37455782294273376,0.3620329797267914,0.37652042508125305,0.372056245803833,0.38236480951309204,0.37854552268981934,0.3724610209465027,0.3634141981601715,0.38900333642959595,0.379609614610672,0.36739662289619446,0.37097543478012085,0.37924277782440186,0.3711707592010498,0.3720358610153198,0.3787451982498169,0.37872812151908875,0.38572439551353455,0.3786863088607788,0.36945924162864685,0.37997177243232727,0.36471134424209595,0.3635544776916504,0.37729519605636597,0.3694978356361389,0.36121827363967896,0.3726995289325714,0.3723098039627075,0.36486878991127014,0.37547776103019714,0.3750481605529785,0.3690018057823181,0.3642467260360718,0.36123672127723694,0.37812283635139465,0.3519008457660675,0.36270084977149963,0.3616483211517334,0.3721647560596466,0.3729625344276428,0.35980224609375,0.3582681119441986,0.354092538356781,0.3732421398162842,0.35795173048973083,0.3551782965660095,0.3810584545135498,0.3606710731983185,0.3664329946041107,0.3690822422504425,0.36049413681030273,0.35522329807281494,0.3549613356590271,0.36415621638298035,0.3740237057209015,0.3472127914428711,0.35780563950538635,0.38075852394104004,0.35547399520874023,0.35494425892829895,0.37534257769584656,0.3591637909412384,0.35298845171928406,0.35308003425598145,0.36726242303848267,0.36490708589553833,0.36748844385147095,0.3675239384174347,0.3670353889465332,0.3620830774307251,0.3703620731830597,0.3715169429779053,0.34584200382232666,0.3645254671573639,0.36701861023902893,0.3610216975212097,0.3645918667316437,0.35612010955810547,0.3717760145664215,0.3559962809085846,0.36811965703964233,0.3706217110157013,0.3590239882469177,0.3486259877681732,0.3629562556743622,0.36685556173324585,0.35292553901672363,0.351723313331604,0.35325145721435547,0.36928504705429077,0.33653366565704346,0.3659811317920685,0.8862223625183105,0.8842995166778564,0.8824660778045654,0.8805922865867615,0.8787304162979126,0.8767653107643127,0.8748817443847656,0.8730081915855408,0.8711132407188416,0.8692596554756165,0.8674575686454773,0.865681529045105,0.8639304637908936,0.862130343914032,0.8603767156600952,0.858622133731842,0.8569315671920776,0.8551622033119202,0.8534466624259949,0.85174560546875,0.8501196503639221,0.8485338687896729,0.8469235301017761,0.8453567028045654,0.843766450881958,0.8420697450637817,0.84043949842453,0.8388116359710693,0.8372196555137634,0.8356175422668457,0.834061324596405,0.8324369192123413,0.8308682441711426,0.8293185234069824,0.8277236819267273,0.8260897397994995,0.8244014382362366,0.8227402567863464,0.8210092186927795,0.8193711042404175,0.817704975605011,0.8160272240638733,0.8142956495285034,0.8126386404037476,0.810936450958252,0.809270441532135,0.8075965046882629,0.8059508204460144,0.8042467832565308,0.8024733066558838,0.8006901144981384,0.7988935708999634,0.7970815896987915,0.7953127026557922,0.7935833930969238,0.7918185591697693,0.7900084853172302,0.7882593870162964,0.7864672541618347,0.7846508026123047,0.7828275561332703,0.7810183763504028,0.7792423367500305,0.77739018201828,0.775501012802124,0.7736565470695496,0.7718179225921631,0.7700043320655823,0.7681117653846741,0.7662255764007568,0.7642134428024292,0.7622225284576416,0.7602238059043884,0.7582136392593384,0.7561755776405334,0.7541755437850952,0.7521569728851318,0.7500734925270081,0.7478261590003967,0.7455853819847107,0.7436019778251648,0.7415412068367004,0.7394101023674011,0.7373302578926086,0.7351846098899841,0.732934832572937,0.7307007908821106,0.728321373462677,0.7259546518325806,0.7235554456710815,0.721191942691803,0.7188068628311157,0.7164999842643738,0.7140354514122009,0.711597740650177,0.7090854048728943,0.7065727710723877,0.7040416598320007,0.7015596628189087,0.6990892291069031,0.6965508460998535,0.6939899325370789,0.6914876103401184,0.6888508796691895,0.6863648295402527,0.6837804317474365,0.6813696026802063,0.6788454055786133,0.67638099193573,0.6737927198410034,0.6711747050285339,0.6685537695884705,0.6660387516021729,0.6636319160461426,0.6611785292625427,0.6589186191558838,0.6566286087036133,0.6540928483009338,0.6515032052993774,0.6490328907966614,0.646556556224823,0.6441996693611145,0.6416734457015991,0.6391352415084839,0.6365827918052673,0.6343989968299866,0.6321938037872314,0.6302650570869446,0.628082811832428,0.6256629824638367,0.6231048703193665,0.6208609938621521,0.6185410022735596,0.6160719394683838,0.6136096715927124,0.6113854050636292,0.6091591715812683,0.6068682670593262,0.6046136021614075,0.6023972034454346,0.6002169847488403,0.5978816747665405,0.59527987241745,0.5926884412765503,0.5903350710868835,0.5875906348228455,0.585054874420166,0.5826517939567566,0.580230176448822,0.5780029296875,0.575904905796051,0.573779284954071,0.5717588067054749,0.5694870948791504,0.567413330078125,0.5652680993080139,0.5630173087120056,0.5608771443367004,0.5585101842880249,0.555866003036499,0.5532642602920532,0.5509974956512451,0.5491927266120911,0.5471270680427551,0.5453004240989685,0.5434173345565796,0.5410904884338379,0.5386642813682556,0.5363405346870422,0.5341231822967529,0.5322263836860657,0.530432939529419,0.5284890532493591,0.5261387825012207,0.5238651037216187,0.5215569734573364,0.5194042921066284,0.5176701545715332,0.5156893730163574,0.5138359069824219,0.512478768825531,0.5106258392333984,0.5085820555686951,0.5069918632507324,0.5047351121902466,0.5024142861366272,0.5003683567047119,0.49872374534606934,0.4972291588783264,0.496034175157547,0.4940294325351715,0.491919606924057,0.48955225944519043,0.48782023787498474,0.48636582493782043,0.48566049337387085,0.48446935415267944,0.48223602771759033,0.47946402430534363,0.4781932532787323,0.4770166873931885,0.4757571518421173,0.47385796904563904,0.47171750664711,0.4690721035003662,0.4665147662162781,0.4644814431667328,0.46267351508140564,0.4608050584793091,0.4587864875793457,0.45611926913261414,0.4538450241088867,0.45147672295570374,0.44978317618370056,0.44853320717811584,0.4472576081752777,0.4464632570743561,0.44562575221061707,0.44371989369392395,0.4422369599342346,0.44080132246017456,0.4395866096019745,0.43793532252311707,0.4360821545124054,0.43392011523246765,0.4313083589076996,0.4290594458580017,0.4276566803455353,0.42548754811286926,0.42464324831962585,0.42374521493911743,0.42204248905181885,0.41949963569641113,0.41767987608909607,0.41539594531059265,0.4138576090335846,0.412798672914505,0.4119584858417511,0.41121748089790344,0.40999382734298706,0.40793147683143616,0.4064487814903259,0.40498778223991394,0.4042832851409912,0.40303465723991394,0.40178415179252625,0.4003012776374817,0.3989105224609375,0.3975776433944702,0.3958371579647064,0.39422717690467834,0.3930538594722748,0.392187237739563,0.39177650213241577,0.3910326659679413,0.3900117874145508,0.3879741430282593,0.38608798384666443,0.3841874599456787,0.38352054357528687,0.38297513127326965,0.3824685215950012,0.3816409111022949,0.3806931972503662,0.38021451234817505,0.3793160915374756,0.37817445397377014,0.3762146830558777,0.3748439848423004,0.37337177991867065,0.3721587657928467,0.37098464369773865,0.37007516622543335,0.3689784109592438,0.3682205080986023,0.36775562167167664,0.367491215467453,0.36728379130363464,0.36611345410346985,0.36507049202919006,0.3633292615413666,0.36148279905319214,0.36039865016937256,0.35911253094673157,0.3586905002593994,0.3582604229450226,0.3576963245868683,0.35699400305747986,0.3559566140174866,0.35473257303237915,0.35363852977752686,0.3527984023094177,0.35205426812171936,0.351671040058136,0.3511698842048645,0.3504086136817932,0.3499951660633087,0.34931617975234985,0.34892842173576355,0.34817594289779663,0.34776559472084045,0.34622153639793396,0.344930499792099,0.3436215817928314,0.3424232602119446,0.3414756655693054,0.3403964936733246,0.3394542336463928,0.3389756381511688,0.3382589817047119,0.33697062730789185,0.3351830542087555,0.3339642286300659,0.33269456028938293,0.33192333579063416,0.33137041330337524,0.3305499255657196,0.329814612865448,0.32958558201789856,0.32924509048461914,0.32883456349372864,0.32829296588897705,0.32790684700012207,0.32789894938468933,0.32730138301849365,0.3262620270252228,0.32521703839302063,0.32359349727630615,0.32307907938957214,0.32333335280418396,0.32325926423072815,0.32315793633461,0.32298117876052856,0.3228940963745117,0.32281553745269775,0.32269880175590515,0.32257553935050964,0.3225480318069458,0.3224356770515442,0.3223048746585846,0.32223838567733765,0.32219064235687256,0.32215601205825806,0.32210636138916016,0.3220765292644501,0.3220438063144684,0.3220021724700928,0.32198143005371094,0.3219658136367798,0.32195109128952026,0.3219369649887085,0.3219250440597534,0.3219181299209595,0.3219080865383148,0.3218984305858612,0.3218919634819031,0.32188740372657776,0.32188016176223755,0.32187119126319885,0.32187169790267944,0.32186853885650635,0.32186615467071533,0.3218608796596527,0.32184985280036926,0.32184216380119324,0.321836918592453,0.32183021306991577,0.3218217194080353,0.3218151032924652,0.32179978489875793,0.32178646326065063,0.32178089022636414,0.32177433371543884,0.32176390290260315,0.32175201177597046,0.3217451572418213,0.32174041867256165,0.3217320442199707,0.32171282172203064,0.32169878482818604,0.3216942846775055,0.32169923186302185,0.32170313596725464,0.32170772552490234,0.3217115104198456,0.32170626521110535]\n",
              "},\n",
              "\"mapping\":{\n",
              "},\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"theme\":{\n",
              "\"legend_position\":[1.0,1.0],\n",
              "\"legend_justification\":[1.0,1.0]\n",
              "},\n",
              "\"ggtitle\":{\n",
              "\"text\":\"Suitable Model Result for Ca_y - (RNN/mae)\",\n",
              "\"subtitle\":\"train loss:0.3517, val_loss:0.3217, test loss:0.2431\"\n",
              "},\n",
              "\"caption\":{\n",
              "\"text\":\"red dash line: Optimal epoch \\n Master Thesis @ Alfred Ofosu, 08/2023\"\n",
              "},\n",
              "\"ggsize\":{\n",
              "\"width\":800.0,\n",
              "\"height\":400.0\n",
              "},\n",
              "\"kind\":\"plot\",\n",
              "\"scales\":[{\n",
              "\"name\":\"Epoch\",\n",
              "\"aesthetic\":\"x\"\n",
              "},{\n",
              "\"name\":\"Error [%]\",\n",
              "\"aesthetic\":\"y\"\n",
              "},{\n",
              "\"name\":\"\",\n",
              "\"aesthetic\":\"color\"\n",
              "},{\n",
              "\"aesthetic\":\"y\",\n",
              "\"trans\":\"log10\"\n",
              "}],\n",
              "\"layers\":[{\n",
              "\"geom\":\"line\",\n",
              "\"mapping\":{\n",
              "\"x\":\"epoch\",\n",
              "\"y\":\"value\",\n",
              "\"color\":\"loss\"\n",
              "},\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"size\":0.75,\n",
              "\"data\":{\n",
              "}\n",
              "},{\n",
              "\"geom\":\"vline\",\n",
              "\"mapping\":{\n",
              "},\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"xintercept\":381.0,\n",
              "\"color\":\"red\",\n",
              "\"linetype\":\"dashed\",\n",
              "\"size\":1.0,\n",
              "\"data\":{\n",
              "}\n",
              "}],\n",
              "\"metainfo_list\":[]\n",
              "};\n",
              "       var plotContainer = document.getElementById(\"3LDMov\");\n",
              "       LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
              "   </script>\n",
              "   </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "rXrAu5raLF6H",
        "outputId": "2bb1f988-400b-4a7b-935a-383ce81e55d9"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lets_plot.plot.core.PlotSpec at 0x7d6a60d20070>"
            ],
            "text/html": [
              "<html lang=\"en\">\n",
              "   <head>\n",
              "       <script type=\"text/javascript\" data-lets-plot-script=\"library\" src=\"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.0/js-package/distr/lets-plot.min.js\"></script>\n",
              "   </head>\n",
              "   <body>\n",
              "          <div id=\"0Z537E\"></div>\n",
              "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
              "       var plotSpec={\n",
              "\"data\":{\n",
              "\"date\":[1.5777504E12,1.5804288E12,1.5829344E12,1.5856128E12,1.5882048E12,1.5908832E12,1.5934752E12,1.5961536E12,1.598832E12,1.601424E12,1.6041024E12,1.6066944E12,1.6093728E12,1.6120512E12,1.6144704E12,1.6171488E12,1.6197408E12,1.6224192E12,1.6250112E12,1.6276896E12,1.630368E12,1.63296E12,1.6356384E12,1.6382304E12,1.6409088E12],\n",
              "\"y_invs\":[104.0,152.0,100.85977935791016,128.0,116.53949737548828,141.0,116.0,55.79999923706055,113.0,115.0,56.099998474121094,58.79999923706055,85.80000305175781,131.0,103.0,100.0,95.80000305175781,97.4000015258789,87.5999984741211,103.1906967163086,74.16666412353516,54.70000076293945,69.0,80.69999694824219,109.0],\n",
              "\"y_pred_invs\":[103.68280792236328,144.0318603515625,102.09149169921875,122.51628112792969,117.32290649414063,134.3835906982422,115.95561218261719,58.9173583984375,113.44918823242188,114.51173400878906,62.873504638671875,62.817039489746094,86.4917221069336,131.18296813964844,104.81536865234375,99.08330535888672,96.51456451416016,96.88066101074219,86.57686614990234,103.82439422607422,76.50871276855469,62.07851028442383,72.08882904052734,79.15631866455078,108.22994232177734]\n",
              "},\n",
              "\"mapping\":{\n",
              "},\n",
              "\"data_meta\":{\n",
              "\"series_annotations\":[{\n",
              "\"column\":\"date\",\n",
              "\"type\":\"datetime\"\n",
              "}]\n",
              "},\n",
              "\"ggtitle\":{\n",
              "\"text\":\"Actual vs. Predicted Values of Ca_y - (RNN/mae)\",\n",
              "\"subtitle\":\"Predicited Water Quality over the last 24 months\"\n",
              "},\n",
              "\"caption\":{\n",
              "\"text\":\"black line: Actual Values, red dash line: Predicted Values \\n Master Thesis @ Alfred Ofosu, 08/2023\"\n",
              "},\n",
              "\"ggsize\":{\n",
              "\"width\":800.0,\n",
              "\"height\":400.0\n",
              "},\n",
              "\"kind\":\"plot\",\n",
              "\"scales\":[{\n",
              "\"name\":\"\",\n",
              "\"aesthetic\":\"x\"\n",
              "},{\n",
              "\"name\":\"Ca_y in [%]\",\n",
              "\"aesthetic\":\"y\"\n",
              "},{\n",
              "\"name\":\"\",\n",
              "\"aesthetic\":\"color\"\n",
              "},{\n",
              "\"aesthetic\":\"x\",\n",
              "\"format\":\"%b %Y\",\n",
              "\"datetime\":true\n",
              "}],\n",
              "\"layers\":[{\n",
              "\"geom\":\"line\",\n",
              "\"mapping\":{\n",
              "\"x\":\"date\",\n",
              "\"y\":\"y_invs\"\n",
              "},\n",
              "\"show_legend\":true,\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"size\":1.0,\n",
              "\"alpha\":1.0,\n",
              "\"color\":\"#1985a1\",\n",
              "\"data\":{\n",
              "}\n",
              "},{\n",
              "\"geom\":\"line\",\n",
              "\"mapping\":{\n",
              "\"x\":\"date\",\n",
              "\"y\":\"y_pred_invs\"\n",
              "},\n",
              "\"show_legend\":true,\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"size\":0.8,\n",
              "\"alpha\":0.6,\n",
              "\"color\":\"red\",\n",
              "\"linetype\":\"dashed\",\n",
              "\"data\":{\n",
              "}\n",
              "}],\n",
              "\"metainfo_list\":[]\n",
              "};\n",
              "       var plotContainer = document.getElementById(\"0Z537E\");\n",
              "       LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
              "   </script>\n",
              "   </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "-fo1lTAI9CGF",
        "outputId": "47248381-ec52-49fd-fbff-9c37abf3318c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lets_plot.plot.core.PlotSpec at 0x7d6ab0d948b0>"
            ],
            "text/html": [
              "<html lang=\"en\">\n",
              "   <head>\n",
              "       <script type=\"text/javascript\" data-lets-plot-script=\"library\" src=\"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.0/js-package/distr/lets-plot.min.js\"></script>\n",
              "   </head>\n",
              "   <body>\n",
              "          <div id=\"oJv37f\"></div>\n",
              "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
              "       var plotSpec={\n",
              "\"data\":{\n",
              "\"date\":[1.419984E12,1.4226624E12,1.4250816E12,1.42776E12,1.430352E12,1.4330304E12,1.4356224E12,1.4383008E12,1.4409792E12,1.4435712E12,1.4462496E12,1.4488416E12,1.45152E12,1.4541984E12,1.456704E12,1.4593824E12,1.4619744E12,1.4646528E12,1.4672448E12,1.4699232E12,1.4726016E12,1.4751936E12,1.477872E12,1.480464E12,1.4831424E12,1.4858208E12,1.48824E12,1.4909184E12,1.4935104E12,1.4961888E12,1.4987808E12,1.5014592E12,1.5041376E12,1.5067296E12,1.509408E12,1.512E12,1.5146784E12,1.5173568E12,1.519776E12,1.5224544E12,1.5250464E12,1.5277248E12,1.5303168E12,1.5329952E12,1.5356736E12,1.5382656E12,1.540944E12,1.543536E12,1.5462144E12,1.5488928E12,1.551312E12,1.5539904E12,1.5565824E12,1.5592608E12,1.5618528E12,1.5645312E12,1.5672096E12,1.5698016E12,1.57248E12,1.575072E12,1.5777504E12,1.5804288E12,1.5829344E12,1.5856128E12,1.5882048E12,1.5908832E12,1.5934752E12,1.5961536E12,1.598832E12,1.601424E12,1.6041024E12,1.6066944E12,1.6093728E12,1.6120512E12,1.6144704E12,1.6171488E12,1.6197408E12,1.6224192E12,1.6250112E12,1.6276896E12,1.630368E12,1.63296E12,1.6356384E12,1.6382304E12,1.6409088E12],\n",
              "\"y_invs\":[122.0,127.0,116.0,128.0,110.0,117.0,91.0999984741211,73.5,70.0999984741211,95.30000305175781,102.0,106.0,84.80000305175781,131.0,138.0,102.0,120.0,97.4000015258789,104.0,98.9000015258789,54.599998474121094,99.30000305175781,92.5999984741211,106.0,140.0,121.0,128.0,130.0,116.0,116.63333129882813,92.0,72.0,72.9000015258789,102.0,80.0999984741211,107.0,134.0,92.0999984741211,99.69999694824219,116.0,84.19999694824219,103.0,103.0,66.69999694824219,56.29999923706055,78.5,111.0,110.0,118.0,92.5,0.11300231516361237,122.0,107.0,105.0,104.0,89.0999984741211,90.5999984741211,87.9000015258789,66.0,84.9000015258789,104.0,152.0,100.85977935791016,128.0,116.53949737548828,141.0,116.0,55.79999923706055,113.0,115.0,56.099998474121094,58.79999923706055,85.80000305175781,131.0,103.0,100.0,95.80000305175781,97.4000015258789,87.5999984741211,103.1906967163086,74.16666412353516,54.70000076293945,69.0,80.69999694824219,109.0],\n",
              "\"y_pred_invs\":[122.31998443603516,126.19274139404297,117.17106628417969,127.43196105957031,111.0659408569336,118.53424835205078,90.58679962158203,75.97593688964844,70.93695831298828,95.47953033447266,101.50946044921875,105.98926544189453,84.62786102294922,129.3179931640625,133.44078063964844,102.12278747558594,122.09053039550781,97.66012573242188,104.87327575683594,98.56157684326172,61.288658142089844,98.79853820800781,92.81657409667969,108.23826599121094,138.0806427001953,121.92851257324219,129.3971710205078,130.19334411621094,116.52143096923828,117.36265563964844,92.1113510131836,72.36793518066406,73.13519287109375,102.7502670288086,79.985107421875,108.28439331054688,135.33920288085938,92.55338287353516,101.30107116699219,117.30722045898438,83.57344055175781,103.42854309082031,102.02745819091797,69.77332305908203,61.22990798950195,80.0855484008789,111.25532531738281,109.65450286865234,118.01126861572266,94.23722076416016,43.48904800415039,124.08967590332031,106.93466186523438,104.0216064453125,102.99006652832031,88.83157348632813,90.24634552001953,87.74607849121094,73.1588363647461,85.16370391845703,103.68280792236328,144.0318603515625,102.09149169921875,122.51628112792969,117.32290649414063,134.3835906982422,115.95561218261719,58.9173583984375,113.44918823242188,114.51173400878906,62.873504638671875,62.817039489746094,86.4917221069336,131.18296813964844,104.81536865234375,99.08330535888672,96.51456451416016,96.88066101074219,86.57686614990234,103.82439422607422,76.50871276855469,62.07851028442383,72.08882904052734,79.15631866455078,108.22994232177734]\n",
              "},\n",
              "\"mapping\":{\n",
              "},\n",
              "\"data_meta\":{\n",
              "\"series_annotations\":[{\n",
              "\"column\":\"date\",\n",
              "\"type\":\"datetime\"\n",
              "}]\n",
              "},\n",
              "\"ggtitle\":{\n",
              "\"text\":\"True vs. Predicted Values of Ca_y - (RNN/mae)\",\n",
              "\"subtitle\":\"Predicited Water Quality over the last 84 months\"\n",
              "},\n",
              "\"caption\":{\n",
              "\"text\":\"black line: Actual Values, red dash line: Predicted Values \\n Master Thesis @ Alfred Ofosu, 08/2023\"\n",
              "},\n",
              "\"ggsize\":{\n",
              "\"width\":800.0,\n",
              "\"height\":400.0\n",
              "},\n",
              "\"kind\":\"plot\",\n",
              "\"scales\":[{\n",
              "\"name\":\"\",\n",
              "\"aesthetic\":\"x\"\n",
              "},{\n",
              "\"name\":\"Ca_y in [%]\",\n",
              "\"aesthetic\":\"y\"\n",
              "},{\n",
              "\"name\":\"\",\n",
              "\"aesthetic\":\"color\"\n",
              "},{\n",
              "\"aesthetic\":\"x\",\n",
              "\"format\":\"%b %Y\",\n",
              "\"datetime\":true\n",
              "}],\n",
              "\"layers\":[{\n",
              "\"geom\":\"line\",\n",
              "\"mapping\":{\n",
              "\"x\":\"date\",\n",
              "\"y\":\"y_invs\"\n",
              "},\n",
              "\"show_legend\":true,\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"size\":1.0,\n",
              "\"alpha\":1.0,\n",
              "\"color\":\"#1985a1\",\n",
              "\"data\":{\n",
              "}\n",
              "},{\n",
              "\"geom\":\"line\",\n",
              "\"mapping\":{\n",
              "\"x\":\"date\",\n",
              "\"y\":\"y_pred_invs\"\n",
              "},\n",
              "\"show_legend\":true,\n",
              "\"data_meta\":{\n",
              "},\n",
              "\"size\":0.8,\n",
              "\"alpha\":0.6,\n",
              "\"color\":\"red\",\n",
              "\"linetype\":\"dashed\",\n",
              "\"data\":{\n",
              "}\n",
              "}],\n",
              "\"metainfo_list\":[]\n",
              "};\n",
              "       var plotContainer = document.getElementById(\"oJv37f\");\n",
              "       LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
              "   </script>\n",
              "   </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.DataFrame({\"y\":y_test, \"y_pred\":y_pred},).reset_index()\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "scaler = normalizer(1)\n",
        "scaler.fit(dataset_to_model[[target]])\n",
        "data['y_invs'] = scaler.inverse_transform(data[['y']])\n",
        "data['y_pred_invs'] = scaler.inverse_transform(data[['y_pred']])\n",
        "plot_3 = (\n",
        "    ggplot(data.tail(85))\n",
        "  + geom_line(aes(x=\"date\", y=\"y_invs\"),size=1, alpha=1, show_legend=True)\n",
        "  + geom_line(aes(x=\"date\", y=\"y_pred_invs\", ),size=0.8, alpha=0.6, color=\"red\",\n",
        "              linetype=\"dashed\",\n",
        "              show_legend=True)\n",
        "  + labs(title=f\"True vs. Predicted Values of {target} - ({model_name}/{loss_fn})\",\n",
        "        subtitle=f\"Predicited Water Quality over the last {85 - 1} months\",\n",
        "      x=\"\", y=f'{target} in [%]', color=\"\",\n",
        "             caption=\"black line: Actual Values, red dash line: Predicted Values \\n Master Thesis @ Alfred Ofosu, 08/2023\")\n",
        "+ scale_x_datetime(format=\"%b %Y\")\n",
        "+ ggsize(800, 400)\n",
        ")\n",
        "\n",
        "ggsave(plot_3, f'predicted_{target}_{model_name}_{loss_fn}_84mons.png',\n",
        "                  path=f'{local_path}/data/images/CCMEWQI', scale=1.0)\n",
        "plot_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "ygPf9LCWIG3X",
        "outputId": "71718dbb-40d4-4ef6-a39a-066b3333f55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-407-de0c541a4d6d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_to_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_invs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred_invs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3813\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6129\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6130\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6132\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Ca_y'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW52a_w2JaHE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Prepare Timeseries Data\n",
        "def create_timeseries_dataset(scaled_dataset:pd.DataFrame,\n",
        "                              approach_type:str,\n",
        "                              sequence_length:int,\n",
        "                              sampling_rate:int, batch_size:int,\n",
        "                              target_list:list,\n",
        "                              test_size:int) -> dict:\n",
        "  dateset_dict = {}\n",
        "  for target in target_list:\n",
        "    global n_features\n",
        "    # Create feature and target values for approach 1\n",
        "    if approach_type == \"all\":\n",
        "      df = scaled_dataset.copy()\n",
        "      X = df.values.astype(np.float32)\n",
        "      y = df[target].values.astype(np.float32)\n",
        "    else:\n",
        "      # Create feature and target values for approach 2\n",
        "      X = scaled_dataset[[column for column in scaled_dataset.columns if column.endswith('_X')]]\n",
        "      X = X.values.astype(np.float32)\n",
        "      df_target = scaled_dataset[[column for column in scaled_dataset.columns if column.endswith('_y')]]\n",
        "      y = df_target[target].values.astype(np.float32)\n",
        "\n",
        "    num_train_samples = int(0.7 * len(X))\n",
        "    num_val_samples = int(0.2 * len(X))\n",
        "    # num_test_samples = test_size, #len(X) - num_train_samples - num_val_samples\n",
        "\n",
        "    sampling_rate = sampling_rate\n",
        "    sequence_length = sequence_length\n",
        "    delay = sampling_rate * (sequence_length + 1 - 1)\n",
        "    batch_size = batch_size\n",
        "\n",
        "    train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        X[:-delay],\n",
        "        targets=y[delay:],\n",
        "        sequence_length=sequence_length,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=SEED,\n",
        "        start_index=0,\n",
        "        end_index=num_train_samples,\n",
        "    )\n",
        "\n",
        "    val_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        X[:-delay],\n",
        "        targets=y[delay:],\n",
        "        sequence_length=sequence_length,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=SEED,\n",
        "        start_index=num_train_samples,\n",
        "        end_index=num_train_samples+num_val_samples,\n",
        "    )\n",
        "\n",
        "    test_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        X[:-delay],\n",
        "        targets=y[delay:],\n",
        "        sequence_length=sequence_length,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=SEED,\n",
        "        start_index=num_train_samples+num_val_samples,\n",
        "    )\n",
        "\n",
        "    full_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        X[:-delay],\n",
        "        targets=y[delay:],\n",
        "        sequence_length=sequence_length,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=SEED,\n",
        "        start_index=0,\n",
        "        end_index=num_train_samples + num_val_samples,\n",
        "    )\n",
        "    # Get the shape for the model\n",
        "    for samples, targets in train_dataset:\n",
        "      sequence_length = samples.shape[1]\n",
        "      n_features = samples.shape[-1]\n",
        "      break\n",
        "    dateset_dict[target] = {'full_dataset':full_dataset,\n",
        "                            'train_dataset': train_dataset,\n",
        "                            'val_dataset': val_dataset,\n",
        "                            'test_dataset': test_dataset,\n",
        "                            'sequence_length':sequence_length,\n",
        "                            'n_features':n_features}\n",
        "  return dateset_dict\n",
        "\n",
        "model_feature_target = create_timeseries_dataset(dataset_scaled,\n",
        "                                                 approach_type=\"all\",\n",
        "                                                 sequence_length=1,\n",
        "                                                 sampling_rate=1,\n",
        "                                                 batch_size= 128, # * strategy.num_replicas_in_sync,\n",
        "                                                 target_list=parameters['water quality parameters'],\n",
        "                                                 test_size=13,\n",
        "                                                 )\n",
        "\n",
        "# model_feature_target_x = create_timeseries_dataset(dataset_scaled_x,\n",
        "#                                                     approach_type=\"\",\n",
        "#                                                     sequence_length=1,\n",
        "#                                                     sampling_rate=1,\n",
        "#                                                     batch_size=256,\n",
        "#                                                     target_list=parameters['water quality parameters x'],\n",
        "#                                                     test_size=26,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Js0JTqT5CF"
      },
      "outputs": [],
      "source": [
        "# for target, inner_dict in model_feature_target.items():\n",
        "#     for key , batch_dataset in inner_dict.items():\n",
        "#       if key == 'target_dataset' or key == 'val_dataset' or key == 'test_dataset' or key == 'full_dataset':\n",
        "#         for samples, targets in batch_dataset:\n",
        "#           print(f\"       target:{target}\" )\n",
        "#           print(f\"samples shape:{samples.shape}\")\n",
        "#           print(f\"targets shape:{targets.shape}\\n\")\n",
        "#           break\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wEoJE8_xI2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522b5f5f-c13f-4df8-8913-22d0a460a6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       target:Ca_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n",
            "       target:Cl_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n",
            "       target:K_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n",
            "       target:Mg_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n",
            "       target:Na_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n",
            "       target:SO42_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n",
            "       target:TDS_y\n",
            "samples shape:(256, 1, 15)\n",
            "targets shape:(256,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# for target, inner_dict in model_feature_target_x.items():\n",
        "#   for key , batch_dataset in inner_dict.items():\n",
        "#     if key == 'target_dataset' or key == 'val_dataset' or key == 'test_dataset':\n",
        "#       for samples, targets in batch_dataset:\n",
        "#         print(f\"       target:{target}\" )\n",
        "#         print(f\"samples shape:{samples.shape}\")\n",
        "#         print(f\"targets shape:{targets.shape}\\n\")\n",
        "#         break\n",
        "#       break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSvjWUpRTtsA"
      },
      "source": [
        "# Tune hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlfEf5xZoOx7"
      },
      "source": [
        "Deciding on the number of hyperparameters to tune can be very challenging because the hyperparameter optimizer can not be left to optimize the entire search space as this may cause overfitting. technically, hyperparameter optimization does not optimize the weights of the model so it would make sense to tune hyperparameters that directly/indirectly have an effect on the objective function, the loss function. Other hyperparameters to consider would be the number of layers and neurons. Additionally, the choice of optimization function to use. Bare in mind that the search space grows combinatorially as the size of the hyperparameters grows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMDqqXVPfH9R"
      },
      "source": [
        "When small training data sizes are fed to large neural networks they tend to overfit and perform poorly on the test data and a well-known technique is to use dropouts[(Hinton et. al, 2012)](https://arxiv.org/pdf/2201.06433.pdf). Dropouts terminate neurons to avoid them from co-adapting features within the neural network layer. The dropout rate\n",
        "is the fraction of the features that are zeroed out; its usually set between 0.2 and 0.5 (Chollet, 2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR11pp9jrraL"
      },
      "outputs": [],
      "source": [
        "approach= \"up_down\" # @param {type:\"string\"}\n",
        "# filepath = f'./data/modelling_data/hyperOpt/keras_tuner/{approach}' # @param {type:\"string\"}\n",
        "# gs_filepath = '/content/colab_bucket ' # @param {type:\"string\"}\n",
        "gs_filepath = 'gs://master-thesis-yorku-aofosu' # @param {type:\"string\"}\n",
        "loss_fn = 'mae' # @param {type:\"string\"}\n",
        "model_name = 'rnn' # @param {type:\"string\"}\n",
        "\n",
        "if loss_fn == 'mae':\n",
        "  metric = 'mse'\n",
        "else: metric = 'mae'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnYYRMdlAHy2"
      },
      "source": [
        "# Tensorflow - Keras Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-5S2q2WAQR_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Single-Layered Neural Network (sLNN - Baseline)\n",
        "# def build_sLnn(hp):\n",
        "#   \"\"\"\n",
        "#   Builds a single layered neural model.\n",
        "#   \"\"\"\n",
        "#   # Create the model\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   x = tf.keras.layers.Flatten()(x)\n",
        "#   # Add the input layer\n",
        "#   x = tf.keras.layers.Dense(units=hp.Int('num_units', min_value=32, max_value=512, step=32,),\n",
        "#                             activation=hp.Choice('activation', values=['relu', 'elu', 'selu', 'gelu', 'tanh', ]),\n",
        "#                             )(x)\n",
        "\n",
        "#   # Tune whether to use dropout.\n",
        "#   if hp.Boolean(\"dropout\"):\n",
        "#       x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "#   outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   # Compile the model\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "#                 optimizer=tf.keras.optimizers.Adam(),\n",
        "#                 metrics=[metric]\n",
        "#                 )\n",
        "#   return model\n",
        "\n",
        "# Optuna\n",
        "def build_sLnn(trial):\n",
        "  \"\"\"Builds a single layer neural model.\"\"\"\n",
        "\n",
        "  # Create the model\n",
        "  inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "  x = inputs\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  # Add the input layer\n",
        "  x = tf.keras.layers.Dense(units=trial.suggest_int(f\"units\", 32, 128, step=32),\n",
        "                            activation=trial.suggest_categorical(f\"activation\", ['elu', 'relu']),\n",
        "                            name=\"hidden_layer\",\n",
        "                            )(x)\n",
        "  # Tune whether to use dropout before passing it to the output layer.\n",
        "  x = tf.keras.layers.Dropout(rate=trial.suggest_categorical(\"dropout\", [0.2, 0.5]))(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='linear', name='main_output')(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=loss_fn,\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "                metrics=[metric]\n",
        "                )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JaENIRktBRaF"
      },
      "outputs": [],
      "source": [
        "#@title Deep Neural Network (DNN)\n",
        "# # Keras\n",
        "# def build_dnn(hp):\n",
        "#   \"\"\"\n",
        "#   Builds a multi-layered neural network model.\n",
        "#   \"\"\"\n",
        "#   # Create the model\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   x = tf.keras.layers.Flatten()(x)\n",
        "#   for i in range(hp.Int(\"dense_layers\", min_value=1, max_value=10,)):\n",
        "#       x = tf.keras.layers.Dense(\n",
        "#           units=hp.Int(\"units_L\" + str(i), min_value=32, max_value=513, step=32,),\n",
        "#           activation=hp.Choice('activation' + str(i), values=['relu', 'elu', 'selu', 'gelu', 'tanh']),\n",
        "#       )(x)\n",
        "#       # Tune whether to use dropout before passing it to the output layer.\n",
        "#       if hp.Boolean(\"dropout_L\"+ str(i)):\n",
        "#           x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "#   outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "#                 optimizer=tf.keras.optimizers.Adam(),\n",
        "#                 metrics=[metric]\n",
        "#   )\n",
        "#   return model\n",
        "# Optuna\n",
        "def build_dnn(trial):\n",
        "  \"\"\"\n",
        "  Builds a multi-layered neural network model.\n",
        "  \"\"\"\n",
        "  # Create the model\n",
        "  inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "  x = inputs\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  for i in range(trial.suggest_int(\"n_layers\", 1, 5, log=True)):\n",
        "    x = tf.keras.layers.Dense(\n",
        "        units=trial.suggest_int(f\"units_L{i}\", 32, 128, step=32),\n",
        "        activation=trial.suggest_categorical(f\"activation_L{i}\", ['elu', 'relu']),\n",
        "        name=f\"dense_{i}\")(x)\n",
        "  # Tune whether to use dropout before passing it to the output layer.\n",
        "  x = tf.keras.layers.Dropout(rate=trial.suggest_categorical(\"dropout\", [0.2, 0.5]))(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(loss=loss_fn,\n",
        "                #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "                metrics=[metric]\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTOdy0V0Bkmk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Convolutional Neural Network (CNN)\n",
        "# # Keras\n",
        "# def build_cnn(hp):\n",
        "#   \"\"\"\n",
        "#   Builds a convolutional neural network model.\n",
        "#   \"\"\"\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   for i in range(hp.Int(\"conv_layers\", min_value=1, max_value=5,)):\n",
        "#       x = tf.keras.layers.Conv1D(\n",
        "#           filters=hp.Int(\"filters_\" + str(i), min_value=32, max_value=513, step=32),\n",
        "#           kernel_size=hp.Int(\"kernel_size_\" + str(i), 3, 5),\n",
        "#           activation=hp.Choice('activation_conv', values=['relu', 'elu', 'selu', 'gelu', 'tanh', ]),\n",
        "#           padding=\"same\",\n",
        "#       )(x)\n",
        "#   if hp.Choice(\"global_pooling\", [\"max\", \"avg\"]) == \"max\":\n",
        "#       x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "#   else:\n",
        "#       x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "#     # Tune whether to use dropout before passing it to the output layer.\n",
        "#   if hp.Boolean(\"dropout_L\"): #+ str(i)\n",
        "#       x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "#   outputs = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "#                 optimizer=tf.keras.optimizers.Adam(),\n",
        "#                 metrics=[metric]\n",
        "#   )\n",
        "#   return model\n",
        "\n",
        "# Optuna\n",
        "def build_cnn(trial):\n",
        "  \"\"\"\n",
        "  Builds a convolutional neural network model.\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "  x = inputs\n",
        "  for i in range(trial.suggest_int(\"n_conv_layers\", 1, 5)):\n",
        "    x = tf.keras.layers.Conv1D(\n",
        "        filters=trial.suggest_int(f\"filters_L{i}\", 32, 128, log=True),\n",
        "        kernel_size=trial.suggest_int(f\"kernel_size_L{i}\", 2, 3),\n",
        "        activation=trial.suggest_categorical(f\"activation_L{i}\", ['elu', 'relu']),\n",
        "        padding=\"causal\", name=f\"conv1d_L{i}\",\n",
        "        # kernel_regularizer=tf.keras.regularizers.L2(0.1),\n",
        "        # bias_regularizer=tf.keras.regularizers.L2(0.1),\n",
        "    )(x)\n",
        "  if trial.suggest_categorical(\"global_pooling\", [\"max\", \"avg\"])  == \"max\":\n",
        "    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "  else:\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "  # Tune whether to use dropout before passing it to the output layer.\n",
        "  x = tf.keras.layers.Dropout(rate=trial.suggest_categorical(\"dropout\", [0.2, 0.5]))(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(loss=loss_fn,\n",
        "                #optimizer=trial.suggest_categorical(\"optimizer\", ['rmsprop', 'adam']),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "                metrics=[metric]\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Yc8IBzB3S9"
      },
      "outputs": [],
      "source": [
        "#@title Recurrent Neural Network (RNN)\n",
        "# Keras\n",
        "# def build_rnn(hp,):\n",
        "#   \"\"\"\n",
        "#   Builds a recurrent neural network model.\n",
        "#   \"\"\"\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   n_LSTM_layers = hp.Int(\"n_lstm_layers\", min_value=1, max_value=3, default=3)\n",
        "#   for i in range(n_LSTM_layers):\n",
        "#     if i == n_LSTM_layers -1:\n",
        "#       # Add the last LSTM layer without return sequences\n",
        "#       x = tf.keras.layers.LSTM(\n",
        "#           units=hp.Int(\"units_L\" + str(i), min_value=32, max_value=513, step=32),\n",
        "#           return_sequences=False,\n",
        "#           name=f\"lstm_L{i}\",\n",
        "#       )(x)\n",
        "#     else:\n",
        "#       # Add the first and intermediate LSTM layers with return sequences\n",
        "#       x = tf.keras.layers.LSTM(\n",
        "#           units=hp.Int(\"units_L\" + str(i), min_value=32, max_value=513, step=32),\n",
        "#           return_sequences=True,\n",
        "#           name=f\"lstm_L{i}\",\n",
        "#           )(x)\n",
        "#   # Tune whether to use dropout before passing it to the output layer.\n",
        "#   if hp.Boolean(\"dropout\"):\n",
        "#       x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "#   outputs = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 optimizer=hp.Choice(name=\"optimizer\", values=['rmsprop','adam']),\n",
        "#                 # optimizer=tf.keras.optimizers.Adam(),\n",
        "#                 metrics=[metric]\n",
        "#   )\n",
        "#   return model\n",
        "\n",
        "# Optuna\n",
        "def build_rnn(trial):\n",
        "  \"\"\"\n",
        "  Builds a recurrent neural network model.\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "  x = inputs\n",
        "  n_LSTM_layers = trial.suggest_int(\"n_lstm_layers\", 1, 2)\n",
        "  for i in range(n_LSTM_layers):\n",
        "    if i == n_LSTM_layers -1:\n",
        "      # Add the last LSTM layer without return sequences\n",
        "      x = tf.keras.layers.LSTM(\n",
        "          units=trial.suggest_int(f\"units_L{i}\",32, 64, log=True),\n",
        "          return_sequences=False,\n",
        "          recurrent_dropout=0.5,\n",
        "          name=f\"lstm_L{i}\",\n",
        "      )(x)\n",
        "    else:\n",
        "      # Add the first and intermediate LSTM layers with return sequences\n",
        "      x = tf.keras.layers.LSTM(\n",
        "          units=trial.suggest_int(f\"units_L{i}\",32, 64, log=True),\n",
        "          return_sequences=True,\n",
        "          recurrent_dropout=0.2,\n",
        "          name=f\"lstm_L{i}\",\n",
        "          )(x)\n",
        "  # Tune whether to use dropout before passing it to the output layer.\n",
        "  x = tf.keras.layers.Dropout(rate=trial.suggest_categorical(\"dropout\", [0.2, 0.5]))(x)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='linear', name='output')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(loss=loss_fn,\n",
        "                optimizer=trial.suggest_categorical(\"optimizer\", ['rmsprop', 'adam']),\n",
        "                # optimizer=tf.keras.optimizers.Adam(learning_rate=trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)),\n",
        "                metrics=[metric]\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJFjt9LUXjIe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Water Quality Index - Multiclass Prediction\n",
        "# def build_cnn_rnn(hp,):\n",
        "#   \"\"\"\n",
        "#   Builds a convolutional / recurrent neural network model.\n",
        "#   \"\"\"\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   # Convolutional Layers\n",
        "#   for i in range(hp.Int(\"conv_layers\", min_value=1, max_value=5,)):\n",
        "#       x = tf.keras.layers.Conv1D(\n",
        "#           filters=hp.Int(\"filters_\" + str(i), min_value=32, max_value=513, step=32),\n",
        "#           kernel_size=hp.Int(\"kernel_size_\" + str(i), 3, 5),\n",
        "#           activation=hp.Choice('activation_conv', values=['relu', 'elu', 'selu', 'gelu', 'tanh', ]),\n",
        "#           padding=\"same\",\n",
        "#       )(x)\n",
        "#   if hp.Choice(\"pooling\", [\"max\", \"avg\"]) == \"max\":\n",
        "#       x = tf.keras.layers.MaxPooling1D()(x)\n",
        "#   else:\n",
        "#       x = tf.keras.layers.AveragePooling1D()(x)\n",
        "#   # Recurrent Neural Network Layers\n",
        "#   n_LSTM_layers = hp.Int(\"n_lstm_layers\", min_value=1, max_value=5,)\n",
        "#   for i in range(n_LSTM_layers):\n",
        "#     if i == n_LSTM_layers - 1:\n",
        "#       # Add the last LSTM layer without return sequences\n",
        "#       x = tf.keras.layers.LSTM(\n",
        "#           units=hp.Int(\"units_L\" + str(i), min_value=32, max_value=513, step=32),\n",
        "#           return_sequences=False,\n",
        "#           name=f\"lstm_L{i}\",\n",
        "#       )(x)\n",
        "#     else:\n",
        "#       # Add the first and intermediate LSTM layers with return sequences\n",
        "#       x = tf.keras.layers.LSTM(\n",
        "#           units=hp.Int(\"units_L\" + str(i), min_value=32, max_value=513, step=32),\n",
        "#           return_sequences=True,\n",
        "#           name=f\"lstm_L{i}\",\n",
        "#           )(x)\n",
        "#   # Tune whether to use dropout before passing it to the output layer.\n",
        "#   if hp.Boolean(\"dropout\"): #+ str(i)\n",
        "#       x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "#   outputs = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 #optimizer=hp.Choice(name=\"optimizer\", values['rmsprop', 'adam']),\n",
        "#                 optimizer=tf.keras.optimizers.Adam(),\n",
        "#                 metrics=[metric]\n",
        "#   )\n",
        "#   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKwfFXVNLalv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Keras Hyperparameter Optimization\n",
        "# for target, batch_dataset in model_feature_target.items():\n",
        "#   train_dataset = batch_dataset['train_dataset']\n",
        "#   val_dataset = batch_dataset['val_dataset']\n",
        "#   test_dataset = model_feature_target[target][\"test_dataset\"]\n",
        "#   full_dataset = model_feature_target[target][\"full_dataset\"]\n",
        "#   sequence_length = batch_dataset['sequence_length']\n",
        "#   n_features = batch_dataset['n_features']\n",
        "\n",
        "#   callbacks = [\n",
        "#       tfdocs.modeling.EpochDots(),\n",
        "#       tf.keras.callbacks.TensorBoard(\n",
        "#           log_dir=f\"{gs_filepath}/hyperOpt/{approach}/{loss_fn}/{model_name}/logs/{target}_{datetime.now(eastern).strftime('%Y%m%d')}/\"\n",
        "#           ),\n",
        "#       tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3),\n",
        "#       ]\n",
        "\n",
        "#   tuner = kt.Hyperband(\n",
        "#     hypermodel=desired_model_function,\n",
        "#     objective=kt.Objective(name=\"val_loss\", direction=\"min\"),\n",
        "#     max_epochs=50,\n",
        "#     factor=3,\n",
        "#     seed=SEED,\n",
        "#     hyperband_iterations=2,\n",
        "#     tune_new_entries=True,\n",
        "#     allow_new_entries=True,\n",
        "#     directory=f\"{gs_filepath}/hyperOpt/{approach}/{loss_fn}/{model_name}/\", #f'{gs_filepath}/trials/{loss_fn}/{model_name}/'\n",
        "#     project_name=f\"trail_{target}_{model_name}\",\n",
        "#     overwrite=False,\n",
        "#     # distribution_strategy=strategy,\n",
        "#     )\n",
        "\n",
        "#   # Perform the hyperparameter search\n",
        "#   tuner.search(\n",
        "#       train_dataset,\n",
        "#       validation_data=val_dataset,\n",
        "#       epochs=50,\n",
        "#       callbacks=callbacks,\n",
        "#       verbose=1,\n",
        "#               )\n",
        "#   print(f'''\n",
        "#   ....................tuning of {target} is complete.\n",
        "#   ''')\n",
        "#   time.sleep(2)\n",
        "# #___________________________________________________________________________________________________________________\n",
        "#   def save_loss_img(history, best_epoch, model_name, loss_fn, target, idx, eval_result):\n",
        "#     metrics = pd.DataFrame(history.history)\n",
        "#     metrics['epoch'] = history.epoch\n",
        "\n",
        "#     loss = metrics.iloc[best_epoch, metrics.columns.get_loc('loss')]\n",
        "#     val_loss = metrics.iloc[best_epoch, metrics.columns.get_loc('val_loss')]\n",
        "\n",
        "#     data = metrics[['epoch','loss','val_loss']].melt(value_name=\"value\", id_vars=\"epoch\", var_name=\"loss\")\n",
        "\n",
        "#     plot = (\n",
        "#         ggplot(data)\n",
        "#       + geom_line(aes(x=\"epoch\", y='value', color=\"loss\",), size=0.75)\n",
        "#       + theme(legend_position=[1, 1], legend_justification=[1, 1],)\n",
        "#       + geom_vline(xintercept=best_epoch, color=\"red\", linetype=\"dashed\", size=1)\n",
        "#       + labs(title=f\"Best Hypermodel Result ({model_name}/{target}) - {loss_fn.upper()}\",\n",
        "#             subtitle=f\"@epoch:{best_epoch}, loss:{loss:.4f}, val_loss:{val_loss:.4f}, test loss*:{eval_result:.4}\",\n",
        "#             x=\"Epoch\", y='Error [mg/L]', color=\"\",\n",
        "#             caption=\"* Test loss evaluated on full dataset\")\n",
        "#       + scale_y_log10()\n",
        "#     )\n",
        "\n",
        "#     return ggsave(plot, f'{model_name}_losses_{target}.png',\n",
        "#                   path=f'{local_path}/data/images/{approach}/{loss_fn}/{model_name}', scale=1.0)\n",
        "\n",
        "#   def get_callbacks(weights_file, patience, lr_factor):\n",
        "#     '''\n",
        "#     Callbacks used for saving the best weights, early stopping and learning rate scheduling.\n",
        "#     '''\n",
        "#     return [\n",
        "#         tfdocs.modeling.EpochDots(),\n",
        "#         # Only save the weights that correspond to the maximum validation accuracy.\n",
        "#         # tf.keras.callbacks.ModelCheckpoint(\n",
        "#         #     filepath= weights_file,\n",
        "#         #     monitor=\"val_loss\",\n",
        "#         #     mode=\"min\",\n",
        "#         #     save_best_only=True,\n",
        "#         #     save_weights_only=True),\n",
        "#         # If val_loss doesn't improve for a number of epochs set with 'patience' var\n",
        "#         # training will stop to avoid overfitting.\n",
        "#         tf.keras.callbacks.EarlyStopping(\n",
        "#             monitor=\"val_loss\",\n",
        "#             mode=\"min\",\n",
        "#             patience = patience,\n",
        "#             verbose=1),\n",
        "#         # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n",
        "#         # for a number of epochs set with 'patience/2' var.\n",
        "#         tf.keras.callbacks.ReduceLROnPlateau(\n",
        "#             monitor=\"val_loss\",\n",
        "#             mode=\"min\",\n",
        "#             factor=lr_factor,\n",
        "#             min_lr=1e-6,\n",
        "#             patience=patience//2,\n",
        "#             verbose=1)\n",
        "#         ]\n",
        "\n",
        "#   # Get the best 4 hyperparameters objects and retrain the model on the training and validation set\n",
        "#   best_hps = tuner.get_best_hyperparameters(num_trials=3)\n",
        "\n",
        "#   def get_best_epoch(hp,):\n",
        "\n",
        "#     model = desired_model_function(hp)\n",
        "#     history = model.fit(\n",
        "#         train_dataset,\n",
        "#         validation_data=val_dataset,\n",
        "#         epochs=500, # Set the epochs higher than the tunning epochs\n",
        "#         verbose=0,\n",
        "#         callbacks=get_callbacks(None,\n",
        "#                                 patience=5, # Set the patience higher but control is with an early stopping\n",
        "#                                 lr_factor=0.3),\n",
        "#         shuffle=False,\n",
        "#     )\n",
        "#     val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "#     best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch))+1\n",
        "#     print(f\"Best epoch: {best_epoch}\")\n",
        "#     return best_epoch, model, history\n",
        "\n",
        "#   def get_best_trained_model(hp,):\n",
        "#     best_epoch, model, history = get_best_epoch(hp)\n",
        "#     model.fit(\n",
        "#         full_dataset,\n",
        "#         epochs=int(best_epoch),\n",
        "#         shuffle=False,\n",
        "#     )\n",
        "#     return model, best_epoch, history\n",
        "#   best_epochs = []\n",
        "#   best_histories = []\n",
        "#   best_models = []\n",
        "#   test_scores = []\n",
        "#   for idx, hp in enumerate(best_hps):\n",
        "#     hypermodel, best_epoch, history = get_best_trained_model(hp,)\n",
        "#     eval_result = hypermodel.evaluate(test_dataset)\n",
        "#     print(f\"{idx}: [Test loss, Test {metric.upper()},]: {eval_result}\")\n",
        "#     best_models.append(hypermodel)\n",
        "#     test_scores.append(eval_result[0])\n",
        "#     best_epochs.append(best_epoch)\n",
        "#     best_histories.append(history)\n",
        "\n",
        "#     # After training is complete, clear the model and release memory\n",
        "#     del hypermodel\n",
        "\n",
        "#   # Print and save jsut the best of the best\n",
        "#   best_idx = test_scores.index(min(test_scores))\n",
        "#   print(f\"The best model is: {best_idx} with a test score of: {min(test_scores)}\")\n",
        "#   print(\"\\n\")\n",
        "#   best_model = best_models[best_idx]\n",
        "#   save_loss_img(best_histories[best_idx], best_epochs[best_idx],\n",
        "#                 model_name, loss_fn, target, best_idx, test_scores[best_idx])\n",
        "#   tf.keras.utils.plot_model(best_model,\n",
        "#                             f\"{local_path}/data/images/{approach}/{loss_fn}/{model_name}/best_{model_name}_{target}.jpeg\",\n",
        "#                             show_shapes=True)\n",
        "#   best_model.summary()\n",
        "\n",
        "#   print(f'''\n",
        "#   ....................training of {target} is complete.\n",
        "#   ''')\n",
        "#   # break\n",
        "#   # Clear GPU memory if using TensorFlow with GPU\n",
        "#   tf.keras.backend.clear_session()\n",
        "#   time.sleep(2)\n",
        "# # shut down the runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameter Optimization with Optuna\n",
        "\n",
        "# # Create a dictionary to map model names to the corresponding model building functions\n",
        "# models = {\n",
        "#     'sLnn': build_sLnn,\n",
        "#     'dnn': build_dnn,\n",
        "#     'cnn': build_cnn,\n",
        "#     'rnn': build_rnn,\n",
        "#     # 'cnn_rnn': build_cnn_rnn\n",
        "# }\n",
        "\n",
        "# # Select the desired model building function based on the name\n",
        "# desired_model_function = models[model_name]\n",
        "\n",
        "# import optuna\n",
        "\n",
        "# from optuna.integration.tensorboard import TensorBoardCallback\n",
        "# from optuna.integration import TFKerasPruningCallback\n",
        "# from optuna.trial import TrialState\n",
        "\n",
        "\n",
        "\n",
        "# # Objective Function\n",
        "# def objective(trial, model_name, ds_train, ds_val, sequence_length, n_features):\n",
        "#   # Clear clutter from previous TensorFlow graphs.\n",
        "#   tf.keras.backend.clear_session()\n",
        "#   tf.keras.backend.reset_uids()\n",
        "\n",
        "#   # Create model instance\n",
        "#   model = desired_model_function(trial)\n",
        "\n",
        "#   # Metrics to be monitored by Optuna\n",
        "#   monitor = \"val_loss\"\n",
        "\n",
        "#   # Create callbacks for early stopping and pruning.\n",
        "#   callbacks = [\n",
        "#       tf.keras.callbacks.EarlyStopping(patience=3),\n",
        "#       TFKerasPruningCallback(trial, monitor),\n",
        "#   ]\n",
        "#   # Train model.\n",
        "#   history = model.fit(\n",
        "#       x=ds_train,\n",
        "#       y=None,\n",
        "#       batch_size=None,\n",
        "#       epochs=100,\n",
        "#       verbose=0,\n",
        "#       callbacks=callbacks,\n",
        "#       validation_data=ds_val,\n",
        "#       shuffle=False,\n",
        "#   )\n",
        "\n",
        "#   return history.history[monitor][-1]\n",
        "# #_________________________________________________________________________________________________________\n",
        "# def show_result(study,):\n",
        "#     import json\n",
        "#     import pickle\n",
        "\n",
        "#     pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "#     complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "#     print(\"\\nStudy statistics for {} using {}\".format(target, model_name))\n",
        "#     print(\"  Number of finished trials: \", len(study.trials))\n",
        "#     print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "#     print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "#     print(\"Best trial:\")\n",
        "#     trial = study.best_trial\n",
        "\n",
        "#     print(\"  Value: \", trial.value)\n",
        "\n",
        "#     print(\"  Params: \")\n",
        "#     for key, value in trial.params.items():\n",
        "#         print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#     trial.params[\"Value\"] = trial.value\n",
        "\n",
        "#     # Save the best parameters as JSON\n",
        "#     with open(f\"/content/colab_bucket/hyperOpt/{approach}/{loss_fn}/{model_name}/optuna_best_hp_{target}_{model_name}.json\",'w') as f:\n",
        "#       json.dump(study.best_trial.params, f, indent=4)\n",
        "\n",
        "#     # Save the study object\n",
        "#     with open(f\"/content/colab_bucket/hyperOpt/{approach}/{loss_fn}/{model_name}/optuna_best_study_{target}_{model_name}.pkl\", 'wb') as f:\n",
        "#       pickle.dump(study, f)\n",
        "# #_________________________________________________________________________________________________________\n",
        "# # An Early Stop Function that terminates the study of trials are being pruned consecutive after n times\n",
        "# # where n is an int value you set\n",
        "# class StopWhenTrialKeepBeingPrunedCallback:\n",
        "#     def __init__(self, threshold: int):\n",
        "#         self.threshold = threshold\n",
        "#         self._consequtive_pruned_count = 0\n",
        "\n",
        "#     def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
        "#         if trial.state == optuna.trial.TrialState.PRUNED:\n",
        "#             self._consequtive_pruned_count += 1\n",
        "#         else:\n",
        "#             self._consequtive_pruned_count = 0\n",
        "\n",
        "#         if self._consequtive_pruned_count >= self.threshold:\n",
        "#             study.stop()\n",
        "# #_________________________________________________________________________________________________________\n",
        "# def save_loss_img(history, best_epoch, model_name, loss_fn, target, eval_result):\n",
        "#     metrics = pd.DataFrame(history.history)\n",
        "#     metrics['epoch'] = history.epoch\n",
        "\n",
        "#     loss = metrics.iloc[best_epoch, metrics.columns.get_loc('loss')]\n",
        "#     val_loss = metrics.iloc[best_epoch, metrics.columns.get_loc('val_loss')]\n",
        "\n",
        "#     data = metrics[['epoch','loss','val_loss']].melt(value_name=\"value\", id_vars=\"epoch\", var_name=\"loss\")\n",
        "\n",
        "#     plot = (\n",
        "#         ggplot(data)\n",
        "#       + geom_line(aes(x=\"epoch\", y='value', color=\"loss\",), size=0.75)\n",
        "#       + theme(legend_position=[1, 1], legend_justification=[1, 1],)\n",
        "#       + geom_vline(xintercept=best_epoch, color=\"red\", linetype=\"dashed\", size=1)\n",
        "#       + labs(title=f\"Optimal Hypermodel Result ({model_name}/{target}) - {loss_fn.upper()}\",\n",
        "#             subtitle=f\"train loss:{loss:.4f}, val_loss:{val_loss:.4f}, test loss:{eval_result:.4f}\",\n",
        "#             x=\"Epoch\", y='Error [mg/L]', color=\"\",\n",
        "#             caption=\"red dash line: Optimal epoch\")\n",
        "#       + scale_y_log10()\n",
        "#     )\n",
        "\n",
        "#     return ggsave(plot, f'optuna_{model_name}_losses_{target}.png',\n",
        "#                   path=f'{local_path}/data/images/{approach}/{loss_fn}/{model_name}', scale=1.0)\n",
        "\n",
        "# #_________________________________________________________________________________________________________\n",
        "# def save_true_and_predicted_img(y, ypred, model_name, target,):\n",
        "#   data = pd.DataFrame({\"y\":y, \"y_pred\":y_pred}, index=dataset_to_model_x.tail(len(y)).index).reset_index()\n",
        "#   data['date'] = pd.to_datetime(data['date'])\n",
        "#   scaler = normalizer(2)\n",
        "#   scaler.fit(dataset_to_model_x[[target]])\n",
        "#   data['y_invs'] = scaler.inverse_transform(data[['y']])\n",
        "#   data['y_pred_invs'] = scaler.inverse_transform(data[['y_pred']])\n",
        "#   # data = pd.melt(data, id_vars=\"date\", value_vars=[\"y\",\"y_pred\"])\n",
        "#   plot = (\n",
        "#       ggplot(data.tail(13))\n",
        "#     + geom_line(aes(x=\"date\", y=\"y_invs\"),size=1, alpha=1, show_legend=True)\n",
        "#     + geom_line(aes(x=\"date\", y=\"y_pred_invs\", ),size=0.8, alpha=0.6, color=\"red\",\n",
        "#                 linetype=\"dashed\",\n",
        "#                 show_legend=True)\n",
        "#     + theme(legend_position=[1, 1], legend_justification=[1, 1],)\n",
        "#     + labs(title=f\"True vs. Predicted Values of {target}. ({model_name}/{loss_fn.upper()})\",\n",
        "#          subtitle=f\"Predicited concentrations of {target} over the last 12 months\",\n",
        "#         x=\"\", y=f'{target} in [mg/L]', color=\"\",\n",
        "#               caption=\"blue line: True Values\\n red dash line: Predicted Values\")\n",
        "#   + scale_x_datetime(format=\"%b %Y\")\n",
        "#   )\n",
        "#   return ggsave(plot, f'optuna_{model_name}_predicted_{target}.png',\n",
        "#                   path=f'{local_path}/data/images/{approach}/{loss_fn}/{model_name}', scale=1.0)\n",
        "# #_________________________________________________________________________________________________________\n",
        "# for target, batch_dataset in model_feature_target_x.items():\n",
        "#   # if target == \"Na_y\" or target == \"SO42_y\" or target == \"TDS_y\":\n",
        "#   train_dataset = batch_dataset['train_dataset']\n",
        "#   val_dataset = batch_dataset['val_dataset']\n",
        "#   test_dataset = batch_dataset[\"test_dataset\"]\n",
        "#   full_dataset = batch_dataset[\"full_dataset\"]\n",
        "#   sequence_length = batch_dataset['sequence_length']\n",
        "#   n_features = batch_dataset['n_features']\n",
        "\n",
        "#   study = optuna.create_study(\n",
        "#       direction=\"minimize\",\n",
        "#       sampler=optuna.samplers.TPESampler(seed=SEED),\n",
        "#       pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto'),\n",
        "#       study_name=\"optuna_{}_using_{}\".format(target, model_name),\n",
        "#       load_if_exists=False,\n",
        "#   )\n",
        "#   study_stop_cb = StopWhenTrialKeepBeingPrunedCallback(10)\n",
        "#   study.optimize(lambda trial: objective(trial, model_name, train_dataset, val_dataset, sequence_length, n_features),\n",
        "#                 n_trials=100,\n",
        "#                 #  n_jobs=-1,\n",
        "#                 timeout=60*60,\n",
        "#                 gc_after_trial=False, # This avoids running out of memory if set to True (a performance trade-off!)\n",
        "#                 show_progress_bar=True,\n",
        "#                 callbacks=[study_stop_cb]\n",
        "#                 )\n",
        "#   # Show study results\n",
        "#   show_result(study)\n",
        "\n",
        "#   def get_callbacks(patience, lr_factor):\n",
        "#     '''\n",
        "#     Callbacks used for saving the best weights, early stopping and learning rate scheduling.\n",
        "#     '''\n",
        "#     return [\n",
        "#         # tfdocs.modeling.EpochDots(),\n",
        "#         tf.keras.callbacks.EarlyStopping(\n",
        "#             monitor=\"val_loss\",\n",
        "#             mode=\"min\",\n",
        "#             patience = patience,\n",
        "#             verbose=1),\n",
        "#         # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n",
        "#         # for a number of epochs set with 'patience/2' var.\n",
        "#         tf.keras.callbacks.ReduceLROnPlateau(\n",
        "#             monitor=\"val_loss\",\n",
        "#             mode=\"min\",\n",
        "#             factor=lr_factor,\n",
        "#             min_lr=1e-6,\n",
        "#             patience=patience//2,\n",
        "#             verbose=1)\n",
        "#         ]\n",
        "#   # Get the best epoch\n",
        "#   best_model = desired_model_function(study.best_trial)\n",
        "#   history = best_model.fit(\n",
        "#       x=train_dataset,\n",
        "#       y=None,\n",
        "#       batch_size=None,\n",
        "#       epochs=1000,\n",
        "#       verbose='auto',\n",
        "#       callbacks=get_callbacks(patience=5, lr_factor=0.3),\n",
        "#       validation_data=val_dataset,\n",
        "#       shuffle=False,\n",
        "#   )\n",
        "#   val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "#   best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch))+1\n",
        "#   print(f\"Best epoch: {best_epoch}\")\n",
        "\n",
        "#   # Save best model architecture\n",
        "#   tf.keras.utils.plot_model(best_model,\n",
        "#                             f\"{local_path}/data/images/{approach}/{loss_fn}/{model_name}/best_optuna_{model_name}_{target}.jpeg\",\n",
        "#                             show_shapes=True)\n",
        "#   # Show summary\n",
        "#   best_model.summary()\n",
        "\n",
        "#   # After training is complete, clear the model and release memory\n",
        "#   del best_model\n",
        "\n",
        "#   # retrain the model on the full training dataset\n",
        "#   hypermodel = desired_model_function(study.best_trial)\n",
        "#   hypermodel.fit(\n",
        "#       x=full_dataset,\n",
        "#       y=None,\n",
        "#       batch_size=None,\n",
        "#       epochs=int(best_epoch*1.2),\n",
        "#       verbose='auto',\n",
        "#       callbacks=None,\n",
        "#       validation_data=None,\n",
        "#       shuffle=False,\n",
        "#   )\n",
        "\n",
        "#   eval_result = hypermodel.evaluate(test_dataset)\n",
        "#   print(f\"[Test loss, Test {metric.upper()}]: {eval_result}\")\n",
        "#   # Save the loss and validation plot\n",
        "#   save_loss_img(history, best_epoch, model_name, loss_fn, target, eval_result[0])\n",
        "\n",
        "#   # Get the X and y values from the test BatchDataset and predict the last 12 months\n",
        "#   X = np.concatenate([x for x, y in test_dataset], axis=0)\n",
        "#   y = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "#   y_pred = hypermodel.predict(X).flatten()\n",
        "\n",
        "#   # Save the drafted predictions\n",
        "#   save_true_and_predicted_img(y, y_pred, model_name, target,)\n",
        "\n",
        "#   # After training is complete, clear the model and release memory\n",
        "#   del hypermodel\n",
        "# # shut down the runtime\n",
        "# # runtime.unassign()"
      ],
      "metadata": {
        "id": "mnYpbHDZevfW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Manual Hyperparamter Optimization\n",
        "\n",
        "# def build_rnn_model():\n",
        "#   \"\"\"\n",
        "#   Builds a recurrent neural network model.\n",
        "#   \"\"\"\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   x = tf.keras.layers.LSTM(\n",
        "#           units=128,\n",
        "#           return_sequences=True,\n",
        "#           # dropout=0.2,\n",
        "#           # recurrent_dropout=0.2,\n",
        "#           name=f\"lstm_L1\",\n",
        "#           )(x)\n",
        "\n",
        "#   # x = tf.keras.layers.LSTM(\n",
        "#   #         units=128,\n",
        "#   #         return_sequences=True,\n",
        "#   #         recurrent_dropout=0.2,\n",
        "#   #         name=f\"lstm_L2\",\n",
        "#   #         )(x)\n",
        "\n",
        "#   x = tf.keras.layers.LSTM(\n",
        "#           units=64,\n",
        "#           return_sequences=False,\n",
        "#           dropout=0.2,\n",
        "#           recurrent_dropout=0.2,\n",
        "#           name=f\"lstm_L2\",\n",
        "#       )(x)\n",
        "#   x = tf.keras.layers.Dropout(rate=0.5, name=\"dropout\")(x)\n",
        "#   x = tf.keras.layers.Dense(units=64, name=\"dense_L1\")(x)\n",
        "\n",
        "#   outputs = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 #optimizer=trial.suggest_categorical(\"optimizer\", ['rmsprop', 'adam']),\n",
        "#                 optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "#                 metrics=[metric]\n",
        "#   )\n",
        "#   return model\n",
        "\n",
        "# mmodel = build_rnn_model()\n",
        "\n",
        "# target =\"Cl_y\"\n",
        "\n",
        "# train_dataset = model_feature_target_x[target]['train_dataset']\n",
        "# val_dataset = model_feature_target_x[target]['val_dataset']\n",
        "# test_dataset = model_feature_target_x[target][\"test_dataset\"]\n",
        "# full_dataset = model_feature_target_x[target][\"full_dataset\"]\n",
        "# sequence_length = model_feature_target_x[target]['sequence_length']\n",
        "# n_features = model_feature_target_x[target]['n_features']\n",
        "\n",
        "# def get_callbacks(patience, lr_factor):\n",
        "#   '''\n",
        "#   Callbacks used for saving the best weights, early stopping and learning rate scheduling.\n",
        "#   '''\n",
        "#   return [\n",
        "#       # tfdocs.modeling.EpochDots(),\n",
        "#       tf.keras.callbacks.EarlyStopping(\n",
        "#           monitor=\"val_loss\",\n",
        "#           mode=\"min\",\n",
        "#           patience = patience,\n",
        "#           verbose=1),\n",
        "#       # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n",
        "#       # for a number of epochs set with 'patience/2' var.\n",
        "#       tf.keras.callbacks.ReduceLROnPlateau(\n",
        "#           monitor=\"val_loss\",\n",
        "#           mode=\"min\",\n",
        "#           factor=lr_factor,\n",
        "#           min_lr=1e-5,\n",
        "#           patience=patience//2,\n",
        "#           verbose=1)\n",
        "#       ]\n",
        "\n",
        "# history = mmodel.fit(\n",
        "#     x=train_dataset,\n",
        "#     y=None,\n",
        "#     batch_size=None,\n",
        "#     epochs=1000,\n",
        "#     verbose='auto',\n",
        "#     callbacks=get_callbacks(patience=5, lr_factor=0.3),\n",
        "#     validation_data=val_dataset,\n",
        "#     shuffle=False,\n",
        "# )\n",
        "# val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "# best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch))+1\n",
        "# print(f\"Best epoch: {best_epoch}\")\n",
        "\n",
        "\n",
        "\n",
        "# eval_result = mmodel.evaluate(test_dataset)\n",
        "# print(f\"[Test loss, Test {metric.upper()}]: {eval_result}\")\n",
        "\n",
        "# metrics = pd.DataFrame(history.history)\n",
        "# metrics['epoch'] = history.epoch\n",
        "\n",
        "# loss = metrics.iloc[best_epoch, metrics.columns.get_loc('loss')]\n",
        "# val_loss = metrics.iloc[best_epoch, metrics.columns.get_loc('val_loss')]\n",
        "\n",
        "# data = metrics[['epoch','loss','val_loss']].melt(value_name=\"value\", id_vars=\"epoch\", var_name=\"loss\")\n",
        "\n",
        "# plot_1 = (\n",
        "#     ggplot(data)\n",
        "#   + geom_line(aes(x=\"epoch\", y='value', color=\"loss\",), size=0.75)\n",
        "#   + theme(legend_position=[1, 1], legend_justification=[1, 1],)\n",
        "#   + geom_vline(xintercept=best_epoch, color=\"red\", linetype=\"dashed\", size=1)\n",
        "#   + labs(title=f\"Optimal Hypermodel Result ({model_name}/{target}) - {loss_fn.upper()}\",\n",
        "#         subtitle=f\"train loss:{loss:.4f}, val_loss:{val_loss:.4f}, test loss:{eval_result[0]:.4f}\",\n",
        "#         x=\"Epoch\", y='Error [mg/L]', color=\"\",\n",
        "#         caption=\"red dash line: Optimal epoch\")\n",
        "#   + scale_y_log10()\n",
        "# )\n",
        "\n",
        "# plot_1\n",
        "\n",
        "# hypermodel = build_rnn_model()\n",
        "# hypermodel.fit(\n",
        "#     x=full_dataset,\n",
        "#     y=None,\n",
        "#     batch_size=None,\n",
        "#     epochs=int(best_epoch*1.2),\n",
        "#     verbose='auto',\n",
        "#     callbacks=None,\n",
        "#     validation_data=None,\n",
        "#     shuffle=False,\n",
        "# )\n",
        "\n",
        "# # Get the X and y values from the test BatchDataset and predict the last 12 months\n",
        "# X = np.concatenate([x for x, y in test_dataset], axis=0)\n",
        "# y = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "# y_pred = hypermodel.predict(X).flatten()\n",
        "\n",
        "# data = pd.DataFrame({\"y\":y, \"y_pred\":y_pred}, index=dataset_to_model_x.tail(len(y)).index).reset_index()\n",
        "# data['date'] = pd.to_datetime(data['date'])\n",
        "# scaler = normalizer(2)\n",
        "# scaler.fit(dataset_to_model_x[['Ca_X']])\n",
        "# data['y_invs'] = scaler.inverse_transform(data[['y']])\n",
        "# data['y_pred_invs'] = scaler.inverse_transform(data[['y_pred']])\n",
        "# plot_2 = (\n",
        "#     ggplot(data.tail(13))\n",
        "#   + geom_line(aes(x=\"date\", y=\"y_invs\"),size=1, alpha=1, show_legend=True)\n",
        "#   + geom_line(aes(x=\"date\", y=\"y_pred_invs\", ),size=0.8, alpha=0.6, color=\"red\",\n",
        "#               linetype=\"dashed\",\n",
        "#               show_legend=True)\n",
        "#   + theme(legend_position=[1, 1], legend_justification=[1, 1],)\n",
        "#   + labs(title=f\"True vs. Predicted Values of {target}. ({model_name}/{loss_fn.upper()})\",\n",
        "#         subtitle=f\"Predicited concentrations of {target} over the last 12 months\",\n",
        "#       x=\"\", y=f'{target} in [mg/L]', color=\"\",\n",
        "#             caption=\"blue line: True Values\\n red dash line: Predicted Values\")\n",
        "# + scale_x_datetime(format=\"%b %Y\")\n",
        "# )\n",
        "# plot_2"
      ],
      "metadata": {
        "id": "Y8bX5I5KVuQ6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cowVsj4S0Tz_"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "# %load_ext tensorboard\n",
        "\n",
        "# %tensorboard --logdir \"/content/drive/MyDrive/hyperOpt-trials/all/tensorboard/tb_logs/mae/cnn/20230703-051156\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q3aK-QHv-G6"
      },
      "outputs": [],
      "source": [
        "#https://www.youtube.com/watch?v=KrbV75Mby5E&t=149s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH9JdDHdxJzK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameter Optimization with Optuna - Old\n",
        "\n",
        "# import tensorflow_docs as tfdocs\n",
        "# import tensorflow_docs.modeling\n",
        "\n",
        "# import warnings\n",
        "# from tensorflow import get_logger\n",
        "# get_logger().setLevel('ERROR')\n",
        "# warnings.filterwarnings(\"ignore\", message=\"Setting the random state for TF\")\n",
        "\n",
        "# import absl.logging\n",
        "# absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "# import optuna\n",
        "\n",
        "# from optuna.integration.tensorboard import TensorBoardCallback\n",
        "# from optuna.integration import TFKerasPruningCallback\n",
        "# from optuna.trial import TrialState\n",
        "\n",
        "# from sklearn.model_selection import TimeSeriesSplit\n",
        "# from sklearn.model_selection import cross_validate\n",
        "# from scikeras.wrappers import KerasRegressor\n",
        "# import IPython\n",
        "# import json\n",
        "# import time\n",
        "\n",
        "# global filepath, loss_fn, model_name, target\n",
        "\n",
        "# # Optimizer\n",
        "# # def create_optimizer(trial):\n",
        "# #     # We optimize the choice of optimizers as well as their parameters.\n",
        "# #     kwargs = {}\n",
        "# #     optimizer_selected = trial.suggest_categorical(\"optimizer\", [ \"Adam\", \"RMSprop\", \"SGD\", ])\n",
        "# #     if optimizer_selected == \"Adam\":\n",
        "# #         kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-4, 1e-1, log=True)\n",
        "# #     elif optimizer_selected == \"RMSprop\":\n",
        "# #         kwargs[\"learning_rate\"] = trial.suggest_float(\n",
        "# #             \"rmsprop_learning_rate\", 1e-4, 1e-1, log=True\n",
        "# #         )\n",
        "# #         kwargs[\"momentum\"] = trial.suggest_float(\"rmsprop_momentum\", 1e-5, 1e-1, log=True)\n",
        "# #     elif optimizer_selected == \"SGD\":\n",
        "# #         kwargs[\"learning_rate\"] = trial.suggest_float(\n",
        "# #             \"sgd_opt_learning_rate\", 1e-2, 1e-1, log=True\n",
        "# #         )\n",
        "# #         kwargs[\"momentum\"] = trial.suggest_float(\"sgd_opt_momentum\", 1e-5, 1e-1, log=True)\n",
        "\n",
        "# #     optimizer = getattr(tf.keras.optimizers, optimizer_selected)(**kwargs)\n",
        "# #     return optimizer\n",
        "# # END_____________________________________________________________________________________________________\n",
        "# # A single layer neural network\n",
        "# def build_sLnn(trial):\n",
        "#   \"\"\"Builds a single layer neural model.\"\"\"\n",
        "\n",
        "#   # Create the model\n",
        "#   inputs = tf.keras.Input(shape=(sequence_length, n_features))\n",
        "#   x = inputs\n",
        "#   x = tf.keras.layers.Flatten()(x)\n",
        "#   # Add the input layer\n",
        "#   x = tf.keras.layers.Dense(trial.suggest_int('num_units', 32, 512, step=32,),\n",
        "#                             activation=trial.suggest_categorical('activation', values=['relu', 'elu', 'selu', 'gelu', 'tanh', ]),\n",
        "#                             )(x)\n",
        "\n",
        "#   # Tune whether to use dropout.\n",
        "#   if hp.Boolean(\"dropout\"):\n",
        "#       x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "\n",
        "#   outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "#   model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#   # Compile the model\n",
        "#   model.compile(loss=loss_fn,\n",
        "#                 optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float(\"learning_rate\", 1e-6, 1e-1, sampling=\"log\")),\n",
        "#                 metrics=['mae', 'mse']\n",
        "#                 )\n",
        "#   return model\n",
        "# # END_____________________________________________________________________________________________________\n",
        "# # Deep Neural Network (DNN)\n",
        "\n",
        "# def dnn_builder(trial,):\n",
        "#     dnn = tf.keras.Sequential()\n",
        "#     dnn.add(tf.keras.Input(shape=(X.shape[1:])))\n",
        "#     for i in range(trial.suggest_int('n_dense_layers', 1, 10)):\n",
        "#         units = trial.suggest_int(f'n_units_dense_L{i}', 32, 512, step=32)\n",
        "#         activation = trial.suggest_categorical(f'activation_dense_L{i}', ['relu', 'elu',])\n",
        "#         weight_decay = trial.suggest_float(f'weight_decay_dense_L{i}', 1e-5, 1e-1, log=True)\n",
        "#         dropout_rate = trial.suggest_float(f'dropout_dense_L{i}', 0.1, 0.5, log=True)\n",
        "\n",
        "#         dnn.add(tf.keras.layers.Dense(\n",
        "#             units=units,\n",
        "#             activation=activation,\n",
        "#             kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#             name=f'dense_L{i}',\n",
        "#         ))\n",
        "#         dnn.add(tf.keras.layers.Dropout(dropout_rate, name=f'dropout_dense_L{i}'))\n",
        "\n",
        "#     dnn.add(tf.keras.layers.Dense(1, name=\"output_L\"))\n",
        "#     dnn.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "#         learning_rate=trial.suggest_float(\"adam_learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "#             loss=loss_fn,\n",
        "#             metrics=['mae', 'mse', ]\n",
        "#         )\n",
        "#     return dnn\n",
        "\n",
        "# # END_____________________________________________________________________________________________________\n",
        "\n",
        "# # Covolutional Neural Network (CNN)\n",
        "# # noinspection PyShadowingNames\n",
        "# def cnn_builder(trial,):\n",
        "\n",
        "#     cnn = tf.keras.Sequential()\n",
        "#     cnn.add(tf.keras.Input(shape=(X.shape[1:])))\n",
        "#     for j in range(trial.suggest_int(\"n_conv1d_layers\", 1, 5)):\n",
        "#         n_filters = trial.suggest_int(f\"filters_conv1d_L{j}\", 32, 512, step=32)\n",
        "#         kernel_size = trial.suggest_int(f\"kernel_size_conv1d_L{j}\", 1, 10)\n",
        "#         weight_decay = trial.suggest_float(f\"weight_decay_conv1d_L{j}\", 1e-5, 1e-1, log=True)\n",
        "#         activation = trial.suggest_categorical(f\"activation_conv1d_L{j}\", ['relu', 'elu',])\n",
        "#         pool_size = trial.suggest_int(f\"pool_size_conv1d_L{j}\", 1, 10)\n",
        "\n",
        "#         cnn.add(tf.keras.layers.Conv1D(\n",
        "#             filters=n_filters,\n",
        "#             padding='same',\n",
        "#             kernel_size=kernel_size,\n",
        "#             activation=activation,\n",
        "#             kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#             name=\"conv1d_L{}\".format(j),\n",
        "#             )\n",
        "#         )\n",
        "#         cnn.add(tf.keras.layers.MaxPooling1D(pool_size=pool_size, padding='same', name=f\"max_pooling1d_{j}\", ))\n",
        "#     # Flatten the Convolutional Output (Matrix) to a one dimensional vector\n",
        "#     cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "#     # Fully connected layers\n",
        "#     for i in range(trial.suggest_int('n_dense_layers', 1, 5)):\n",
        "#         units = trial.suggest_int(f'n_units_dense_L{i}', 32, 512, step=32)\n",
        "#         activation = trial.suggest_categorical(f'activation_dense_L{i}', ['relu', 'elu',])\n",
        "#         weight_decay = trial.suggest_float(f'weight_decay_dense_L{i}', 1e-5, 1e-1, log=True)\n",
        "#         dropout_rate = trial.suggest_float(f'dropout_dense_L{i}', 0.1, 0.5, log=True)\n",
        "\n",
        "#         cnn.add(tf.keras.layers.Dense(\n",
        "#             units=units,\n",
        "#             activation=activation,\n",
        "#             kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#             name=f'dense_L{i}',\n",
        "#         ))\n",
        "#         cnn.add(tf.keras.layers.Dropout(dropout_rate, name=f'dropout_dense_L{i}'))\n",
        "#     cnn.add(tf.keras.layers.Dense(1, name=\"output_L\"))\n",
        "#     cnn.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "#         learning_rate=trial.suggest_float(\"adam_learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "#             loss=loss_fn,\n",
        "#             metrics=['mae', 'mse', ]\n",
        "#         )\n",
        "#     return cnn\n",
        "# # END_____________________________________________________________________________________________________\n",
        "\n",
        "# # Recurrent Neural Network (RNN)\n",
        "# # noinspection PyShadowingNames\n",
        "\n",
        "# def rnn_builder(trial,):\n",
        "#     # We optimize the numbers of layers and their units.\n",
        "#     n_layers = trial.suggest_int(\"n_lstm_layers\", 1, 5)\n",
        "#     rnn = tf.keras.Sequential()\n",
        "#     rnn.add(tf.keras.Input(shape=(X.shape[1:])))\n",
        "#     for k in range(n_layers):\n",
        "\n",
        "#         # Define the hyperparameters to be tuned\n",
        "#         units =trial.suggest_int(f\"lstm_units_L{k}\", 32, 512, log=True)\n",
        "\n",
        "#         if k == n_layers - 1:\n",
        "#             # Add the last LSTM layer without return sequences\n",
        "#             rnn.add(tf.keras.layers.LSTM(units=units,\n",
        "#                                          return_sequences=False,\n",
        "#                                          name=f\"lstm_L{k}\",\n",
        "#                                          recurrent_dropout=0.2,\n",
        "#                                             )\n",
        "#             )\n",
        "#         else:\n",
        "#             # Add the first and intermediate LSTM layers with return sequences\n",
        "#             rnn.add(tf.keras.layers.LSTM(units=units,\n",
        "#                                          recurrent_dropout=0.2,\n",
        "#                                          return_sequences=True,\n",
        "#                                          name=f\"lstm_L{k}\",\n",
        "#                                             )\n",
        "#             )\n",
        "#     rnn.add(tf.keras.layers.Dropout(trial.suggest_float(f'dropout_dense_Lx', 0.1, 0.5, log=True)))\n",
        "#     # Fully connected layers\n",
        "#     for i in range(trial.suggest_int('n_dense_layers', 1, 5)):\n",
        "#         units = trial.suggest_int(f'n_units_dense_L{i}', 32, 512, log=True)\n",
        "#         activation = trial.suggest_categorical(f'activation_dense_L{i}', ['relu', 'elu',])\n",
        "#         weight_decay = trial.suggest_float(f'weight_decay_dense_L{i}', 1e-5, 1e-1, log=True)\n",
        "#         dropout_rate = trial.suggest_float(f'dropout_dense_L{i}', 0.1, 0.5, log=True)\n",
        "\n",
        "#         rnn.add(tf.keras.layers.Dense(\n",
        "#             units=units,\n",
        "#             activation=activation,\n",
        "#             kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#             name=f'dense_L{i}',\n",
        "#         ))\n",
        "#         rnn.add(tf.keras.layers.Dropout(dropout_rate, name=f'dropout_dense_L{i}'))\n",
        "#     rnn.add(tf.keras.layers.Dense(1, name=\"output_L\"))\n",
        "#     rnn.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "#         learning_rate=trial.suggest_float(\"adam_learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "#             loss=loss_fn,\n",
        "#             metrics=['mae', 'mse', ]\n",
        "#         )\n",
        "\n",
        "#     return rnn\n",
        "# # END_____________________________________________________________________________________________________\n",
        "\n",
        "# # Convolutional Neural Network + Recurrent Neural Network (CNN+RNN)\n",
        "# # noinspection PyShadowingNames\n",
        "# def cnn_rnn_builder(trial,):\n",
        "\n",
        "#     cnn_rnn = tf.keras.Sequential()\n",
        "#     cnn_rnn.add(tf.keras.Input(shape=(X.shape[1:])))\n",
        "#     for l in range(trial.suggest_int(\"n_cnn_layers\", 1, 5)):\n",
        "#         n_filters = trial.suggest_int(f\"filters_conv1d_L{l}\", 32, 512, step=32)\n",
        "#         kernel_size = trial.suggest_int(f\"kernel_size_conv1d_L{l}\", 1, 10)\n",
        "#         weight_decay = trial.suggest_float(f\"weight_decay_conv1d_L{l}\", 1e-5, 1e-1, log=True)\n",
        "#         activation = trial.suggest_categorical(f\"activation_conv1d_L{l}\", ['relu', 'elu',])\n",
        "#         pool_size = trial.suggest_int(f\"pool_size_conv1d_L{l}\", 1, 10)\n",
        "\n",
        "\n",
        "#         cnn_rnn.add(tf.keras.layers.Conv1D(\n",
        "#             filters=n_filters,\n",
        "#             padding='same',\n",
        "#             kernel_size=kernel_size,\n",
        "#             activation=activation,\n",
        "#             kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#             name=\"con1d_L{}\".format(l),\n",
        "#         )\n",
        "#         )\n",
        "#         cnn_rnn.add(tf.keras.layers.MaxPooling1D(pool_size=pool_size,\n",
        "#                                                  padding='same',\n",
        "#                                                  name=f\"max_pooling1d_L{l}\", ))\n",
        "#     cnn_rnn.add(tf.keras.layers.Flatten())\n",
        "#     n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 5)\n",
        "#     for m in range(n_rnn_layers):\n",
        "\n",
        "#         # Define the hyperparameters to be tuned\n",
        "#         units =trial.suggest_int(f\"lstm_units_L{m}\", 32, 512, step=32)\n",
        "#         weight_decay = trial.suggest_float(f\"weight_decay_lstm_L{m}\", 1e-5, 1e-1, log=True)\n",
        "#         dropout_rate = trial.suggest_float(f\"lstm_dropout_lstm_L{m}\", 0.1, 0.5, log=True)\n",
        "\n",
        "#         if m == n_rnn_layers - 1:\n",
        "#             # Add the last LSTM layer without return sequences\n",
        "#             cnn_rnn.add(tf.keras.layers.LSTM(units=units,\n",
        "#                                         kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#                                         dropout=dropout_rate,\n",
        "#                                         return_sequences=False,\n",
        "#                                         name=f\"lstm_L{m}\",\n",
        "#                                         )\n",
        "#             )\n",
        "#         else:\n",
        "#             # Add the first and intermediate LSTM layers with return sequences\n",
        "#             cnn_rnn.add(tf.keras.layers.LSTM(units=units,\n",
        "#                                         kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#                                         dropout=dropout_rate,\n",
        "#                                         return_sequences=True,\n",
        "#                                         name=f\"lstm_L{m}\",\n",
        "#                                         )\n",
        "#             )\n",
        "#     cnn_rnn.add(tf.keras.layers.Flatten())\n",
        "#     # Fully connected layers\n",
        "#     for i in range(trial.suggest_int('n_dense_layers', 1, 5)):\n",
        "#         units = trial.suggest_int(f'n_units_dense_L{i}', 32, 512, step=32)\n",
        "#         activation = trial.suggest_categorical(f'activation_dense_L{i}', ['relu', 'elu',])\n",
        "#         weight_decay = trial.suggest_float(f'weight_decay_dense_L{i}', 1e-5, 1e-1, log=True)\n",
        "#         dropout_rate = trial.suggest_float(f'dropout_dense_L{i}', 0.1, 0.5, log=True)\n",
        "\n",
        "#         cnn_rnn.add(tf.keras.layers.Dense(\n",
        "#             units=units,\n",
        "#             activation=activation,\n",
        "#             kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
        "#             name=f'dense_L{i}',\n",
        "#         ))\n",
        "#         cnn_rnn.add(tf.keras.layers.Dropout(dropout_rate, name=f'dropout_dense_L{i}'))\n",
        "#     cnn_rnn.add(tf.keras.layers.Dense(1, name=\"output_L\"))\n",
        "#     cnn_rnn.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "#         learning_rate=trial.suggest_float(\"adam_learning_rate\", 1e-4, 1e-1, log=True)),\n",
        "#             loss=loss_fn,\n",
        "#             metrics=['mae', 'mse', ]\n",
        "#         )\n",
        "\n",
        "#     return cnn_rnn\n",
        "# # END_____________________________________________________________________________________________________\n",
        "\n",
        "# # noinspection PyShadowingNames\n",
        "# def show_result(study,):\n",
        "#     import json\n",
        "#     import pickle\n",
        "\n",
        "#     pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "#     complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "#     print(\"\\nStudy statistics for {} using {}\".format(target, model_name))\n",
        "#     print(\"  Number of finished trials: \", len(study.trials))\n",
        "#     print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "#     print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "#     print(\"Best trial:\")\n",
        "#     trial = study.best_trial\n",
        "\n",
        "#     print(\"  Value: \", trial.value)\n",
        "\n",
        "#     print(\"  Params: \")\n",
        "#     for key, value in trial.params.items():\n",
        "#         print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#     trial.params[\"Value\"] = trial.value\n",
        "\n",
        "#     # Save the best parameters as JSON\n",
        "#     with open(f'{filepath}/best_hps/{loss_fn}/{model_name}/best_hp_{target}_{model_name}.json','w') as f:\n",
        "#       json.dump(study.best_trial.params, f, indent=4)\n",
        "\n",
        "#     # Save the study object\n",
        "#     with open(f'{filepath}/studies/{loss_fn}/{model_name}/best_study_{target}_{model_name}.pkl', 'wb') as f:\n",
        "#       pickle.dump(study, f)\n",
        "# #_________________________________________________________________________________________________________\n",
        "# # Objective function can take additional arguements\n",
        "# # https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args\n",
        "\n",
        "# # noinspection PyRedeclaration\n",
        "# def objective(trial, model_name, X, y):\n",
        "\n",
        "#      # Clear clutter from previous TensorFlow graphs.\n",
        "#     tf.keras.backend.clear_session()\n",
        "\n",
        "#     global model\n",
        "\n",
        "#     from sklearn.model_selection import TimeSeriesSplit\n",
        "#     from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "#     tscv = TimeSeriesSplit(\n",
        "#             n_splits=5,\n",
        "#             gap=0,\n",
        "#             max_train_size=None,\n",
        "#             test_size=None,\n",
        "#         )\n",
        "\n",
        "#     # Metrics to be monitored by Optuna.\n",
        "#     monitor = 'loss'\n",
        "\n",
        "#     if model_name == \"dnn\": model = dnn_builder(trial)\n",
        "#     elif model_name == \"cnn\": model = cnn_builder(trial)\n",
        "#     elif model_name == \"rnn\": model = rnn_builder(trial)\n",
        "#     elif model_name == \"cnn_rnn\": model = cnn_rnn_builder(trial)\n",
        "\n",
        "\n",
        "#     estimator =  KerasRegressor(\n",
        "#         model=model,\n",
        "#         random_state=SEED,\n",
        "#         batch_size=128,\n",
        "#         verbose=0,\n",
        "#         callbacks=[\n",
        "#             # tfdocs.modeling.EpochDots(),\n",
        "#             tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3),\n",
        "#             TFKerasPruningCallback(trial, monitor)],\n",
        "#         shuffle=False,\n",
        "#         run_eagerly=False,\n",
        "#         epochs=100, #trial.suggest_int(f'epochs', 1, 1000, step=100),\n",
        "#         # meta = {\"input_shape\": input_shape, \"sequence_length\": sequence_length},\n",
        "#     )\n",
        "\n",
        "#     cv_results = cross_validate(\n",
        "#         estimator,\n",
        "#         X=X,\n",
        "#         y=y,\n",
        "#         cv=tscv,\n",
        "#         scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\",\"neg_root_mean_squared_error\"],\n",
        "#         error_score='raise',\n",
        "#     )\n",
        "\n",
        "#     mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
        "#     mse = -cv_results[\"test_neg_mean_squared_error\"]\n",
        "#     rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
        "\n",
        "#     print(\n",
        "#         f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n",
        "#         f\"Mean Squared Error:      {mse.mean():.3f} +/- {mse.std():.3f}\\n\"\n",
        "#         f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n",
        "#     )\n",
        "\n",
        "#     if loss_fn == 'mae':\n",
        "#         return np.mean(mae)\n",
        "#     elif loss_fn == 'mae':\n",
        "#         return np.mean(mse)\n",
        "# #_________________________________________________________________________________________________________\n",
        "# # noinspection PyShadowingNames\n",
        "# for loss_fn in ['mae', 'mse']:\n",
        "#   if loss_fn == 'mae':\n",
        "#     for model_name, model_data in model_feature_target.items():\n",
        "#       if model_name == 'rnn':\n",
        "#         for target, data in model_data.items():\n",
        "#           if target == \"SO4\" or target == \"TDS\":\n",
        "#             X, y = data['X'], data['y']\n",
        "#             filepath = '/content/drive/MyDrive/optuna-studies/all'\n",
        "#             study = optuna.create_study(\n",
        "#                 direction=\"minimize\",\n",
        "#                 sampler=optuna.samplers.TPESampler(seed=SEED),\n",
        "#                 pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto'),\n",
        "#                 study_name=\"study_to_predict_{}_using_{}\".format(target, model_name),\n",
        "#             )\n",
        "#             study.optimize(lambda trial: objective(trial, model_name, X, y),\n",
        "#                           n_trials=15,\n",
        "#                           n_jobs=-1,\n",
        "#                           timeout=60*60,\n",
        "#                             gc_after_trial=True, # This avoids running out of memory if set to True (a performance trade-off!)\n",
        "#                             show_progress_bar=True)\n",
        "\n",
        "#             show_result(study)\n",
        "# import time\n",
        "# time.sleep(10)\n",
        "# for loss_fn in ['mae', 'mse']:\n",
        "#   for model, model_data in model_feature_target_x.items():\n",
        "#       for param, data in model_data.items():\n",
        "#           target = param\n",
        "#           X, y = data['X'], data['y']\n",
        "#           filepath = '/content/drive/MyDrive/optuna-studies/up_down'\n",
        "#           study = optuna.create_study(\n",
        "#               direction=\"minimize\",\n",
        "#               sampler=optuna.samplers.TPESampler(seed=42),\n",
        "#               pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource='auto'),\n",
        "#               study_name=\"study_to_predict_{}_using_{}\".format(target, model_name),\n",
        "#           )\n",
        "#           study.optimize(lambda trial: objective(trial, loss_fn, model_name, target, X, y),\n",
        "#                         n_trials=25,\n",
        "#                         n_jobs=-1,\n",
        "#                         timeout=60*60)\n",
        "\n",
        "#           show_result(study,loss_fn, model_name, target,)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}